<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    <script type='text/javascript' src='https://udbaa.com/bnr.php?section=General_1&pub=316912&format=728x90&ga=g'></script> <meta name='monetag' content='07735af43d5282f24e58b1717078c013'>
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="Databases, Big Data, and Stream Processors have long had the property that it can be hard to *demonstrate* their value, like in a demo setting.
Databases coordinate the work of multiple teams of independent workers, and don't shine when there is just one user.
Big Data systems introduce scalable patterns that can be purely overhead when the data fit on a single laptop.
Stream Processors aim to get the lowest of end-to-end latencies, but do nothing of any consequence on static data.
These systems demonstrate value when you have variety, volume, and velocity, and most demo data sets have none of these.

Materialize, an operational data warehouse backed by scalable streaming systems, has all three of these challenges!

Fortunately, Materialize is powerful enough to synthesize its own operational data for demonstration purposes.
In this post, we'll build a recipe for a generic live data source using standard SQL primitives and some Materialize magic.
We'll then add various additional flavors: distributions over keys, irregular validity, foreign key relationships.
It's all based off of Materialize's own [auction load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction), but it's written entirely in SQL and something that I can customize as my needs evolve.

The thing I find most amazing here is that with just SQL you can create *live* data. 
Data that comes and goes, changes, and respects invariants as it does.
And that the gap between your idea for live data and making it happen is just typing some SQL.

### My Motivation: Materialize

Materialize has a few product beats it wants to hit when we demo it, derived from our product principles.

* **Responsiveness**: Materialize should be able to get back to you ASAP, even with lots of data involved.
* **Freshness**: Materialize should reflect arbitrary updates almost immediately, even through complex logic.
* **Consistency**: Materialize's outputs should always reflect a consistent state, even across multiple users and views.

We want to get folks to that 'aha!' moment where they realize that Materialize is like no other technology they know of.
Until that moment, Materialize could just be a trenchcoat containing Postgres, Spark, and Flink stacked according to your preferences.

Of course, different contexts connect for different users.
Some folks think about transactions and fraud and want to see how to get in front of that.
Others have users of their own, and know that sluggish, stale, inconsistent results are how they lose their users, and want to feel the lived experience.
Many users won't believe a thing until the data looks like their data, with the same schemas and data distributions, and the same business logic.
These are all legitimate concerns, and to me they speak to the inherent *heterogeneity* involved in demonstrating something.

I want to be able to demonstrate Materialize more **effectively**, which is some amount tied up in demonstrating it more **flexibly**.

As a personal first, I'm going to try telling the story in reverse order, Memento-style.
We'll start with the outcomes, which I hope will make sense, and then figure out how we got there, and eventually arrive at the wall of SQL that makes it happen.
It does mean we'll need some suspension of disbelief as we go, though; bear with me!
I do hope that whichever prefix you can tolerate makes sense and is engaging, and am only certain that if we started with the SQL it would not be.

The outine is, roughly:

1.  [Demonstrating Materialize with auction data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#demonstrating-materialize)

    We'll work through Materialize's quick start to show off `auctions` and `bids` data, and give a feel for what we need to have our live data do.
    We're going to hit the beats of responsiveness, freshness, and consistency along the way.

2.  [Building an Auction loadgen from unrelated live data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#auction-data-from-changing-moments)

    Here we'll build live views that define `auctions` and `bids`, starting from a live view that just contains recent timestamps.
    We'll see how to turn largely nonsense data into plausible auctions and bids, through the magic of pseudorandomness.

3.  [Building live random data from just SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#operational-data-from-thin-air)

    Starting from nothing more than SQL, we'll create a live view that Materialize can maintain containing recent moments as timestamps.
    As time continually moves forward, those moments continually change.

4.  [All the SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#appendix-all-the-sql) Really, just SQL.

Feel more than welcome to leap to the sections that interest you most.
I recommend starting at the beginning, though!

### Demonstrating Materialize

Let's sit down with Materialize and some live auction data and see if we can't hit the beats of responsiveness, freshness, and consistency.
The story is borrowed from our own quickstart, but by the end of it we'll find we've swapped out the quickstart's built-in load generator.

Materialize's [`AUCTION` load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction) populates `auctions` and `bids` tables.
Their contents look roughly like so:
```
materialize=> select * from auctions;
 id | seller |        item        |          end_time          
----+--------+--------------------+----------------------------
  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00
  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00
  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00
  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00
  ...
(4 rows)
```
```
materialize=> select * from bids;
 id | buyer | auction_id | amount |          bid_time          
----+-------+------------+--------+----------------------------
 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00
 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00
 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00
 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00
 ...
```

We will root around in this data, as it changes, and show off Materialize as something unlike other data tools.
Specifically we'll want to show off responsiveness, freshness, and consistency, which we'll do in that order.
However, the point is that you get them all at the same time, rather than one at a time, and by the end we should be able to see all three at once.

#### Beat 1: Responsiveness

Materialize is able to respond immediately, even to complex queries over large volumes of data.
Let's start by looking at the data, counting the number of auctions and the number of bids.
```
materialize=> select count(*) from auctions;
 count 
-------
 86400
(1 row)

Time: 52.580 ms
```
```
materialize=> select count(*) from bids;
  count   
----------
 10994252
(1 row)

Time: 8139.897 ms (00:08.140)
```
It's almost 100k auctions, and over 10M bids across them.
The specific numbers will make more sense when we get to the generator, but some of you may already recognize 86,400.
Ten seconds to count ten million things is not great, but this is running on our smallest instance (`25cc`; roughly 1/4 of a core).
Also, we aren't yet using Materialize's super-power to *maintain* results.

Materialize maintains computed results in indexes, created via the `CREATE INDEX` command.
```sql
-- Maintain bids indexed by id.
CREATE INDEX bids_id ON bids (id);
```

When we want to find a specific bid by id, this can be very fast .
```
materialize=> select * from bids where id = 4;
 id | buyer | auction_id | amount |        bid_time        
----+-------+------------+--------+------------------------
  4 |   228 |    6492730 |    149 | 2024-06-19 13:57:50+00
(1 row)

Time: 19.711 ms
```
Inspecting the query history (a feature in Materialize's console) we can see it only took 5ms for the DB, and the additional latency is between NYC and AWS's us-east-1.
This really is just a look-up into a maintained index, admittedly only on `bids` rather than some sophisticated query.

You can build indexes on any collection of data, not just raw data like `bids`.
We could build an index on `SELECT COUNT(*) FROM bids` to make that fast too, for example.
Instead, let's go straight to the good stuff.

Here's a view that determines which auctions are won by which bids.
```sql
-- Determine auction winners: the greatest bid before expiration.
CREATE VIEW winning_bids AS
  SELECT DISTINCT ON (auctions.id) bids.*,
    auctions.item,
    auctions.seller
  FROM auctions, bids
  WHERE auctions.id = bids.auction_id
    AND bids.bid_time < auctions.end_time
    AND mz_now() >= auctions.end_time
  ORDER BY auctions.id,
    bids.amount DESC,
    bids.bid_time,
    bids.buyer;
```

Directly querying this view results in a not-especially-responsive experience
```
materialize=> select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
        217 |    41 |    252
       3328 |   209 |     55
      19201 |   147 |    255
      18947 |    34 |    254
       7173 |   143 |      5
(5 rows)

Time: 87428.589 ms (01:27.429)
```
We are grinding through all the bids from scratch when you select from a view, because the view only explains what query you want to run.
A view by itself doesn't cause any work to be done ahead of time.

However, we can create indexes on `winning_bids`, and once they are up and running everything gets better.
We are going to create two indexes, on the columns `buyer` and `seller`, for future storytelling reasons.
```sql
-- Compute and maintain winning bids, indexed two ways.
CREATE INDEX wins_by_buyer ON winning_bids (buyer);
CREATE INDEX wins_by_seller ON winning_bids (seller);
```
The auctions aren't faster to magic in to existence than the original query was, so we'll have to wait a moment for them to hydrate.
Once this has happened, you get responsive interactions with the view.
```
materialize=> select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
    7647534 |     0 |    254
    6568079 |     0 |    239
   10578840 |     0 |    254
   14208479 |     0 |    249
   15263465 |     0 |    199
(5 rows)

Time: 61.283 ms
```
Rather than grind over the ten million or so bids to find winners, the ~80,000 results are maintained and its easy to read the first five.
Moreover, the results are all immediately up to date, rather than being fast-but-stale.
Let's hit that **freshness** beat now!

<!-- 
In addition, our indexes set us up for responsive ad-hoc queries.
Here's an example where we look for 'auction flippers': folks who are both buyers and sellers of the same item at increased amounts:
```sql
-- Look for users who re-sell their winnings
CREATE VIEW potential_flips AS
  SELECT w2.seller,
         w2.item AS item,
         w2.amount AS seller_amount,
         w1.amount AS buyer_amount
  FROM winning_bids w1,
       winning_bids w2
  WHERE w1.buyer = w2.seller
    AND w2.amount > w1.amount
    AND w1.item = w2.item;
```

We have enough auctions that some folks will be both buyers and sellers, and for some fraction of them its the same item for an increased price.
```
materialize=> select count(*) from potential_flips;
 count 
-------
  9755
(1 row)

Time: 602.481 ms
```
```
materialize=> select seller, count(*) from potential_flips group by seller order by count(*) desc limit 5;
 seller | count 
--------+-------
  42091 |     7
  42518 |     6
  10529 |     6
  39840 |     6
  49317 |     6
(5 rows)

Time: 678.330 ms
```

This is now pretty interactive, using scant resources, over enough data and through complex views that to start from scratch would be exhausting.
However, maintained indexes keep intermediate results up to date, and you get the same results as if re-run from scratch, just without the latency. -->

#### Beat 2: Freshness

All of this auction data is synthetic, and while it changes often the show is pretty clearly on rails.
That is, Materialize knows ahead of time what the changes will be.
You want to know that Materialize can respond fast to *arbitrary* changes, including ones that Materialize doesn't anticipate.

We need **interaction**!

Let's create a table we can modify, through our own whims and fancies.
Our modifications to this table, not part of the load generator, will be how we demonstrate the speed at which Materialize updates results as data change.
```sql
-- Accounts that we might flag for fraud.
CREATE TABLE fraud_accounts (id bigint);
```

Let's look at a query that calls out the top five accounts that win auctions.
We'll subscribe to it, meaning we get to watch the updates as they happen.
```sql
-- Top five non-fraud accounts, by auction wins.
COPY (SUBSCRIBE TO (
  SELECT buyer, count(*)
  FROM winning_bids
  WHERE buyer NOT IN (SELECT id FROM fraud_accounts)
  GROUP BY buyer
  ORDER BY count(*) DESC, buyer LIMIT 5
)) TO STDOUT;
```
This produces first a snapshot and then a continual stream of updates.
In our case, the updates are going to derive from our manipulation of `fraud_accounts`.
```
1718981380562	1	7247	7
1718981380562	1	17519	7
1718981380562	1	27558	7
1718981380562	1	20403	7
1718981380562	1	16584	7
```
The data are not really changing much, on account of the winners all having the same counts.
But, this is actually good for us, because we can see what happens when we force a change.

At this point, let's insert the record `17519` into `fraud_accounts`.
```
-- Mark 17519 as fraudulent
1718981387841	-1	17519	7
1718981387841	1	32134	7
```
We can do the same with `16584`, and then `34985`.
```
-- Mark 16584 as fraudulent
1718981392977	1	34985	7
1718981392977	-1	16584	7
-- Mark 34985 as fraudulent
1718981398158	1	35131	7
1718981398158	-1	34985	7
```
Finally, let's remove all records from `fraud_accounts` and we can see that we return back to the original state.
```
-- Remove all fraud indicators.
1718981403087	-1	35131	7
1718981403087	1	17519	7
1718981403087	-1	32134	7
1718981403087	1	16584	7
...
```
That `34985` record isn't mention here because it only showed up due to our other removals.
We don't hear about a change because there is no moment when it is in the top five, even transiently.
That is a great lead-in to Materailize's **consistency** properties!

#### Beat 3: Consistency

All the freshness and responsiveness in the world doesn't mean much if the results are incoherent.
Materialize only ever presents actual results that actually happened, with no transient errors.
When you see results, you can confidently act on them knowing that they are real, and don't need further second to bake.

Let's take a look at consistency through the lens of account balances as auctions close and winning buyers must pay sellers.
```sql
-- Account ids, with credits and debits from auctions sold and won.
CREATE VIEW funds_movement AS
  SELECT id,
         SUM(credits) AS credits,
         SUM(debits) AS debits
  FROM (
    SELECT seller AS id, amount AS credits, 0 AS debits
    FROM winning_bids
    UNION ALL
    SELECT buyer AS id, 0 AS credits, amount AS debits
    FROM winning_bids
  )
  GROUP BY id;
```

These balances derive from the same source: `winning_bids`, and although they'll vary from account to account, they should all add up.
Specifically, if we get the total credits and the total debits, they should 100% of the time be exactly equal.
```sql
-- Discrepancy between credits and debits.
SELECT SUM(credits) - SUM(debits) 
FROM funds_movement;
```
This query reports zero, 100% of the time.
We can `SUBSCRIBE` to the query to be notified of any change.
```
materialize=> COPY (SUBSCRIBE (
    SELECT SUM(credits) - SUM(debits) 
    FROM funds_movement
)) TO STDOUT;

1716312983129	1	0
```
This tells us that starting at time `1716312983129`, there was `1` record, and it was `0`.
You can sit there a while, and there will be no changes.
You could also add the `WITH (PROGRESS)` option, and it will provide regular heartbeats confirming that second-by-second it is still zero.
The credits and debits always add up, and aren't for a moment inconsistent.

We can set up similar views for other assertions.
For example, every account that has sold or won an auction should have a balance.
A SQL query can look for violations of this, and we can monitor it to see that it is always empty.
If it is ever non-empty, perhaps there are bugs in the query logic, its contents are immediately actionable: 
there is a specific time where the inputs evaluated to an invariant-violating output, and if you return to that moment you'll see the inputs that produce the bad output.

The consistency extends across multiple independent sessions.
The moment you get confirmation that the insert into `fraud_accounts`, you can be certain that no one will see that account in the top five non-fraudulent auction winners.
This guarantee is called 'strict serializability', that the system behaves as if every event occurred at a specific time between its start and end, and is the strongest guarantee that databases provide.

#### Demo over!

That's it!
We've completed the introduction to Materialize, and used auction data to show off responsiveness, freshness, and consistency.
There's a lot more to show off, of course, and if any of this sounded fascinating you should swing by https://materialize.com/register/ to spin up a trial environment.

However, in this post we will continue to unpack how we got all of that `auctions` and `bids` data in the first place!

### Auction Data from Changing Moments

Where do the `auctions` and `bids` data come from?
You can get them from our load generator, but we're going to try and coax them out of raw SQL.
We're going to start with something we haven't introduced yet, but it's a view whose content looks like this
```sql
-- All seconds within the past 24 hours.
CREATE VIEW moments AS
SELECT generate_series(
    now() - '1 day'::interval + '1 second'::interval,
    now(),
    '1 second'
) moment;
``` 

Unpacking this, `moments` contains rows with a single column containing a timestamp.
Whenever we look at it, the view contains those timestamps at most one day less than `now()`.
It should have at any moment exactly 86,400 records present, as many as `auctions` up above.

Importantly, this view definition will not actually work for us.
You are welcome to try it out, but you'll find out that while it can be *inspected*, it cannot be *maintained*.
We'll fix that by the end of the post, but it will need to wait until the next section.
For the moment, let's assume we have this view and the magical ability to keep it up to date.

These 'moments' are not auction data, though.
How do we get from moments to auctions and bids?

The `auctions` and `bids` collections look roughly like so:
```
materialize=> select * from auctions;
 id | seller |        item        |          end_time          
----+--------+--------------------+----------------------------
  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00
  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00
  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00
  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00
  ...
(4 rows)
```
```
materialize=> select * from bids;
 id | buyer | auction_id | amount |          bid_time          
----+-------+------------+--------+----------------------------
 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00
 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00
 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00
 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00
 ...
```

Auctions have a unique id, a seller id, an item description, and an end time.
Bids have a unique id (no relation), a buyer id, an auction id, the amount of the bid, and the time of the bid.

The `seller`, `item`, `buyer`, and `amount` fields are all random, within some bounds.
As a first cut, we'll think about just using random values for each of the columns.
Where might we get randomness, you ask?
Well, if *pseudo*-randomness is good enough (it will be), we can use cryptographic hashes of the moments.
```sql
-- Extract pseudorandom bytes from each moment.
CREATE VIEW random AS
SELECT moment, digest(moment::text, 'md5') as random
FROM moments;
```
Let's start with bytes from `random` to populate columns, and we'd have a first cut at random data.
Columns like `auctions.item` are populated by joining with a constant collection (part of the generator), but `id` and `seller` could just be random.
The `end_time` we'll pick to be a random time up to 256 minutes after the auction starts.
```sql
-- Totally accurate auction generator.
CREATE VIEW auctions_core AS
SELECT 
    moment,
    random,
    get_byte(random, 0) + 
    get_byte(random, 1) * 256 + 
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) +
    get_byte(random, 4) * 256 as seller,
    get_byte(random, 5) as item,
    -- Have each auction expire after up to 256 minutes.
    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time
FROM random;
```
We've clearly made some calls about how random each of these should be, and those calls influence what we'll see in the data.
For example, we've established at most 65,536 sellers, which lines up fine with our 86,400 auctions at any moment; some sellers will have multiple auctions and many will not.
Auctions are open for a few hours on average, close out but linger, and then vanish after 24 hours.
If we want to change any of these, perhaps to add more distinct items, or keep auctions running longer, or to skew the distribution over sellers, we can!

Similarly, the columns of `bids` are also pretty random, but columns like `auction_id` and `bid_time` do need to have some relationship to `auctions` and the referenced auction.
We'll build those out in just a moment, but have a bit more tidying to do for `auctions` first.

#### Adding Custom Expiration

Our auctions wind down after some random amount of time, but they are not removed from `auctions` for three hours.
Thematically we can think of this as auctions whose winners have been locked in, but whose accounts have not yet been settled.

If we want the auction to vanish from `auctions` at this time it closed, we could accomplish this with a temporal filter:
```sql
WHERE mz_now() < end_time
```
As soon as we reach `end_time` the auction would vanish from `auctions`.

This is a very helpful pattern for load generators that want to control when data arrive and when it departs, in finer detail than 'a twenty four hour window'.
For example, one could randomly generate `insert_ts` and `delete_ts`, and then use
```sql
-- Create an event that is live for the interval `[insert_ts, delete_ts]`.
WHERE mz_now() BETWEEN insert_ts AND delete_ts
```
This pattern allows careful control of when events *appear* to occur, by holding them back until `mz_now()` reaches a value, and then retracting them when it reaches a later value. 

#### Making More Realistic Data

Our random numbers for `item` aren't nearly as nice as what the existing load generator produces.
However, we can get the same results by putting those nice values in a view and using our integer `item` to join against the view.
```sql
-- A static view giving names to items.
CREATE VIEW items (id, item) AS VALUES
    (0, 'Signed Memorabilia'),
    (1, 'City Bar Crawl'),
    (2, 'Best Pizza in Town'),
    (3, 'Gift Basket'),
    (4, 'Custom Art');
```

Now when we want to produce an actual auction record, we can join against items like so
```sql
-- View that mirrors the `auctions` table from our load generator.
CREATE VIEW auctions AS
SELECT id, seller, items.item, end_time
FROM auctions_core, items
WHERE auction.item = items.id;
```

We've now got a view `auctions` that mirrors what Materialize's load generator produces, at least superficially.

#### Introducing Foreign Key Constraints

Each bid in `bids` references an auction, and we are unlikely to find an extant auction if we just use random numbers for `auction_id`.
We'd like to base our `bids` on the available auctions, and have them occur at times that make sense for the auction.

We can accomplish this by deriving the bids for an auction from `auctions` itself.
We will use some available pseudorandomness to propose a number of bids, and then create further pseudorandomness to determine the details of each bid.
```sql
CREATE VIEW bids AS
-- Establish per-bid records and pseudorandomness.
WITH prework AS (
    -- Create `get_byte(random, 6)` many bids for each auction, 
    -- each with their own freshly generated pseudorandomness.
    SELECT 
        id as auction_id,
        moment as auction_start,
        end_time as auction_end,
        digest(random::text || generate_series(1, get_byte(random, 6))::text, 'md5') as random
    FROM auctions_core
)
SELECT
    get_byte(random, 0) +
    get_byte(random, 1) * 256 +
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) AS buyer,
    auction_id,
    get_byte(random, 4)::numeric AS amount,
    auction_start + (get_byte(random, 5)::text || ' seconds')::interval as bid_time
FROM prework;
```

We now have a pile of bids for each auction, with the compelling property that when the auction goes away so too do its bids.
This gives us 'referential integrity', the property of foreign keys (`bids.auction_id`) that their referent (`auction.id`) is always valid.

And with this, we have generated the `auctions` and `bids` data that continually change, but always make sense.

There are several other changes you might want to make!
For example, random bids means that auctions stop changing as they go on, because new random bids are unlikely to beat all prior bids.
You could instead have the bids trend up with time, to keep the data interesting.
But, the changes are pretty easy to roll out, and just amount to editing the SQL that defines them.

Let's pause for now on noodling on ways we could make the data even more realistic.
Up next we have to unpack how we got that `moments` view in the first place.
Once we've done that, you are welcome to go back to playing around with load generator novelties and variations!

### Operational Data from Thin Air

Our `auctions` and `bids` data was based on a view `moments` that showed us all timestamps within the past three hours.
We saw how we could go from that to pretty much anything, through extracted pseudorandomness.

We used a view that seemed maybe too easy, that looked roughly like so:
```sql
-- Generate a sliding window over timestamp data.
-- Arguments: <volume>, <velocity>
SELECT moment,
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    <velocity>
) moment
WHERE now() BETWEEN moment AND moment + <volume>;
```

This example uses `generate_series` to produce moments at which events will occur.
The `<velocity>` argument chooses the step size of the `generate_series` call, and locks in the cadence of updates.
The `<volume>` argument controls for how long each record lingers, and sets the steady state size.
The result is a sliding window over random data, where you get to control the volume and velocity.

We used `'1 second'` for the velocity and `'1 day'` for the volume.

Now, while you can *type* the above, it won't actually run properly if you press enter.
The query describes 130 years of data, probably at something like a one second update frequency (because you wanted live data, right?).
I don't even know how to determine how many records this is accurately based on all the leap-action that occurs.
Moreover, you won't be able to materialize this view, because `now()` prevents materializations.

To actually get this to work, we'll have to use some clever tricks.
The coming subsections are a sequence of such tricks, and the punchline will be 'it works!', in case that saves you any time.

#### Clever trick 1: using `mz_now()`

Our first clever trick is to move from `now()` to `mz_now()`.
These are very similar functions, where the `now()` function gets you the contents of the system clock, and `mz_now()` gets you the transaction time of your command.
The main difference between the two is that we can materialize some queries containing `mz_now()`, unlike any query containing `now()`.

```sql
-- Generate a sliding window over timestamp data.
SELECT moment,
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    '1 second'
) moment
--    /------\---- LOOK HERE!
WHERE mz_now() BETWEEN moment AND moment + '1 day';
```
This very simple change means that Materialize now has the ability to keep the query up to date.
Materialize has a feature called ['temporal filters'](https://materialize.com/docs/transform-data/patterns/temporal-filters/) that allows `mz_now()` in `WHERE` clauses, because we are able to invert the clause and see the moment (Materialize time) at which changes will occur.

Unfortunately, the implementation strategy for keeping this view up to date still involves first producing all the data, and then filtering it (we don't have any magical insight into `generate_series` that allows us to invert its implementation).
But fortunately, we have other clever tricks available to us.

#### Clever trick 2: Hierachical Generation

The problem above is that we generate all the data at once, and then filter it.
We could instead generate the years of interest, from them the days of interest, from them the hours of interest, then minutes of interest, then seconds of interest, and finally milliseconds of interest.
In a sense we are generating *intervals* rather than *moments*, and then producing moments from the intervals.

Let's start by generating all the years we might be interested in.
We start with all the years we might reasonably need, and a `WHERE` clause that checks for intersection of the interval (`+ '1 year'`) and the extension by volume (`+ '1 day'`).
```sql
-- Each year-long interval of interest
CREATE VIEW years AS
SELECT * 
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    '1 year') year
WHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';
```
This view does not have all that many years in it. 
Roughly 130 of them.
Few enough that we can filter them down, and get to work on days.

At this point, we'll repeatedly refine the intervals by subdividing into the next granularity.
We'll do this for years into days, but you'll have to use your imagination for the others.
We have all the SQL at the end, so don't worry that you'll miss out on that.
```sql
-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(
        year, 
        year + '1 year' - '1 day'::interval, 
        '1 day') as day
    FROM years
)
WHERE mz_now() BETWEEN day AND day + '1 day';
```
We'll repeat this on to a view `seconds`, and stop there.

Although we could continue to milliseconds, experience has been that it's hard to demo things changing that quickly through SQL.
Lines of text flow past like the Matrix, and all you can really see is that there is change, not what the change is.

Unfortunately, there is a final gotcha.
Materialize is too clever by half, and if you materialize the `seconds` view, it will see that it is able to determine the entire 130 year timeline of the view, history and future, and record it for you.
At great expense.
These declarative systems are sometimes just too smart.

#### Clever trick 3: An empty table

We can fix everything by introducing an empty table.

The empty table is only present to ruin Materialize's ability to be certain it already knows the right answer about the future.
We'll introduce it to each of our views in the same place, and its only function is to menace Materialize with the possibility that it *could* contain data.
But it won't.
But we wont tell Materialize that.

```sql
-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(
        year, 
        year + '1 year' - '1 day'::interval, 
        '1 day') as day
    FROM years
    -- THIS NEXT LINE IS NEW!!
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN day AND day + '1 day';
```

With these tricks in hand, we now have the ability to spin it up and see what it looks like.

```sql
CREATE DEFAULT INDEX ON days;
```

We'll want to create the same default indexes on our other views: `hours`, `minutes`, and `seconds`.
Importantly, we want to create them in this order, also, to make sure that each relies on the one before it.
If they did not, we would be back in the world of the previous section, where each would read ahead until the end of time (the year 2099, in this example).

#### Finishing touches

As a final bit of housekeeping, we'll want to go from intervals back to moments, with some additional inequalities.
```sql
-- The final view we'll want to use.
CREATE VIEW moments AS
SELECT second AS moment FROM seconds
WHERE mz_now() >= second
  AND mz_now() < second + '1 day';
```
The only change here is the `mz_now()` inequality, which now avoids `BETWEEN` because it has inclusive upper bounds.
The result is now a view that always has exactly 24 * 60 * 60 = 86400 elements in it.
We can verify this by subscribing to the changelog of the count query:
```sql
-- Determine the count and monitor its changes.
COPY (
    SUBSCRIBE (SELECT COUNT(*) FROM moments) 
    WITH (progress = true)
)
TO stdout;
```
This reports an initial value of 86400, and then repeatedly reports (second by second) that there are no additional changes.
```
materialize=> COPY (
    SUBSCRIBE (SELECT COUNT(*) FROM moments) 
    WITH (progress = true)
)
TO stdout;
1716210913609	t	\N	\N
1716210913609	f	1	86400
1716210914250	t	\N	\N
1716210914264	t	\N	\N
1716210914685	t	\N	\N
1716210915000	t	\N	\N
1716210915684	t	\N	\N
1716210916000	t	\N	\N
1716210916248	t	\N	\N
1716210916288	t	\N	\N
1716210916330	t	\N	\N
1716210916683	t	\N	\N
^CCancel request sent
ERROR:  canceling statement due to user request
materialize=> 
```
All rows with a second column of `t` are 'progress' statements rather than data updates.
The second row, the only one with a `f`, confirms a single record (`1`) with a value of `86400`.

Yeah, that's it! The only thing left is to read a wall of text containing all the SQL.
Actually, I recommend bouncing up to the start of the post again, and confirming that the pieces fit together for you.
It's also a fine time to [try out Materialize](https://materialize.com/register/), the only system that can run all of these views. 

### Appendix: All the SQL

```sql
CREATE TABLE empty (e TIMESTAMP);

-- Supporting view to translate ids into text.
CREATE VIEW items (id, item) AS VALUES
    (0, 'Signed Memorabilia'),
    (1, 'City Bar Crawl'),
    (2, 'Best Pizza in Town'),
    (3, 'Gift Basket'),
    (4, 'Custom Art');

-- Each year-long interval of interest
CREATE VIEW years AS
SELECT * 
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    '1 year') year
WHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';

-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(year, year + '1 year' - '1 day'::interval, '1 day') as day
    FROM years
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN day AND day + '1 day' + '1 day';

-- Each hour-long interval of interest
CREATE VIEW hours AS
SELECT * FROM (
    SELECT generate_series(day, day + '1 day' - '1 hour'::interval, '1 hour') as hour
    FROM days
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN hour AND hour + '1 hour' + '1 day';

-- Each minute-long interval of interest
CREATE VIEW minutes AS
SELECT * FROM (
    SELECT generate_series(hour, hour + '1 hour' - '1 minute'::interval, '1 minute') AS minute
    FROM hours
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN minute AND minute + '1 minute' + '1 day';

-- Any second-long interval of interest
CREATE VIEW seconds AS
SELECT * FROM (
    SELECT generate_series(minute, minute + '1 minute' - '1 second'::interval, '1 second') as second
    FROM minutes
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN second AND second + '1 second' + '1 day';

-- Indexes are important to ensure we expand intervals carefully.
CREATE DEFAULT INDEX ON years;
CREATE DEFAULT INDEX ON days;
CREATE DEFAULT INDEX ON hours;
CREATE DEFAULT INDEX ON minutes;
CREATE DEFAULT INDEX ON seconds;

-- The final view we'll want to use .
CREATE VIEW moments AS
SELECT second AS moment FROM seconds
WHERE mz_now() >= second
  AND mz_now() < second + '1 day';

-- Extract pseudorandom bytes from each moment.
CREATE VIEW random AS
SELECT moment, digest(moment::text, 'md5') as random
FROM moments;

-- Present as auction 
CREATE VIEW auctions_core AS
SELECT 
    moment,
    random,
    get_byte(random, 0) + 
    get_byte(random, 1) * 256 + 
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) +
    get_byte(random, 4) * 256 as seller,
    get_byte(random, 5) as item,
    -- Have each auction expire after up to 256 minutes.
    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time
FROM random;

-- Refine and materialize auction data.
CREATE MATERIALIZED VIEW auctions AS
SELECT auctions_core.id, seller, items.item, end_time
FROM auctions_core, items
WHERE auctions_core.item % 5 = items.id;

-- Create and materialize bid data.
CREATE MATERIALIZED VIEW bids AS
-- Establish per-bid records and randomness.
WITH prework AS (
    SELECT 
        id AS auction_id,
        moment as auction_start,
        end_time as auction_end,
        digest(random::text || generate_series(1, get_byte(random, 5))::text, 'md5') as random
    FROM auctions_core
)
SELECT 
    get_byte(random, 0) + 
    get_byte(random, 1) * 256 + 
    get_byte(random, 2) * 65536 as id, 
    get_byte(random, 3) +
    get_byte(random, 4) * 256 AS buyer,
    auction_id,
    get_byte(random, 5)::numeric AS amount,
    auction_start + (get_byte(random, 6)::text || ' minutes')::interval as bid_time
FROM prework;
```
<!-- ##{'timestamp':1716094800}## -->

ã€‚">
<meta property="og:title" content="Demonstrating Operational Data with SQL">
<meta property="og:description" content="Databases, Big Data, and Stream Processors have long had the property that it can be hard to *demonstrate* their value, like in a demo setting.
Databases coordinate the work of multiple teams of independent workers, and don't shine when there is just one user.
Big Data systems introduce scalable patterns that can be purely overhead when the data fit on a single laptop.
Stream Processors aim to get the lowest of end-to-end latencies, but do nothing of any consequence on static data.
These systems demonstrate value when you have variety, volume, and velocity, and most demo data sets have none of these.

Materialize, an operational data warehouse backed by scalable streaming systems, has all three of these challenges!

Fortunately, Materialize is powerful enough to synthesize its own operational data for demonstration purposes.
In this post, we'll build a recipe for a generic live data source using standard SQL primitives and some Materialize magic.
We'll then add various additional flavors: distributions over keys, irregular validity, foreign key relationships.
It's all based off of Materialize's own [auction load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction), but it's written entirely in SQL and something that I can customize as my needs evolve.

The thing I find most amazing here is that with just SQL you can create *live* data. 
Data that comes and goes, changes, and respects invariants as it does.
And that the gap between your idea for live data and making it happen is just typing some SQL.

### My Motivation: Materialize

Materialize has a few product beats it wants to hit when we demo it, derived from our product principles.

* **Responsiveness**: Materialize should be able to get back to you ASAP, even with lots of data involved.
* **Freshness**: Materialize should reflect arbitrary updates almost immediately, even through complex logic.
* **Consistency**: Materialize's outputs should always reflect a consistent state, even across multiple users and views.

We want to get folks to that 'aha!' moment where they realize that Materialize is like no other technology they know of.
Until that moment, Materialize could just be a trenchcoat containing Postgres, Spark, and Flink stacked according to your preferences.

Of course, different contexts connect for different users.
Some folks think about transactions and fraud and want to see how to get in front of that.
Others have users of their own, and know that sluggish, stale, inconsistent results are how they lose their users, and want to feel the lived experience.
Many users won't believe a thing until the data looks like their data, with the same schemas and data distributions, and the same business logic.
These are all legitimate concerns, and to me they speak to the inherent *heterogeneity* involved in demonstrating something.

I want to be able to demonstrate Materialize more **effectively**, which is some amount tied up in demonstrating it more **flexibly**.

As a personal first, I'm going to try telling the story in reverse order, Memento-style.
We'll start with the outcomes, which I hope will make sense, and then figure out how we got there, and eventually arrive at the wall of SQL that makes it happen.
It does mean we'll need some suspension of disbelief as we go, though; bear with me!
I do hope that whichever prefix you can tolerate makes sense and is engaging, and am only certain that if we started with the SQL it would not be.

The outine is, roughly:

1.  [Demonstrating Materialize with auction data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#demonstrating-materialize)

    We'll work through Materialize's quick start to show off `auctions` and `bids` data, and give a feel for what we need to have our live data do.
    We're going to hit the beats of responsiveness, freshness, and consistency along the way.

2.  [Building an Auction loadgen from unrelated live data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#auction-data-from-changing-moments)

    Here we'll build live views that define `auctions` and `bids`, starting from a live view that just contains recent timestamps.
    We'll see how to turn largely nonsense data into plausible auctions and bids, through the magic of pseudorandomness.

3.  [Building live random data from just SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#operational-data-from-thin-air)

    Starting from nothing more than SQL, we'll create a live view that Materialize can maintain containing recent moments as timestamps.
    As time continually moves forward, those moments continually change.

4.  [All the SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#appendix-all-the-sql) Really, just SQL.

Feel more than welcome to leap to the sections that interest you most.
I recommend starting at the beginning, though!

### Demonstrating Materialize

Let's sit down with Materialize and some live auction data and see if we can't hit the beats of responsiveness, freshness, and consistency.
The story is borrowed from our own quickstart, but by the end of it we'll find we've swapped out the quickstart's built-in load generator.

Materialize's [`AUCTION` load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction) populates `auctions` and `bids` tables.
Their contents look roughly like so:
```
materialize=> select * from auctions;
 id | seller |        item        |          end_time          
----+--------+--------------------+----------------------------
  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00
  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00
  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00
  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00
  ...
(4 rows)
```
```
materialize=> select * from bids;
 id | buyer | auction_id | amount |          bid_time          
----+-------+------------+--------+----------------------------
 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00
 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00
 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00
 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00
 ...
```

We will root around in this data, as it changes, and show off Materialize as something unlike other data tools.
Specifically we'll want to show off responsiveness, freshness, and consistency, which we'll do in that order.
However, the point is that you get them all at the same time, rather than one at a time, and by the end we should be able to see all three at once.

#### Beat 1: Responsiveness

Materialize is able to respond immediately, even to complex queries over large volumes of data.
Let's start by looking at the data, counting the number of auctions and the number of bids.
```
materialize=> select count(*) from auctions;
 count 
-------
 86400
(1 row)

Time: 52.580 ms
```
```
materialize=> select count(*) from bids;
  count   
----------
 10994252
(1 row)

Time: 8139.897 ms (00:08.140)
```
It's almost 100k auctions, and over 10M bids across them.
The specific numbers will make more sense when we get to the generator, but some of you may already recognize 86,400.
Ten seconds to count ten million things is not great, but this is running on our smallest instance (`25cc`; roughly 1/4 of a core).
Also, we aren't yet using Materialize's super-power to *maintain* results.

Materialize maintains computed results in indexes, created via the `CREATE INDEX` command.
```sql
-- Maintain bids indexed by id.
CREATE INDEX bids_id ON bids (id);
```

When we want to find a specific bid by id, this can be very fast .
```
materialize=> select * from bids where id = 4;
 id | buyer | auction_id | amount |        bid_time        
----+-------+------------+--------+------------------------
  4 |   228 |    6492730 |    149 | 2024-06-19 13:57:50+00
(1 row)

Time: 19.711 ms
```
Inspecting the query history (a feature in Materialize's console) we can see it only took 5ms for the DB, and the additional latency is between NYC and AWS's us-east-1.
This really is just a look-up into a maintained index, admittedly only on `bids` rather than some sophisticated query.

You can build indexes on any collection of data, not just raw data like `bids`.
We could build an index on `SELECT COUNT(*) FROM bids` to make that fast too, for example.
Instead, let's go straight to the good stuff.

Here's a view that determines which auctions are won by which bids.
```sql
-- Determine auction winners: the greatest bid before expiration.
CREATE VIEW winning_bids AS
  SELECT DISTINCT ON (auctions.id) bids.*,
    auctions.item,
    auctions.seller
  FROM auctions, bids
  WHERE auctions.id = bids.auction_id
    AND bids.bid_time < auctions.end_time
    AND mz_now() >= auctions.end_time
  ORDER BY auctions.id,
    bids.amount DESC,
    bids.bid_time,
    bids.buyer;
```

Directly querying this view results in a not-especially-responsive experience
```
materialize=> select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
        217 |    41 |    252
       3328 |   209 |     55
      19201 |   147 |    255
      18947 |    34 |    254
       7173 |   143 |      5
(5 rows)

Time: 87428.589 ms (01:27.429)
```
We are grinding through all the bids from scratch when you select from a view, because the view only explains what query you want to run.
A view by itself doesn't cause any work to be done ahead of time.

However, we can create indexes on `winning_bids`, and once they are up and running everything gets better.
We are going to create two indexes, on the columns `buyer` and `seller`, for future storytelling reasons.
```sql
-- Compute and maintain winning bids, indexed two ways.
CREATE INDEX wins_by_buyer ON winning_bids (buyer);
CREATE INDEX wins_by_seller ON winning_bids (seller);
```
The auctions aren't faster to magic in to existence than the original query was, so we'll have to wait a moment for them to hydrate.
Once this has happened, you get responsive interactions with the view.
```
materialize=> select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
    7647534 |     0 |    254
    6568079 |     0 |    239
   10578840 |     0 |    254
   14208479 |     0 |    249
   15263465 |     0 |    199
(5 rows)

Time: 61.283 ms
```
Rather than grind over the ten million or so bids to find winners, the ~80,000 results are maintained and its easy to read the first five.
Moreover, the results are all immediately up to date, rather than being fast-but-stale.
Let's hit that **freshness** beat now!

<!-- 
In addition, our indexes set us up for responsive ad-hoc queries.
Here's an example where we look for 'auction flippers': folks who are both buyers and sellers of the same item at increased amounts:
```sql
-- Look for users who re-sell their winnings
CREATE VIEW potential_flips AS
  SELECT w2.seller,
         w2.item AS item,
         w2.amount AS seller_amount,
         w1.amount AS buyer_amount
  FROM winning_bids w1,
       winning_bids w2
  WHERE w1.buyer = w2.seller
    AND w2.amount > w1.amount
    AND w1.item = w2.item;
```

We have enough auctions that some folks will be both buyers and sellers, and for some fraction of them its the same item for an increased price.
```
materialize=> select count(*) from potential_flips;
 count 
-------
  9755
(1 row)

Time: 602.481 ms
```
```
materialize=> select seller, count(*) from potential_flips group by seller order by count(*) desc limit 5;
 seller | count 
--------+-------
  42091 |     7
  42518 |     6
  10529 |     6
  39840 |     6
  49317 |     6
(5 rows)

Time: 678.330 ms
```

This is now pretty interactive, using scant resources, over enough data and through complex views that to start from scratch would be exhausting.
However, maintained indexes keep intermediate results up to date, and you get the same results as if re-run from scratch, just without the latency. -->

#### Beat 2: Freshness

All of this auction data is synthetic, and while it changes often the show is pretty clearly on rails.
That is, Materialize knows ahead of time what the changes will be.
You want to know that Materialize can respond fast to *arbitrary* changes, including ones that Materialize doesn't anticipate.

We need **interaction**!

Let's create a table we can modify, through our own whims and fancies.
Our modifications to this table, not part of the load generator, will be how we demonstrate the speed at which Materialize updates results as data change.
```sql
-- Accounts that we might flag for fraud.
CREATE TABLE fraud_accounts (id bigint);
```

Let's look at a query that calls out the top five accounts that win auctions.
We'll subscribe to it, meaning we get to watch the updates as they happen.
```sql
-- Top five non-fraud accounts, by auction wins.
COPY (SUBSCRIBE TO (
  SELECT buyer, count(*)
  FROM winning_bids
  WHERE buyer NOT IN (SELECT id FROM fraud_accounts)
  GROUP BY buyer
  ORDER BY count(*) DESC, buyer LIMIT 5
)) TO STDOUT;
```
This produces first a snapshot and then a continual stream of updates.
In our case, the updates are going to derive from our manipulation of `fraud_accounts`.
```
1718981380562	1	7247	7
1718981380562	1	17519	7
1718981380562	1	27558	7
1718981380562	1	20403	7
1718981380562	1	16584	7
```
The data are not really changing much, on account of the winners all having the same counts.
But, this is actually good for us, because we can see what happens when we force a change.

At this point, let's insert the record `17519` into `fraud_accounts`.
```
-- Mark 17519 as fraudulent
1718981387841	-1	17519	7
1718981387841	1	32134	7
```
We can do the same with `16584`, and then `34985`.
```
-- Mark 16584 as fraudulent
1718981392977	1	34985	7
1718981392977	-1	16584	7
-- Mark 34985 as fraudulent
1718981398158	1	35131	7
1718981398158	-1	34985	7
```
Finally, let's remove all records from `fraud_accounts` and we can see that we return back to the original state.
```
-- Remove all fraud indicators.
1718981403087	-1	35131	7
1718981403087	1	17519	7
1718981403087	-1	32134	7
1718981403087	1	16584	7
...
```
That `34985` record isn't mention here because it only showed up due to our other removals.
We don't hear about a change because there is no moment when it is in the top five, even transiently.
That is a great lead-in to Materailize's **consistency** properties!

#### Beat 3: Consistency

All the freshness and responsiveness in the world doesn't mean much if the results are incoherent.
Materialize only ever presents actual results that actually happened, with no transient errors.
When you see results, you can confidently act on them knowing that they are real, and don't need further second to bake.

Let's take a look at consistency through the lens of account balances as auctions close and winning buyers must pay sellers.
```sql
-- Account ids, with credits and debits from auctions sold and won.
CREATE VIEW funds_movement AS
  SELECT id,
         SUM(credits) AS credits,
         SUM(debits) AS debits
  FROM (
    SELECT seller AS id, amount AS credits, 0 AS debits
    FROM winning_bids
    UNION ALL
    SELECT buyer AS id, 0 AS credits, amount AS debits
    FROM winning_bids
  )
  GROUP BY id;
```

These balances derive from the same source: `winning_bids`, and although they'll vary from account to account, they should all add up.
Specifically, if we get the total credits and the total debits, they should 100% of the time be exactly equal.
```sql
-- Discrepancy between credits and debits.
SELECT SUM(credits) - SUM(debits) 
FROM funds_movement;
```
This query reports zero, 100% of the time.
We can `SUBSCRIBE` to the query to be notified of any change.
```
materialize=> COPY (SUBSCRIBE (
    SELECT SUM(credits) - SUM(debits) 
    FROM funds_movement
)) TO STDOUT;

1716312983129	1	0
```
This tells us that starting at time `1716312983129`, there was `1` record, and it was `0`.
You can sit there a while, and there will be no changes.
You could also add the `WITH (PROGRESS)` option, and it will provide regular heartbeats confirming that second-by-second it is still zero.
The credits and debits always add up, and aren't for a moment inconsistent.

We can set up similar views for other assertions.
For example, every account that has sold or won an auction should have a balance.
A SQL query can look for violations of this, and we can monitor it to see that it is always empty.
If it is ever non-empty, perhaps there are bugs in the query logic, its contents are immediately actionable: 
there is a specific time where the inputs evaluated to an invariant-violating output, and if you return to that moment you'll see the inputs that produce the bad output.

The consistency extends across multiple independent sessions.
The moment you get confirmation that the insert into `fraud_accounts`, you can be certain that no one will see that account in the top five non-fraudulent auction winners.
This guarantee is called 'strict serializability', that the system behaves as if every event occurred at a specific time between its start and end, and is the strongest guarantee that databases provide.

#### Demo over!

That's it!
We've completed the introduction to Materialize, and used auction data to show off responsiveness, freshness, and consistency.
There's a lot more to show off, of course, and if any of this sounded fascinating you should swing by https://materialize.com/register/ to spin up a trial environment.

However, in this post we will continue to unpack how we got all of that `auctions` and `bids` data in the first place!

### Auction Data from Changing Moments

Where do the `auctions` and `bids` data come from?
You can get them from our load generator, but we're going to try and coax them out of raw SQL.
We're going to start with something we haven't introduced yet, but it's a view whose content looks like this
```sql
-- All seconds within the past 24 hours.
CREATE VIEW moments AS
SELECT generate_series(
    now() - '1 day'::interval + '1 second'::interval,
    now(),
    '1 second'
) moment;
``` 

Unpacking this, `moments` contains rows with a single column containing a timestamp.
Whenever we look at it, the view contains those timestamps at most one day less than `now()`.
It should have at any moment exactly 86,400 records present, as many as `auctions` up above.

Importantly, this view definition will not actually work for us.
You are welcome to try it out, but you'll find out that while it can be *inspected*, it cannot be *maintained*.
We'll fix that by the end of the post, but it will need to wait until the next section.
For the moment, let's assume we have this view and the magical ability to keep it up to date.

These 'moments' are not auction data, though.
How do we get from moments to auctions and bids?

The `auctions` and `bids` collections look roughly like so:
```
materialize=> select * from auctions;
 id | seller |        item        |          end_time          
----+--------+--------------------+----------------------------
  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00
  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00
  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00
  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00
  ...
(4 rows)
```
```
materialize=> select * from bids;
 id | buyer | auction_id | amount |          bid_time          
----+-------+------------+--------+----------------------------
 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00
 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00
 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00
 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00
 ...
```

Auctions have a unique id, a seller id, an item description, and an end time.
Bids have a unique id (no relation), a buyer id, an auction id, the amount of the bid, and the time of the bid.

The `seller`, `item`, `buyer`, and `amount` fields are all random, within some bounds.
As a first cut, we'll think about just using random values for each of the columns.
Where might we get randomness, you ask?
Well, if *pseudo*-randomness is good enough (it will be), we can use cryptographic hashes of the moments.
```sql
-- Extract pseudorandom bytes from each moment.
CREATE VIEW random AS
SELECT moment, digest(moment::text, 'md5') as random
FROM moments;
```
Let's start with bytes from `random` to populate columns, and we'd have a first cut at random data.
Columns like `auctions.item` are populated by joining with a constant collection (part of the generator), but `id` and `seller` could just be random.
The `end_time` we'll pick to be a random time up to 256 minutes after the auction starts.
```sql
-- Totally accurate auction generator.
CREATE VIEW auctions_core AS
SELECT 
    moment,
    random,
    get_byte(random, 0) + 
    get_byte(random, 1) * 256 + 
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) +
    get_byte(random, 4) * 256 as seller,
    get_byte(random, 5) as item,
    -- Have each auction expire after up to 256 minutes.
    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time
FROM random;
```
We've clearly made some calls about how random each of these should be, and those calls influence what we'll see in the data.
For example, we've established at most 65,536 sellers, which lines up fine with our 86,400 auctions at any moment; some sellers will have multiple auctions and many will not.
Auctions are open for a few hours on average, close out but linger, and then vanish after 24 hours.
If we want to change any of these, perhaps to add more distinct items, or keep auctions running longer, or to skew the distribution over sellers, we can!

Similarly, the columns of `bids` are also pretty random, but columns like `auction_id` and `bid_time` do need to have some relationship to `auctions` and the referenced auction.
We'll build those out in just a moment, but have a bit more tidying to do for `auctions` first.

#### Adding Custom Expiration

Our auctions wind down after some random amount of time, but they are not removed from `auctions` for three hours.
Thematically we can think of this as auctions whose winners have been locked in, but whose accounts have not yet been settled.

If we want the auction to vanish from `auctions` at this time it closed, we could accomplish this with a temporal filter:
```sql
WHERE mz_now() < end_time
```
As soon as we reach `end_time` the auction would vanish from `auctions`.

This is a very helpful pattern for load generators that want to control when data arrive and when it departs, in finer detail than 'a twenty four hour window'.
For example, one could randomly generate `insert_ts` and `delete_ts`, and then use
```sql
-- Create an event that is live for the interval `[insert_ts, delete_ts]`.
WHERE mz_now() BETWEEN insert_ts AND delete_ts
```
This pattern allows careful control of when events *appear* to occur, by holding them back until `mz_now()` reaches a value, and then retracting them when it reaches a later value. 

#### Making More Realistic Data

Our random numbers for `item` aren't nearly as nice as what the existing load generator produces.
However, we can get the same results by putting those nice values in a view and using our integer `item` to join against the view.
```sql
-- A static view giving names to items.
CREATE VIEW items (id, item) AS VALUES
    (0, 'Signed Memorabilia'),
    (1, 'City Bar Crawl'),
    (2, 'Best Pizza in Town'),
    (3, 'Gift Basket'),
    (4, 'Custom Art');
```

Now when we want to produce an actual auction record, we can join against items like so
```sql
-- View that mirrors the `auctions` table from our load generator.
CREATE VIEW auctions AS
SELECT id, seller, items.item, end_time
FROM auctions_core, items
WHERE auction.item = items.id;
```

We've now got a view `auctions` that mirrors what Materialize's load generator produces, at least superficially.

#### Introducing Foreign Key Constraints

Each bid in `bids` references an auction, and we are unlikely to find an extant auction if we just use random numbers for `auction_id`.
We'd like to base our `bids` on the available auctions, and have them occur at times that make sense for the auction.

We can accomplish this by deriving the bids for an auction from `auctions` itself.
We will use some available pseudorandomness to propose a number of bids, and then create further pseudorandomness to determine the details of each bid.
```sql
CREATE VIEW bids AS
-- Establish per-bid records and pseudorandomness.
WITH prework AS (
    -- Create `get_byte(random, 6)` many bids for each auction, 
    -- each with their own freshly generated pseudorandomness.
    SELECT 
        id as auction_id,
        moment as auction_start,
        end_time as auction_end,
        digest(random::text || generate_series(1, get_byte(random, 6))::text, 'md5') as random
    FROM auctions_core
)
SELECT
    get_byte(random, 0) +
    get_byte(random, 1) * 256 +
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) AS buyer,
    auction_id,
    get_byte(random, 4)::numeric AS amount,
    auction_start + (get_byte(random, 5)::text || ' seconds')::interval as bid_time
FROM prework;
```

We now have a pile of bids for each auction, with the compelling property that when the auction goes away so too do its bids.
This gives us 'referential integrity', the property of foreign keys (`bids.auction_id`) that their referent (`auction.id`) is always valid.

And with this, we have generated the `auctions` and `bids` data that continually change, but always make sense.

There are several other changes you might want to make!
For example, random bids means that auctions stop changing as they go on, because new random bids are unlikely to beat all prior bids.
You could instead have the bids trend up with time, to keep the data interesting.
But, the changes are pretty easy to roll out, and just amount to editing the SQL that defines them.

Let's pause for now on noodling on ways we could make the data even more realistic.
Up next we have to unpack how we got that `moments` view in the first place.
Once we've done that, you are welcome to go back to playing around with load generator novelties and variations!

### Operational Data from Thin Air

Our `auctions` and `bids` data was based on a view `moments` that showed us all timestamps within the past three hours.
We saw how we could go from that to pretty much anything, through extracted pseudorandomness.

We used a view that seemed maybe too easy, that looked roughly like so:
```sql
-- Generate a sliding window over timestamp data.
-- Arguments: <volume>, <velocity>
SELECT moment,
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    <velocity>
) moment
WHERE now() BETWEEN moment AND moment + <volume>;
```

This example uses `generate_series` to produce moments at which events will occur.
The `<velocity>` argument chooses the step size of the `generate_series` call, and locks in the cadence of updates.
The `<volume>` argument controls for how long each record lingers, and sets the steady state size.
The result is a sliding window over random data, where you get to control the volume and velocity.

We used `'1 second'` for the velocity and `'1 day'` for the volume.

Now, while you can *type* the above, it won't actually run properly if you press enter.
The query describes 130 years of data, probably at something like a one second update frequency (because you wanted live data, right?).
I don't even know how to determine how many records this is accurately based on all the leap-action that occurs.
Moreover, you won't be able to materialize this view, because `now()` prevents materializations.

To actually get this to work, we'll have to use some clever tricks.
The coming subsections are a sequence of such tricks, and the punchline will be 'it works!', in case that saves you any time.

#### Clever trick 1: using `mz_now()`

Our first clever trick is to move from `now()` to `mz_now()`.
These are very similar functions, where the `now()` function gets you the contents of the system clock, and `mz_now()` gets you the transaction time of your command.
The main difference between the two is that we can materialize some queries containing `mz_now()`, unlike any query containing `now()`.

```sql
-- Generate a sliding window over timestamp data.
SELECT moment,
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    '1 second'
) moment
--    /------\---- LOOK HERE!
WHERE mz_now() BETWEEN moment AND moment + '1 day';
```
This very simple change means that Materialize now has the ability to keep the query up to date.
Materialize has a feature called ['temporal filters'](https://materialize.com/docs/transform-data/patterns/temporal-filters/) that allows `mz_now()` in `WHERE` clauses, because we are able to invert the clause and see the moment (Materialize time) at which changes will occur.

Unfortunately, the implementation strategy for keeping this view up to date still involves first producing all the data, and then filtering it (we don't have any magical insight into `generate_series` that allows us to invert its implementation).
But fortunately, we have other clever tricks available to us.

#### Clever trick 2: Hierachical Generation

The problem above is that we generate all the data at once, and then filter it.
We could instead generate the years of interest, from them the days of interest, from them the hours of interest, then minutes of interest, then seconds of interest, and finally milliseconds of interest.
In a sense we are generating *intervals* rather than *moments*, and then producing moments from the intervals.

Let's start by generating all the years we might be interested in.
We start with all the years we might reasonably need, and a `WHERE` clause that checks for intersection of the interval (`+ '1 year'`) and the extension by volume (`+ '1 day'`).
```sql
-- Each year-long interval of interest
CREATE VIEW years AS
SELECT * 
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    '1 year') year
WHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';
```
This view does not have all that many years in it. 
Roughly 130 of them.
Few enough that we can filter them down, and get to work on days.

At this point, we'll repeatedly refine the intervals by subdividing into the next granularity.
We'll do this for years into days, but you'll have to use your imagination for the others.
We have all the SQL at the end, so don't worry that you'll miss out on that.
```sql
-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(
        year, 
        year + '1 year' - '1 day'::interval, 
        '1 day') as day
    FROM years
)
WHERE mz_now() BETWEEN day AND day + '1 day';
```
We'll repeat this on to a view `seconds`, and stop there.

Although we could continue to milliseconds, experience has been that it's hard to demo things changing that quickly through SQL.
Lines of text flow past like the Matrix, and all you can really see is that there is change, not what the change is.

Unfortunately, there is a final gotcha.
Materialize is too clever by half, and if you materialize the `seconds` view, it will see that it is able to determine the entire 130 year timeline of the view, history and future, and record it for you.
At great expense.
These declarative systems are sometimes just too smart.

#### Clever trick 3: An empty table

We can fix everything by introducing an empty table.

The empty table is only present to ruin Materialize's ability to be certain it already knows the right answer about the future.
We'll introduce it to each of our views in the same place, and its only function is to menace Materialize with the possibility that it *could* contain data.
But it won't.
But we wont tell Materialize that.

```sql
-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(
        year, 
        year + '1 year' - '1 day'::interval, 
        '1 day') as day
    FROM years
    -- THIS NEXT LINE IS NEW!!
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN day AND day + '1 day';
```

With these tricks in hand, we now have the ability to spin it up and see what it looks like.

```sql
CREATE DEFAULT INDEX ON days;
```

We'll want to create the same default indexes on our other views: `hours`, `minutes`, and `seconds`.
Importantly, we want to create them in this order, also, to make sure that each relies on the one before it.
If they did not, we would be back in the world of the previous section, where each would read ahead until the end of time (the year 2099, in this example).

#### Finishing touches

As a final bit of housekeeping, we'll want to go from intervals back to moments, with some additional inequalities.
```sql
-- The final view we'll want to use.
CREATE VIEW moments AS
SELECT second AS moment FROM seconds
WHERE mz_now() >= second
  AND mz_now() < second + '1 day';
```
The only change here is the `mz_now()` inequality, which now avoids `BETWEEN` because it has inclusive upper bounds.
The result is now a view that always has exactly 24 * 60 * 60 = 86400 elements in it.
We can verify this by subscribing to the changelog of the count query:
```sql
-- Determine the count and monitor its changes.
COPY (
    SUBSCRIBE (SELECT COUNT(*) FROM moments) 
    WITH (progress = true)
)
TO stdout;
```
This reports an initial value of 86400, and then repeatedly reports (second by second) that there are no additional changes.
```
materialize=> COPY (
    SUBSCRIBE (SELECT COUNT(*) FROM moments) 
    WITH (progress = true)
)
TO stdout;
1716210913609	t	\N	\N
1716210913609	f	1	86400
1716210914250	t	\N	\N
1716210914264	t	\N	\N
1716210914685	t	\N	\N
1716210915000	t	\N	\N
1716210915684	t	\N	\N
1716210916000	t	\N	\N
1716210916248	t	\N	\N
1716210916288	t	\N	\N
1716210916330	t	\N	\N
1716210916683	t	\N	\N
^CCancel request sent
ERROR:  canceling statement due to user request
materialize=> 
```
All rows with a second column of `t` are 'progress' statements rather than data updates.
The second row, the only one with a `f`, confirms a single record (`1`) with a value of `86400`.

Yeah, that's it! The only thing left is to read a wall of text containing all the SQL.
Actually, I recommend bouncing up to the start of the post again, and confirming that the pieces fit together for you.
It's also a fine time to [try out Materialize](https://materialize.com/register/), the only system that can run all of these views. 

### Appendix: All the SQL

```sql
CREATE TABLE empty (e TIMESTAMP);

-- Supporting view to translate ids into text.
CREATE VIEW items (id, item) AS VALUES
    (0, 'Signed Memorabilia'),
    (1, 'City Bar Crawl'),
    (2, 'Best Pizza in Town'),
    (3, 'Gift Basket'),
    (4, 'Custom Art');

-- Each year-long interval of interest
CREATE VIEW years AS
SELECT * 
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    '1 year') year
WHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';

-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(year, year + '1 year' - '1 day'::interval, '1 day') as day
    FROM years
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN day AND day + '1 day' + '1 day';

-- Each hour-long interval of interest
CREATE VIEW hours AS
SELECT * FROM (
    SELECT generate_series(day, day + '1 day' - '1 hour'::interval, '1 hour') as hour
    FROM days
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN hour AND hour + '1 hour' + '1 day';

-- Each minute-long interval of interest
CREATE VIEW minutes AS
SELECT * FROM (
    SELECT generate_series(hour, hour + '1 hour' - '1 minute'::interval, '1 minute') AS minute
    FROM hours
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN minute AND minute + '1 minute' + '1 day';

-- Any second-long interval of interest
CREATE VIEW seconds AS
SELECT * FROM (
    SELECT generate_series(minute, minute + '1 minute' - '1 second'::interval, '1 second') as second
    FROM minutes
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN second AND second + '1 second' + '1 day';

-- Indexes are important to ensure we expand intervals carefully.
CREATE DEFAULT INDEX ON years;
CREATE DEFAULT INDEX ON days;
CREATE DEFAULT INDEX ON hours;
CREATE DEFAULT INDEX ON minutes;
CREATE DEFAULT INDEX ON seconds;

-- The final view we'll want to use .
CREATE VIEW moments AS
SELECT second AS moment FROM seconds
WHERE mz_now() >= second
  AND mz_now() < second + '1 day';

-- Extract pseudorandom bytes from each moment.
CREATE VIEW random AS
SELECT moment, digest(moment::text, 'md5') as random
FROM moments;

-- Present as auction 
CREATE VIEW auctions_core AS
SELECT 
    moment,
    random,
    get_byte(random, 0) + 
    get_byte(random, 1) * 256 + 
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) +
    get_byte(random, 4) * 256 as seller,
    get_byte(random, 5) as item,
    -- Have each auction expire after up to 256 minutes.
    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time
FROM random;

-- Refine and materialize auction data.
CREATE MATERIALIZED VIEW auctions AS
SELECT auctions_core.id, seller, items.item, end_time
FROM auctions_core, items
WHERE auctions_core.item % 5 = items.id;

-- Create and materialize bid data.
CREATE MATERIALIZED VIEW bids AS
-- Establish per-bid records and randomness.
WITH prework AS (
    SELECT 
        id AS auction_id,
        moment as auction_start,
        end_time as auction_end,
        digest(random::text || generate_series(1, get_byte(random, 5))::text, 'md5') as random
    FROM auctions_core
)
SELECT 
    get_byte(random, 0) + 
    get_byte(random, 1) * 256 + 
    get_byte(random, 2) * 65536 as id, 
    get_byte(random, 3) +
    get_byte(random, 4) * 256 AS buyer,
    auction_id,
    get_byte(random, 5)::numeric AS amount,
    auction_start + (get_byte(random, 6)::text || ' minutes')::interval as bid_time
FROM prework;
```
<!-- ##{'timestamp':1716094800}## -->

ã€‚">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nuowoo.github.io/blog/post/Demonstrating%20Operational%20Data%20with%20SQL.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>Demonstrating Operational Data with SQL</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">Demonstrating Operational Data with SQL</h1>
<div class="title-right">
    <a href="https://nuowoo.github.io/blog" id="buttonHome" class="btn btn-invisible circle" title="é¦–é¡µ">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/nuowoo/blog/issues/2" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="åˆ‡æ¢ä¸»é¢˜">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p>Databases, Big Data, and Stream Processors have long had the property that it can be hard to <em>demonstrate</em> their value, like in a demo setting.<br>
Databases coordinate the work of multiple teams of independent workers, and don't shine when there is just one user.<br>
Big Data systems introduce scalable patterns that can be purely overhead when the data fit on a single laptop.<br>
Stream Processors aim to get the lowest of end-to-end latencies, but do nothing of any consequence on static data.<br>
These systems demonstrate value when you have variety, volume, and velocity, and most demo data sets have none of these.</p>
<p>Materialize, an operational data warehouse backed by scalable streaming systems, has all three of these challenges!</p>
<p>Fortunately, Materialize is powerful enough to synthesize its own operational data for demonstration purposes.<br>
In this post, we'll build a recipe for a generic live data source using standard SQL primitives and some Materialize magic.<br>
We'll then add various additional flavors: distributions over keys, irregular validity, foreign key relationships.<br>
It's all based off of Materialize's own <a href="https://materialize.com/docs/sql/create-source/load-generator/#auction" rel="nofollow">auction load generator</a>, but it's written entirely in SQL and something that I can customize as my needs evolve.</p>
<p>The thing I find most amazing here is that with just SQL you can create <em>live</em> data.<br>
Data that comes and goes, changes, and respects invariants as it does.<br>
And that the gap between your idea for live data and making it happen is just typing some SQL.</p>
<h3>My Motivation: Materialize</h3>
<p>Materialize has a few product beats it wants to hit when we demo it, derived from our product principles.</p>
<ul>
<li><strong>Responsiveness</strong>: Materialize should be able to get back to you ASAP, even with lots of data involved.</li>
<li><strong>Freshness</strong>: Materialize should reflect arbitrary updates almost immediately, even through complex logic.</li>
<li><strong>Consistency</strong>: Materialize's outputs should always reflect a consistent state, even across multiple users and views.</li>
</ul>
<p>We want to get folks to that "aha!" moment where they realize that Materialize is like no other technology they know of.<br>
Until that moment, Materialize could just be a trenchcoat containing Postgres, Spark, and Flink stacked according to your preferences.</p>
<p>Of course, different contexts connect for different users.<br>
Some folks think about transactions and fraud and want to see how to get in front of that.<br>
Others have users of their own, and know that sluggish, stale, inconsistent results are how they lose their users, and want to feel the lived experience.<br>
Many users won't believe a thing until the data looks like their data, with the same schemas and data distributions, and the same business logic.<br>
These are all legitimate concerns, and to me they speak to the inherent <em>heterogeneity</em> involved in demonstrating something.</p>
<p>I want to be able to demonstrate Materialize more <strong>effectively</strong>, which is some amount tied up in demonstrating it more <strong>flexibly</strong>.</p>
<p>As a personal first, I'm going to try telling the story in reverse order, Memento-style.<br>
We'll start with the outcomes, which I hope will make sense, and then figure out how we got there, and eventually arrive at the wall of SQL that makes it happen.<br>
It does mean we'll need some suspension of disbelief as we go, though; bear with me!<br>
I do hope that whichever prefix you can tolerate makes sense and is engaging, and am only certain that if we started with the SQL it would not be.</p>
<p>The outine is, roughly:</p>
<ol>
<li>
<p><a href="https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#demonstrating-materialize">Demonstrating Materialize with auction data</a></p>
<p>We'll work through Materialize's quick start to show off <code class="notranslate">auctions</code> and <code class="notranslate">bids</code> data, and give a feel for what we need to have our live data do.<br>
We're going to hit the beats of responsiveness, freshness, and consistency along the way.</p>
</li>
<li>
<p><a href="https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#auction-data-from-changing-moments">Building an Auction loadgen from unrelated live data</a></p>
<p>Here we'll build live views that define <code class="notranslate">auctions</code> and <code class="notranslate">bids</code>, starting from a live view that just contains recent timestamps.<br>
We'll see how to turn largely nonsense data into plausible auctions and bids, through the magic of pseudorandomness.</p>
</li>
<li>
<p><a href="https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#operational-data-from-thin-air">Building live random data from just SQL</a></p>
<p>Starting from nothing more than SQL, we'll create a live view that Materialize can maintain containing recent moments as timestamps.<br>
As time continually moves forward, those moments continually change.</p>
</li>
<li>
<p><a href="https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#appendix-all-the-sql">All the SQL</a> Really, just SQL.</p>
</li>
</ol>
<p>Feel more than welcome to leap to the sections that interest you most.<br>
I recommend starting at the beginning, though!</p>
<h3>Demonstrating Materialize</h3>
<p>Let's sit down with Materialize and some live auction data and see if we can't hit the beats of responsiveness, freshness, and consistency.<br>
The story is borrowed from our own quickstart, but by the end of it we'll find we've swapped out the quickstart's built-in load generator.</p>
<p>Materialize's <a href="https://materialize.com/docs/sql/create-source/load-generator/#auction" rel="nofollow"><code class="notranslate">AUCTION</code> load generator</a> populates <code class="notranslate">auctions</code> and <code class="notranslate">bids</code> tables.<br>
Their contents look roughly like so:</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select * from auctions;
 id | seller |        item        |          end_time          
----+--------+--------------------+----------------------------
  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00
  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00
  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00
  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00
  ...
(4 rows)
</code></pre>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select * from bids;
 id | buyer | auction_id | amount |          bid_time          
----+-------+------------+--------+----------------------------
 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00
 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00
 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00
 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00
 ...
</code></pre>
<p>We will root around in this data, as it changes, and show off Materialize as something unlike other data tools.<br>
Specifically we'll want to show off responsiveness, freshness, and consistency, which we'll do in that order.<br>
However, the point is that you get them all at the same time, rather than one at a time, and by the end we should be able to see all three at once.</p>
<h4>Beat 1: Responsiveness</h4>
<p>Materialize is able to respond immediately, even to complex queries over large volumes of data.<br>
Let's start by looking at the data, counting the number of auctions and the number of bids.</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select count(*) from auctions;
 count 
-------
 86400
(1 row)

Time: 52.580 ms
</code></pre>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select count(*) from bids;
  count   
----------
 10994252
(1 row)

Time: 8139.897 ms (00:08.140)
</code></pre>
<p>It's almost 100k auctions, and over 10M bids across them.<br>
The specific numbers will make more sense when we get to the generator, but some of you may already recognize 86,400.<br>
Ten seconds to count ten million things is not great, but this is running on our smallest instance (<code class="notranslate">25cc</code>; roughly 1/4 of a core).<br>
Also, we aren't yet using Materialize's super-power to <em>maintain</em> results.</p>
<p>Materialize maintains computed results in indexes, created via the <code class="notranslate">CREATE INDEX</code> command.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Maintain bids indexed by id.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">INDEX</span> <span class="pl-en">bids_id</span> <span class="pl-k">ON</span> bids (id);</pre></div>
<p>When we want to find a specific bid by id, this can be very fast .</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select * from bids where id = 4;
 id | buyer | auction_id | amount |        bid_time        
----+-------+------------+--------+------------------------
  4 |   228 |    6492730 |    149 | 2024-06-19 13:57:50+00
(1 row)

Time: 19.711 ms
</code></pre>
<p>Inspecting the query history (a feature in Materialize's console) we can see it only took 5ms for the DB, and the additional latency is between NYC and AWS's us-east-1.<br>
This really is just a look-up into a maintained index, admittedly only on <code class="notranslate">bids</code> rather than some sophisticated query.</p>
<p>You can build indexes on any collection of data, not just raw data like <code class="notranslate">bids</code>.<br>
We could build an index on <code class="notranslate">SELECT COUNT(*) FROM bids</code> to make that fast too, for example.<br>
Instead, let's go straight to the good stuff.</p>
<p>Here's a view that determines which auctions are won by which bids.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Determine auction winners: the greatest bid before expiration.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">winning_bids</span> <span class="pl-k">AS</span>
  <span class="pl-k">SELECT DISTINCT</span> <span class="pl-k">ON</span> (<span class="pl-c1">auctions</span>.<span class="pl-c1">id</span>) bids.<span class="pl-k">*</span>,
    <span class="pl-c1">auctions</span>.<span class="pl-c1">item</span>,
    <span class="pl-c1">auctions</span>.<span class="pl-c1">seller</span>
  <span class="pl-k">FROM</span> auctions, bids
  <span class="pl-k">WHERE</span> <span class="pl-c1">auctions</span>.<span class="pl-c1">id</span> <span class="pl-k">=</span> <span class="pl-c1">bids</span>.<span class="pl-c1">auction_id</span>
    <span class="pl-k">AND</span> <span class="pl-c1">bids</span>.<span class="pl-c1">bid_time</span> <span class="pl-k">&lt;</span> <span class="pl-c1">auctions</span>.<span class="pl-c1">end_time</span>
    <span class="pl-k">AND</span> mz_now() <span class="pl-k">&gt;=</span> <span class="pl-c1">auctions</span>.<span class="pl-c1">end_time</span>
  <span class="pl-k">ORDER BY</span> <span class="pl-c1">auctions</span>.<span class="pl-c1">id</span>,
    <span class="pl-c1">bids</span>.<span class="pl-c1">amount</span> <span class="pl-k">DESC</span>,
    <span class="pl-c1">bids</span>.<span class="pl-c1">bid_time</span>,
    <span class="pl-c1">bids</span>.<span class="pl-c1">buyer</span>;</pre></div>
<p>Directly querying this view results in a not-especially-responsive experience</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
        217 |    41 |    252
       3328 |   209 |     55
      19201 |   147 |    255
      18947 |    34 |    254
       7173 |   143 |      5
(5 rows)

Time: 87428.589 ms (01:27.429)
</code></pre>
<p>We are grinding through all the bids from scratch when you select from a view, because the view only explains what query you want to run.<br>
A view by itself doesn't cause any work to be done ahead of time.</p>
<p>However, we can create indexes on <code class="notranslate">winning_bids</code>, and once they are up and running everything gets better.<br>
We are going to create two indexes, on the columns <code class="notranslate">buyer</code> and <code class="notranslate">seller</code>, for future storytelling reasons.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Compute and maintain winning bids, indexed two ways.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">INDEX</span> <span class="pl-en">wins_by_buyer</span> <span class="pl-k">ON</span> winning_bids (buyer);
<span class="pl-k">CREATE</span> <span class="pl-k">INDEX</span> <span class="pl-en">wins_by_seller</span> <span class="pl-k">ON</span> winning_bids (seller);</pre></div>
<p>The auctions aren't faster to magic in to existence than the original query was, so we'll have to wait a moment for them to hydrate.<br>
Once this has happened, you get responsive interactions with the view.</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
    7647534 |     0 |    254
    6568079 |     0 |    239
   10578840 |     0 |    254
   14208479 |     0 |    249
   15263465 |     0 |    199
(5 rows)

Time: 61.283 ms
</code></pre>
<p>Rather than grind over the ten million or so bids to find winners, the ~80,000 results are maintained and its easy to read the first five.<br>
Moreover, the results are all immediately up to date, rather than being fast-but-stale.<br>
Let's hit that <strong>freshness</strong> beat now!</p>

<h4>Beat 2: Freshness</h4>
<p>All of this auction data is synthetic, and while it changes often the show is pretty clearly on rails.<br>
That is, Materialize knows ahead of time what the changes will be.<br>
You want to know that Materialize can respond fast to <em>arbitrary</em> changes, including ones that Materialize doesn't anticipate.</p>
<p>We need <strong>interaction</strong>!</p>
<p>Let's create a table we can modify, through our own whims and fancies.<br>
Our modifications to this table, not part of the load generator, will be how we demonstrate the speed at which Materialize updates results as data change.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Accounts that we might flag for fraud.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">fraud_accounts</span> (id <span class="pl-k">bigint</span>);</pre></div>
<p>Let's look at a query that calls out the top five accounts that win auctions.<br>
We'll subscribe to it, meaning we get to watch the updates as they happen.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Top five non-fraud accounts, by auction wins.</span>
COPY (SUBSCRIBE TO (
  <span class="pl-k">SELECT</span> buyer, <span class="pl-c1">count</span>(<span class="pl-k">*</span>)
  <span class="pl-k">FROM</span> winning_bids
  <span class="pl-k">WHERE</span> buyer NOT <span class="pl-k">IN</span> (<span class="pl-k">SELECT</span> id <span class="pl-k">FROM</span> fraud_accounts)
  <span class="pl-k">GROUP BY</span> buyer
  <span class="pl-k">ORDER BY</span> <span class="pl-c1">count</span>(<span class="pl-k">*</span>) <span class="pl-k">DESC</span>, buyer <span class="pl-k">LIMIT</span> <span class="pl-c1">5</span>
)) TO STDOUT;</pre></div>
<p>This produces first a snapshot and then a continual stream of updates.<br>
In our case, the updates are going to derive from our manipulation of <code class="notranslate">fraud_accounts</code>.</p>
<pre class="notranslate"><code class="notranslate">1718981380562	1	7247	7
1718981380562	1	17519	7
1718981380562	1	27558	7
1718981380562	1	20403	7
1718981380562	1	16584	7
</code></pre>
<p>The data are not really changing much, on account of the winners all having the same counts.<br>
But, this is actually good for us, because we can see what happens when we force a change.</p>
<p>At this point, let's insert the record <code class="notranslate">17519</code> into <code class="notranslate">fraud_accounts</code>.</p>
<pre class="notranslate"><code class="notranslate">-- Mark 17519 as fraudulent
1718981387841	-1	17519	7
1718981387841	1	32134	7
</code></pre>
<p>We can do the same with <code class="notranslate">16584</code>, and then <code class="notranslate">34985</code>.</p>
<pre class="notranslate"><code class="notranslate">-- Mark 16584 as fraudulent
1718981392977	1	34985	7
1718981392977	-1	16584	7
-- Mark 34985 as fraudulent
1718981398158	1	35131	7
1718981398158	-1	34985	7
</code></pre>
<p>Finally, let's remove all records from <code class="notranslate">fraud_accounts</code> and we can see that we return back to the original state.</p>
<pre class="notranslate"><code class="notranslate">-- Remove all fraud indicators.
1718981403087	-1	35131	7
1718981403087	1	17519	7
1718981403087	-1	32134	7
1718981403087	1	16584	7
...
</code></pre>
<p>That <code class="notranslate">34985</code> record isn't mention here because it only showed up due to our other removals.<br>
We don't hear about a change because there is no moment when it is in the top five, even transiently.<br>
That is a great lead-in to Materailize's <strong>consistency</strong> properties!</p>
<h4>Beat 3: Consistency</h4>
<p>All the freshness and responsiveness in the world doesn't mean much if the results are incoherent.<br>
Materialize only ever presents actual results that actually happened, with no transient errors.<br>
When you see results, you can confidently act on them knowing that they are real, and don't need further second to bake.</p>
<p>Let's take a look at consistency through the lens of account balances as auctions close and winning buyers must pay sellers.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Account ids, with credits and debits from auctions sold and won.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">funds_movement</span> <span class="pl-k">AS</span>
  <span class="pl-k">SELECT</span> id,
         <span class="pl-c1">SUM</span>(credits) <span class="pl-k">AS</span> credits,
         <span class="pl-c1">SUM</span>(debits) <span class="pl-k">AS</span> debits
  <span class="pl-k">FROM</span> (
    <span class="pl-k">SELECT</span> seller <span class="pl-k">AS</span> id, amount <span class="pl-k">AS</span> credits, <span class="pl-c1">0</span> <span class="pl-k">AS</span> debits
    <span class="pl-k">FROM</span> winning_bids
    <span class="pl-k">UNION ALL</span>
    <span class="pl-k">SELECT</span> buyer <span class="pl-k">AS</span> id, <span class="pl-c1">0</span> <span class="pl-k">AS</span> credits, amount <span class="pl-k">AS</span> debits
    <span class="pl-k">FROM</span> winning_bids
  )
  <span class="pl-k">GROUP BY</span> id;</pre></div>
<p>These balances derive from the same source: <code class="notranslate">winning_bids</code>, and although they'll vary from account to account, they should all add up.<br>
Specifically, if we get the total credits and the total debits, they should 100% of the time be exactly equal.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Discrepancy between credits and debits.</span>
<span class="pl-k">SELECT</span> <span class="pl-c1">SUM</span>(credits) <span class="pl-k">-</span> <span class="pl-c1">SUM</span>(debits) 
<span class="pl-k">FROM</span> funds_movement;</pre></div>
<p>This query reports zero, 100% of the time.<br>
We can <code class="notranslate">SUBSCRIBE</code> to the query to be notified of any change.</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; COPY (SUBSCRIBE (
    SELECT SUM(credits) - SUM(debits) 
    FROM funds_movement
)) TO STDOUT;

1716312983129	1	0
</code></pre>
<p>This tells us that starting at time <code class="notranslate">1716312983129</code>, there was <code class="notranslate">1</code> record, and it was <code class="notranslate">0</code>.<br>
You can sit there a while, and there will be no changes.<br>
You could also add the <code class="notranslate">WITH (PROGRESS)</code> option, and it will provide regular heartbeats confirming that second-by-second it is still zero.<br>
The credits and debits always add up, and aren't for a moment inconsistent.</p>
<p>We can set up similar views for other assertions.<br>
For example, every account that has sold or won an auction should have a balance.<br>
A SQL query can look for violations of this, and we can monitor it to see that it is always empty.<br>
If it is ever non-empty, perhaps there are bugs in the query logic, its contents are immediately actionable:<br>
there is a specific time where the inputs evaluated to an invariant-violating output, and if you return to that moment you'll see the inputs that produce the bad output.</p>
<p>The consistency extends across multiple independent sessions.<br>
The moment you get confirmation that the insert into <code class="notranslate">fraud_accounts</code>, you can be certain that no one will see that account in the top five non-fraudulent auction winners.<br>
This guarantee is called "strict serializability", that the system behaves as if every event occurred at a specific time between its start and end, and is the strongest guarantee that databases provide.</p>
<h4>Demo over!</h4>
<p>That's it!<br>
We've completed the introduction to Materialize, and used auction data to show off responsiveness, freshness, and consistency.<br>
There's a lot more to show off, of course, and if any of this sounded fascinating you should swing by <a href="https://materialize.com/register/" rel="nofollow">https://materialize.com/register/</a> to spin up a trial environment.</p>
<p>However, in this post we will continue to unpack how we got all of that <code class="notranslate">auctions</code> and <code class="notranslate">bids</code> data in the first place!</p>
<h3>Auction Data from Changing Moments</h3>
<p>Where do the <code class="notranslate">auctions</code> and <code class="notranslate">bids</code> data come from?<br>
You can get them from our load generator, but we're going to try and coax them out of raw SQL.<br>
We're going to start with something we haven't introduced yet, but it's a view whose content looks like this</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> All seconds within the past 24 hours.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">moments</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> generate_series(
    now() <span class="pl-k">-</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>::interval <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 second<span class="pl-pds">'</span></span>::interval,
    now(),
    <span class="pl-s"><span class="pl-pds">'</span>1 second<span class="pl-pds">'</span></span>
) moment;</pre></div>
<p>Unpacking this, <code class="notranslate">moments</code> contains rows with a single column containing a timestamp.<br>
Whenever we look at it, the view contains those timestamps at most one day less than <code class="notranslate">now()</code>.<br>
It should have at any moment exactly 86,400 records present, as many as <code class="notranslate">auctions</code> up above.</p>
<p>Importantly, this view definition will not actually work for us.<br>
You are welcome to try it out, but you'll find out that while it can be <em>inspected</em>, it cannot be <em>maintained</em>.<br>
We'll fix that by the end of the post, but it will need to wait until the next section.<br>
For the moment, let's assume we have this view and the magical ability to keep it up to date.</p>
<p>These "moments" are not auction data, though.<br>
How do we get from moments to auctions and bids?</p>
<p>The <code class="notranslate">auctions</code> and <code class="notranslate">bids</code> collections look roughly like so:</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select * from auctions;
 id | seller |        item        |          end_time          
----+--------+--------------------+----------------------------
  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00
  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00
  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00
  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00
  ...
(4 rows)
</code></pre>
<pre class="notranslate"><code class="notranslate">materialize=&gt; select * from bids;
 id | buyer | auction_id | amount |          bid_time          
----+-------+------------+--------+----------------------------
 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00
 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00
 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00
 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00
 ...
</code></pre>
<p>Auctions have a unique id, a seller id, an item description, and an end time.<br>
Bids have a unique id (no relation), a buyer id, an auction id, the amount of the bid, and the time of the bid.</p>
<p>The <code class="notranslate">seller</code>, <code class="notranslate">item</code>, <code class="notranslate">buyer</code>, and <code class="notranslate">amount</code> fields are all random, within some bounds.<br>
As a first cut, we'll think about just using random values for each of the columns.<br>
Where might we get randomness, you ask?<br>
Well, if <em>pseudo</em>-randomness is good enough (it will be), we can use cryptographic hashes of the moments.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Extract pseudorandom bytes from each moment.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">random</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> moment, digest(moment::<span class="pl-k">text</span>, <span class="pl-s"><span class="pl-pds">'</span>md5<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> random
<span class="pl-k">FROM</span> moments;</pre></div>
<p>Let's start with bytes from <code class="notranslate">random</code> to populate columns, and we'd have a first cut at random data.<br>
Columns like <code class="notranslate">auctions.item</code> are populated by joining with a constant collection (part of the generator), but <code class="notranslate">id</code> and <code class="notranslate">seller</code> could just be random.<br>
The <code class="notranslate">end_time</code> we'll pick to be a random time up to 256 minutes after the auction starts.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Totally accurate auction generator.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">auctions_core</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> 
    moment,
    random,
    get_byte(random, <span class="pl-c1">0</span>) <span class="pl-k">+</span> 
    get_byte(random, <span class="pl-c1">1</span>) <span class="pl-k">*</span> <span class="pl-c1">256</span> <span class="pl-k">+</span> 
    get_byte(random, <span class="pl-c1">2</span>) <span class="pl-k">*</span> <span class="pl-c1">65536</span> <span class="pl-k">as</span> id,
    get_byte(random, <span class="pl-c1">3</span>) <span class="pl-k">+</span>
    get_byte(random, <span class="pl-c1">4</span>) <span class="pl-k">*</span> <span class="pl-c1">256</span> <span class="pl-k">as</span> seller,
    get_byte(random, <span class="pl-c1">5</span>) <span class="pl-k">as</span> item,
    <span class="pl-c"><span class="pl-c">--</span> Have each auction expire after up to 256 minutes.</span>
    moment <span class="pl-k">+</span> (get_byte(random, <span class="pl-c1">6</span>)::<span class="pl-k">text</span> <span class="pl-k">||</span> <span class="pl-s"><span class="pl-pds">'</span> minutes<span class="pl-pds">'</span></span>)::interval <span class="pl-k">as</span> end_time
<span class="pl-k">FROM</span> random;</pre></div>
<p>We've clearly made some calls about how random each of these should be, and those calls influence what we'll see in the data.<br>
For example, we've established at most 65,536 sellers, which lines up fine with our 86,400 auctions at any moment; some sellers will have multiple auctions and many will not.<br>
Auctions are open for a few hours on average, close out but linger, and then vanish after 24 hours.<br>
If we want to change any of these, perhaps to add more distinct items, or keep auctions running longer, or to skew the distribution over sellers, we can!</p>
<p>Similarly, the columns of <code class="notranslate">bids</code> are also pretty random, but columns like <code class="notranslate">auction_id</code> and <code class="notranslate">bid_time</code> do need to have some relationship to <code class="notranslate">auctions</code> and the referenced auction.<br>
We'll build those out in just a moment, but have a bit more tidying to do for <code class="notranslate">auctions</code> first.</p>
<h4>Adding Custom Expiration</h4>
<p>Our auctions wind down after some random amount of time, but they are not removed from <code class="notranslate">auctions</code> for three hours.<br>
Thematically we can think of this as auctions whose winners have been locked in, but whose accounts have not yet been settled.</p>
<p>If we want the auction to vanish from <code class="notranslate">auctions</code> at this time it closed, we could accomplish this with a temporal filter:</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-k">WHERE</span> mz_now() <span class="pl-k">&lt;</span> end_time</pre></div>
<p>As soon as we reach <code class="notranslate">end_time</code> the auction would vanish from <code class="notranslate">auctions</code>.</p>
<p>This is a very helpful pattern for load generators that want to control when data arrive and when it departs, in finer detail than "a twenty four hour window".<br>
For example, one could randomly generate <code class="notranslate">insert_ts</code> and <code class="notranslate">delete_ts</code>, and then use</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Create an event that is live for the interval `[insert_ts, delete_ts]`.</span>
<span class="pl-k">WHERE</span> mz_now() BETWEEN insert_ts <span class="pl-k">AND</span> delete_ts</pre></div>
<p>This pattern allows careful control of when events <em>appear</em> to occur, by holding them back until <code class="notranslate">mz_now()</code> reaches a value, and then retracting them when it reaches a later value.</p>
<h4>Making More Realistic Data</h4>
<p>Our random numbers for <code class="notranslate">item</code> aren't nearly as nice as what the existing load generator produces.<br>
However, we can get the same results by putting those nice values in a view and using our integer <code class="notranslate">item</code> to join against the view.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> A static view giving names to items.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">items</span> (id, item) <span class="pl-k">AS</span> <span class="pl-k">VALUES</span>
    (<span class="pl-c1">0</span>, <span class="pl-s"><span class="pl-pds">'</span>Signed Memorabilia<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">1</span>, <span class="pl-s"><span class="pl-pds">'</span>City Bar Crawl<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">2</span>, <span class="pl-s"><span class="pl-pds">'</span>Best Pizza in Town<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">3</span>, <span class="pl-s"><span class="pl-pds">'</span>Gift Basket<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">4</span>, <span class="pl-s"><span class="pl-pds">'</span>Custom Art<span class="pl-pds">'</span></span>);</pre></div>
<p>Now when we want to produce an actual auction record, we can join against items like so</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> View that mirrors the `auctions` table from our load generator.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">auctions</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> id, seller, <span class="pl-c1">items</span>.<span class="pl-c1">item</span>, end_time
<span class="pl-k">FROM</span> auctions_core, items
<span class="pl-k">WHERE</span> <span class="pl-c1">auction</span>.<span class="pl-c1">item</span> <span class="pl-k">=</span> <span class="pl-c1">items</span>.<span class="pl-c1">id</span>;</pre></div>
<p>We've now got a view <code class="notranslate">auctions</code> that mirrors what Materialize's load generator produces, at least superficially.</p>
<h4>Introducing Foreign Key Constraints</h4>
<p>Each bid in <code class="notranslate">bids</code> references an auction, and we are unlikely to find an extant auction if we just use random numbers for <code class="notranslate">auction_id</code>.<br>
We'd like to base our <code class="notranslate">bids</code> on the available auctions, and have them occur at times that make sense for the auction.</p>
<p>We can accomplish this by deriving the bids for an auction from <code class="notranslate">auctions</code> itself.<br>
We will use some available pseudorandomness to propose a number of bids, and then create further pseudorandomness to determine the details of each bid.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">bids</span> <span class="pl-k">AS</span>
<span class="pl-c"><span class="pl-c">--</span> Establish per-bid records and pseudorandomness.</span>
WITH prework <span class="pl-k">AS</span> (
    <span class="pl-c"><span class="pl-c">--</span> Create `get_byte(random, 6)` many bids for each auction, </span>
    <span class="pl-c"><span class="pl-c">--</span> each with their own freshly generated pseudorandomness.</span>
    <span class="pl-k">SELECT</span> 
        id <span class="pl-k">as</span> auction_id,
        moment <span class="pl-k">as</span> auction_start,
        end_time <span class="pl-k">as</span> auction_end,
        digest(random::<span class="pl-k">text</span> <span class="pl-k">||</span> generate_series(<span class="pl-c1">1</span>, get_byte(random, <span class="pl-c1">6</span>))::<span class="pl-k">text</span>, <span class="pl-s"><span class="pl-pds">'</span>md5<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> random
    <span class="pl-k">FROM</span> auctions_core
)
<span class="pl-k">SELECT</span>
    get_byte(random, <span class="pl-c1">0</span>) <span class="pl-k">+</span>
    get_byte(random, <span class="pl-c1">1</span>) <span class="pl-k">*</span> <span class="pl-c1">256</span> <span class="pl-k">+</span>
    get_byte(random, <span class="pl-c1">2</span>) <span class="pl-k">*</span> <span class="pl-c1">65536</span> <span class="pl-k">as</span> id,
    get_byte(random, <span class="pl-c1">3</span>) <span class="pl-k">AS</span> buyer,
    auction_id,
    get_byte(random, <span class="pl-c1">4</span>)::<span class="pl-k">numeric</span> <span class="pl-k">AS</span> amount,
    auction_start <span class="pl-k">+</span> (get_byte(random, <span class="pl-c1">5</span>)::<span class="pl-k">text</span> <span class="pl-k">||</span> <span class="pl-s"><span class="pl-pds">'</span> seconds<span class="pl-pds">'</span></span>)::interval <span class="pl-k">as</span> bid_time
<span class="pl-k">FROM</span> prework;</pre></div>
<p>We now have a pile of bids for each auction, with the compelling property that when the auction goes away so too do its bids.<br>
This gives us "referential integrity", the property of foreign keys (<code class="notranslate">bids.auction_id</code>) that their referent (<code class="notranslate">auction.id</code>) is always valid.</p>
<p>And with this, we have generated the <code class="notranslate">auctions</code> and <code class="notranslate">bids</code> data that continually change, but always make sense.</p>
<p>There are several other changes you might want to make!<br>
For example, random bids means that auctions stop changing as they go on, because new random bids are unlikely to beat all prior bids.<br>
You could instead have the bids trend up with time, to keep the data interesting.<br>
But, the changes are pretty easy to roll out, and just amount to editing the SQL that defines them.</p>
<p>Let's pause for now on noodling on ways we could make the data even more realistic.<br>
Up next we have to unpack how we got that <code class="notranslate">moments</code> view in the first place.<br>
Once we've done that, you are welcome to go back to playing around with load generator novelties and variations!</p>
<h3>Operational Data from Thin Air</h3>
<p>Our <code class="notranslate">auctions</code> and <code class="notranslate">bids</code> data was based on a view <code class="notranslate">moments</code> that showed us all timestamps within the past three hours.<br>
We saw how we could go from that to pretty much anything, through extracted pseudorandomness.</p>
<p>We used a view that seemed maybe too easy, that looked roughly like so:</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Generate a sliding window over timestamp data.</span>
<span class="pl-c"><span class="pl-c">--</span> Arguments: &lt;volume&gt;, &lt;velocity&gt;</span>
<span class="pl-k">SELECT</span> moment,
<span class="pl-k">FROM</span> generate_series(
    <span class="pl-s"><span class="pl-pds">'</span>1970-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-s"><span class="pl-pds">'</span>2099-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-k">&lt;</span>velocity<span class="pl-k">&gt;</span>
) moment
<span class="pl-k">WHERE</span> now() BETWEEN moment <span class="pl-k">AND</span> moment <span class="pl-k">+</span> <span class="pl-k">&lt;</span>volume<span class="pl-k">&gt;</span>;</pre></div>
<p>This example uses <code class="notranslate">generate_series</code> to produce moments at which events will occur.<br>
The <code class="notranslate">&lt;velocity&gt;</code> argument chooses the step size of the <code class="notranslate">generate_series</code> call, and locks in the cadence of updates.<br>
The <code class="notranslate">&lt;volume&gt;</code> argument controls for how long each record lingers, and sets the steady state size.<br>
The result is a sliding window over random data, where you get to control the volume and velocity.</p>
<p>We used <code class="notranslate">'1 second'</code> for the velocity and <code class="notranslate">'1 day'</code> for the volume.</p>
<p>Now, while you can <em>type</em> the above, it won't actually run properly if you press enter.<br>
The query describes 130 years of data, probably at something like a one second update frequency (because you wanted live data, right?).<br>
I don't even know how to determine how many records this is accurately based on all the leap-action that occurs.<br>
Moreover, you won't be able to materialize this view, because <code class="notranslate">now()</code> prevents materializations.</p>
<p>To actually get this to work, we'll have to use some clever tricks.<br>
The coming subsections are a sequence of such tricks, and the punchline will be "it works!", in case that saves you any time.</p>
<h4>Clever trick 1: using <code class="notranslate">mz_now()</code></h4>
<p>Our first clever trick is to move from <code class="notranslate">now()</code> to <code class="notranslate">mz_now()</code>.<br>
These are very similar functions, where the <code class="notranslate">now()</code> function gets you the contents of the system clock, and <code class="notranslate">mz_now()</code> gets you the transaction time of your command.<br>
The main difference between the two is that we can materialize some queries containing <code class="notranslate">mz_now()</code>, unlike any query containing <code class="notranslate">now()</code>.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Generate a sliding window over timestamp data.</span>
<span class="pl-k">SELECT</span> moment,
<span class="pl-k">FROM</span> generate_series(
    <span class="pl-s"><span class="pl-pds">'</span>1970-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-s"><span class="pl-pds">'</span>2099-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-s"><span class="pl-pds">'</span>1 second<span class="pl-pds">'</span></span>
) moment
<span class="pl-c"><span class="pl-c">--</span>    /------\---- LOOK HERE!</span>
<span class="pl-k">WHERE</span> mz_now() BETWEEN moment <span class="pl-k">AND</span> moment <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;</pre></div>
<p>This very simple change means that Materialize now has the ability to keep the query up to date.<br>
Materialize has a feature called <a href="https://materialize.com/docs/transform-data/patterns/temporal-filters/" rel="nofollow">"temporal filters"</a> that allows <code class="notranslate">mz_now()</code> in <code class="notranslate">WHERE</code> clauses, because we are able to invert the clause and see the moment (Materialize time) at which changes will occur.</p>
<p>Unfortunately, the implementation strategy for keeping this view up to date still involves first producing all the data, and then filtering it (we don't have any magical insight into <code class="notranslate">generate_series</code> that allows us to invert its implementation).<br>
But fortunately, we have other clever tricks available to us.</p>
<h4>Clever trick 2: Hierachical Generation</h4>
<p>The problem above is that we generate all the data at once, and then filter it.<br>
We could instead generate the years of interest, from them the days of interest, from them the hours of interest, then minutes of interest, then seconds of interest, and finally milliseconds of interest.<br>
In a sense we are generating <em>intervals</em> rather than <em>moments</em>, and then producing moments from the intervals.</p>
<p>Let's start by generating all the years we might be interested in.<br>
We start with all the years we might reasonably need, and a <code class="notranslate">WHERE</code> clause that checks for intersection of the interval (<code class="notranslate">+ '1 year'</code>) and the extension by volume (<code class="notranslate">+ '1 day'</code>).</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Each year-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">years</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> 
<span class="pl-k">FROM</span> generate_series(
    <span class="pl-s"><span class="pl-pds">'</span>1970-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-s"><span class="pl-pds">'</span>2099-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-s"><span class="pl-pds">'</span>1 year<span class="pl-pds">'</span></span>) year
<span class="pl-k">WHERE</span> mz_now() BETWEEN year <span class="pl-k">AND</span> year <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 year<span class="pl-pds">'</span></span> <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;</pre></div>
<p>This view does not have all that many years in it.<br>
Roughly 130 of them.<br>
Few enough that we can filter them down, and get to work on days.</p>
<p>At this point, we'll repeatedly refine the intervals by subdividing into the next granularity.<br>
We'll do this for years into days, but you'll have to use your imagination for the others.<br>
We have all the SQL at the end, so don't worry that you'll miss out on that.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Each day-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">days</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> (
    <span class="pl-k">SELECT</span> generate_series(
        year, 
        year <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 year<span class="pl-pds">'</span></span> <span class="pl-k">-</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>::interval, 
        <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> day
    <span class="pl-k">FROM</span> years
)
<span class="pl-k">WHERE</span> mz_now() BETWEEN day <span class="pl-k">AND</span> day <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;</pre></div>
<p>We'll repeat this on to a view <code class="notranslate">seconds</code>, and stop there.</p>
<p>Although we could continue to milliseconds, experience has been that it's hard to demo things changing that quickly through SQL.<br>
Lines of text flow past like the Matrix, and all you can really see is that there is change, not what the change is.</p>
<p>Unfortunately, there is a final gotcha.<br>
Materialize is too clever by half, and if you materialize the <code class="notranslate">seconds</code> view, it will see that it is able to determine the entire 130 year timeline of the view, history and future, and record it for you.<br>
At great expense.<br>
These declarative systems are sometimes just too smart.</p>
<h4>Clever trick 3: An empty table</h4>
<p>We can fix everything by introducing an empty table.</p>
<p>The empty table is only present to ruin Materialize's ability to be certain it already knows the right answer about the future.<br>
We'll introduce it to each of our views in the same place, and its only function is to menace Materialize with the possibility that it <em>could</em> contain data.<br>
But it won't.<br>
But we wont tell Materialize that.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Each day-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">days</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> (
    <span class="pl-k">SELECT</span> generate_series(
        year, 
        year <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 year<span class="pl-pds">'</span></span> <span class="pl-k">-</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>::interval, 
        <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> day
    <span class="pl-k">FROM</span> years
    <span class="pl-c"><span class="pl-c">--</span> THIS NEXT LINE IS NEW!!</span>
    <span class="pl-k">UNION ALL</span> <span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> empty
)
<span class="pl-k">WHERE</span> mz_now() BETWEEN day <span class="pl-k">AND</span> day <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;</pre></div>
<p>With these tricks in hand, we now have the ability to spin it up and see what it looks like.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate">CREATE DEFAULT INDEX <span class="pl-k">ON</span> days;</pre></div>
<p>We'll want to create the same default indexes on our other views: <code class="notranslate">hours</code>, <code class="notranslate">minutes</code>, and <code class="notranslate">seconds</code>.<br>
Importantly, we want to create them in this order, also, to make sure that each relies on the one before it.<br>
If they did not, we would be back in the world of the previous section, where each would read ahead until the end of time (the year 2099, in this example).</p>
<h4>Finishing touches</h4>
<p>As a final bit of housekeeping, we'll want to go from intervals back to moments, with some additional inequalities.</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> The final view we'll want to use.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">moments</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> second <span class="pl-k">AS</span> moment <span class="pl-k">FROM</span> seconds
<span class="pl-k">WHERE</span> mz_now() <span class="pl-k">&gt;=</span> second
  <span class="pl-k">AND</span> mz_now() <span class="pl-k">&lt;</span> second <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;</pre></div>
<p>The only change here is the <code class="notranslate">mz_now()</code> inequality, which now avoids <code class="notranslate">BETWEEN</code> because it has inclusive upper bounds.<br>
The result is now a view that always has exactly 24 * 60 * 60 = 86400 elements in it.<br>
We can verify this by subscribing to the changelog of the count query:</p>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-c"><span class="pl-c">--</span> Determine the count and monitor its changes.</span>
COPY (
    SUBSCRIBE (<span class="pl-k">SELECT</span> <span class="pl-c1">COUNT</span>(<span class="pl-k">*</span>) <span class="pl-k">FROM</span> moments) 
    WITH (progress <span class="pl-k">=</span> true)
)
TO stdout;</pre></div>
<p>This reports an initial value of 86400, and then repeatedly reports (second by second) that there are no additional changes.</p>
<pre class="notranslate"><code class="notranslate">materialize=&gt; COPY (
    SUBSCRIBE (SELECT COUNT(*) FROM moments) 
    WITH (progress = true)
)
TO stdout;
1716210913609	t	\N	\N
1716210913609	f	1	86400
1716210914250	t	\N	\N
1716210914264	t	\N	\N
1716210914685	t	\N	\N
1716210915000	t	\N	\N
1716210915684	t	\N	\N
1716210916000	t	\N	\N
1716210916248	t	\N	\N
1716210916288	t	\N	\N
1716210916330	t	\N	\N
1716210916683	t	\N	\N
^CCancel request sent
ERROR:  canceling statement due to user request
materialize=&gt; 
</code></pre>
<p>All rows with a second column of <code class="notranslate">t</code> are "progress" statements rather than data updates.<br>
The second row, the only one with a <code class="notranslate">f</code>, confirms a single record (<code class="notranslate">1</code>) with a value of <code class="notranslate">86400</code>.</p>
<p>Yeah, that's it! The only thing left is to read a wall of text containing all the SQL.<br>
Actually, I recommend bouncing up to the start of the post again, and confirming that the pieces fit together for you.<br>
It's also a fine time to <a href="https://materialize.com/register/" rel="nofollow">try out Materialize</a>, the only system that can run all of these views.</p>
<h3>Appendix: All the SQL</h3>
<div class="highlight highlight-source-sql"><pre class="notranslate"><span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">empty</span> (e <span class="pl-k">TIMESTAMP</span>);

<span class="pl-c"><span class="pl-c">--</span> Supporting view to translate ids into text.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">items</span> (id, item) <span class="pl-k">AS</span> <span class="pl-k">VALUES</span>
    (<span class="pl-c1">0</span>, <span class="pl-s"><span class="pl-pds">'</span>Signed Memorabilia<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">1</span>, <span class="pl-s"><span class="pl-pds">'</span>City Bar Crawl<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">2</span>, <span class="pl-s"><span class="pl-pds">'</span>Best Pizza in Town<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">3</span>, <span class="pl-s"><span class="pl-pds">'</span>Gift Basket<span class="pl-pds">'</span></span>),
    (<span class="pl-c1">4</span>, <span class="pl-s"><span class="pl-pds">'</span>Custom Art<span class="pl-pds">'</span></span>);

<span class="pl-c"><span class="pl-c">--</span> Each year-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">years</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> 
<span class="pl-k">FROM</span> generate_series(
    <span class="pl-s"><span class="pl-pds">'</span>1970-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-s"><span class="pl-pds">'</span>2099-01-01 00:00:00+00<span class="pl-pds">'</span></span>, 
    <span class="pl-s"><span class="pl-pds">'</span>1 year<span class="pl-pds">'</span></span>) year
<span class="pl-k">WHERE</span> mz_now() BETWEEN year <span class="pl-k">AND</span> year <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 year<span class="pl-pds">'</span></span> <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;

<span class="pl-c"><span class="pl-c">--</span> Each day-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">days</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> (
    <span class="pl-k">SELECT</span> generate_series(year, year <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 year<span class="pl-pds">'</span></span> <span class="pl-k">-</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>::interval, <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> day
    <span class="pl-k">FROM</span> years
    <span class="pl-k">UNION ALL</span> <span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> empty
)
<span class="pl-k">WHERE</span> mz_now() BETWEEN day <span class="pl-k">AND</span> day <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span> <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;

<span class="pl-c"><span class="pl-c">--</span> Each hour-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">hours</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> (
    <span class="pl-k">SELECT</span> generate_series(day, day <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span> <span class="pl-k">-</span> <span class="pl-s"><span class="pl-pds">'</span>1 hour<span class="pl-pds">'</span></span>::interval, <span class="pl-s"><span class="pl-pds">'</span>1 hour<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> hour
    <span class="pl-k">FROM</span> days
    <span class="pl-k">UNION ALL</span> <span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> empty
)
<span class="pl-k">WHERE</span> mz_now() BETWEEN hour <span class="pl-k">AND</span> hour <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 hour<span class="pl-pds">'</span></span> <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;

<span class="pl-c"><span class="pl-c">--</span> Each minute-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">minutes</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> (
    <span class="pl-k">SELECT</span> generate_series(hour, hour <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 hour<span class="pl-pds">'</span></span> <span class="pl-k">-</span> <span class="pl-s"><span class="pl-pds">'</span>1 minute<span class="pl-pds">'</span></span>::interval, <span class="pl-s"><span class="pl-pds">'</span>1 minute<span class="pl-pds">'</span></span>) <span class="pl-k">AS</span> minute
    <span class="pl-k">FROM</span> hours
    <span class="pl-k">UNION ALL</span> <span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> empty
)
<span class="pl-k">WHERE</span> mz_now() BETWEEN minute <span class="pl-k">AND</span> minute <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 minute<span class="pl-pds">'</span></span> <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;

<span class="pl-c"><span class="pl-c">--</span> Any second-long interval of interest</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">seconds</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> (
    <span class="pl-k">SELECT</span> generate_series(minute, minute <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 minute<span class="pl-pds">'</span></span> <span class="pl-k">-</span> <span class="pl-s"><span class="pl-pds">'</span>1 second<span class="pl-pds">'</span></span>::interval, <span class="pl-s"><span class="pl-pds">'</span>1 second<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> second
    <span class="pl-k">FROM</span> minutes
    <span class="pl-k">UNION ALL</span> <span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> empty
)
<span class="pl-k">WHERE</span> mz_now() BETWEEN second <span class="pl-k">AND</span> second <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 second<span class="pl-pds">'</span></span> <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;

<span class="pl-c"><span class="pl-c">--</span> Indexes are important to ensure we expand intervals carefully.</span>
CREATE DEFAULT INDEX <span class="pl-k">ON</span> years;
CREATE DEFAULT INDEX <span class="pl-k">ON</span> days;
CREATE DEFAULT INDEX <span class="pl-k">ON</span> hours;
CREATE DEFAULT INDEX <span class="pl-k">ON</span> minutes;
CREATE DEFAULT INDEX <span class="pl-k">ON</span> seconds;

<span class="pl-c"><span class="pl-c">--</span> The final view we'll want to use .</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">moments</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> second <span class="pl-k">AS</span> moment <span class="pl-k">FROM</span> seconds
<span class="pl-k">WHERE</span> mz_now() <span class="pl-k">&gt;=</span> second
  <span class="pl-k">AND</span> mz_now() <span class="pl-k">&lt;</span> second <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>1 day<span class="pl-pds">'</span></span>;

<span class="pl-c"><span class="pl-c">--</span> Extract pseudorandom bytes from each moment.</span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">random</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> moment, digest(moment::<span class="pl-k">text</span>, <span class="pl-s"><span class="pl-pds">'</span>md5<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> random
<span class="pl-k">FROM</span> moments;

<span class="pl-c"><span class="pl-c">--</span> Present as auction </span>
<span class="pl-k">CREATE</span> <span class="pl-k">VIEW</span> <span class="pl-en">auctions_core</span> <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> 
    moment,
    random,
    get_byte(random, <span class="pl-c1">0</span>) <span class="pl-k">+</span> 
    get_byte(random, <span class="pl-c1">1</span>) <span class="pl-k">*</span> <span class="pl-c1">256</span> <span class="pl-k">+</span> 
    get_byte(random, <span class="pl-c1">2</span>) <span class="pl-k">*</span> <span class="pl-c1">65536</span> <span class="pl-k">as</span> id,
    get_byte(random, <span class="pl-c1">3</span>) <span class="pl-k">+</span>
    get_byte(random, <span class="pl-c1">4</span>) <span class="pl-k">*</span> <span class="pl-c1">256</span> <span class="pl-k">as</span> seller,
    get_byte(random, <span class="pl-c1">5</span>) <span class="pl-k">as</span> item,
    <span class="pl-c"><span class="pl-c">--</span> Have each auction expire after up to 256 minutes.</span>
    moment <span class="pl-k">+</span> (get_byte(random, <span class="pl-c1">6</span>)::<span class="pl-k">text</span> <span class="pl-k">||</span> <span class="pl-s"><span class="pl-pds">'</span> minutes<span class="pl-pds">'</span></span>)::interval <span class="pl-k">as</span> end_time
<span class="pl-k">FROM</span> random;

<span class="pl-c"><span class="pl-c">--</span> Refine and materialize auction data.</span>
CREATE MATERIALIZED VIEW auctions <span class="pl-k">AS</span>
<span class="pl-k">SELECT</span> <span class="pl-c1">auctions_core</span>.<span class="pl-c1">id</span>, seller, <span class="pl-c1">items</span>.<span class="pl-c1">item</span>, end_time
<span class="pl-k">FROM</span> auctions_core, items
<span class="pl-k">WHERE</span> <span class="pl-c1">auctions_core</span>.<span class="pl-c1">item</span> % <span class="pl-c1">5</span> <span class="pl-k">=</span> <span class="pl-c1">items</span>.<span class="pl-c1">id</span>;

<span class="pl-c"><span class="pl-c">--</span> Create and materialize bid data.</span>
CREATE MATERIALIZED VIEW bids <span class="pl-k">AS</span>
<span class="pl-c"><span class="pl-c">--</span> Establish per-bid records and randomness.</span>
WITH prework <span class="pl-k">AS</span> (
    <span class="pl-k">SELECT</span> 
        id <span class="pl-k">AS</span> auction_id,
        moment <span class="pl-k">as</span> auction_start,
        end_time <span class="pl-k">as</span> auction_end,
        digest(random::<span class="pl-k">text</span> <span class="pl-k">||</span> generate_series(<span class="pl-c1">1</span>, get_byte(random, <span class="pl-c1">5</span>))::<span class="pl-k">text</span>, <span class="pl-s"><span class="pl-pds">'</span>md5<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> random
    <span class="pl-k">FROM</span> auctions_core
)
<span class="pl-k">SELECT</span> 
    get_byte(random, <span class="pl-c1">0</span>) <span class="pl-k">+</span> 
    get_byte(random, <span class="pl-c1">1</span>) <span class="pl-k">*</span> <span class="pl-c1">256</span> <span class="pl-k">+</span> 
    get_byte(random, <span class="pl-c1">2</span>) <span class="pl-k">*</span> <span class="pl-c1">65536</span> <span class="pl-k">as</span> id, 
    get_byte(random, <span class="pl-c1">3</span>) <span class="pl-k">+</span>
    get_byte(random, <span class="pl-c1">4</span>) <span class="pl-k">*</span> <span class="pl-c1">256</span> <span class="pl-k">AS</span> buyer,
    auction_id,
    get_byte(random, <span class="pl-c1">5</span>)::<span class="pl-k">numeric</span> <span class="pl-k">AS</span> amount,
    auction_start <span class="pl-k">+</span> (get_byte(random, <span class="pl-c1">6</span>)::<span class="pl-k">text</span> <span class="pl-k">||</span> <span class="pl-s"><span class="pl-pds">'</span> minutes<span class="pl-pds">'</span></span>)::interval <span class="pl-k">as</span> bid_time
<span class="pl-k">FROM</span> prework;</pre></div>
</div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">è¯„è®º</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright Â© <span id="copyrightYear"></span> <a href="https://nuowoo.github.io/blog">Computer Scientist</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="ç½‘ç«™è¿è¡Œ"+diffDay+"å¤©"+" â€¢ ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","nuowoo/blog");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>


</html>
