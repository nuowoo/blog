{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "<script type='text/javascript' src='https://udbaa.com/bnr.php?section=General_1&pub=316912&format=728x90&ga=g'></script> <noscript><a href='https://yllix.com/publishers/316912' target='_blank'><img src='//ylx-aff.advertica-cdn.com/pub/728x90.png' style='border:none;margin:0;padding:0;vertical-align:baseline;' alt='ylliX - Online Advertising Network' /></a></noscript> <meta name='monetag' content='07735af43d5282f24e58b1717078c013'>", "title": "Computer Scientist", "subTitle": "I am a researcher and computer scientist. I used to work in San Francisco, then I traveled a bit", "avatarUrl": "https://github.githubassets.com/favicons/favicon.svg", "GMEEK_VERSION": "last", "postListJson": {"P9": {"htmlDir": "docs/post/Consistency and Operational Confidence.html", "labels": ["documentation"], "postTitle": "Consistency and Operational Confidence", "postUrl": "post/Consistency%20and%20Operational%20Confidence.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/9", "commentNum": 0, "wordCount": 20582, "description": "\r\nConsistency is one facet of Materialize's 'Trust' pillar, the others being responsiveness and freshness.\r\nIt turns out that being super responsive and ultra fresh doesn't amount to much if the results don't make any sense.\r\nThe last thing you need in your operational data plane is a layer that introduces chaos and confusion, even if it is fast and scalable.\r\n*Especially* if it is fast and scalable.\r\n\r\nMany popular platforms ultimately bring weak consistency properties.\r\nWe've discussed in [our product principles post](https://materialize.com/blog/operational-attributes/) how caches and bespoke microservices are one way to get both responsiveness and freshness, but at the expense of consistency.\r\nBut even internally consistent platforms, like some stream processors and data warehouses, often end up wrapped in caches and serving layers for operational work. \r\nTheir consistency properties largely go out the window at that point, and it becomes your job to make sure that these systems operate as intended.\r\n\r\nAt Materialize we believe consistency is at the heart of the value that a database provides.\r\nThe *order* that a database introduces is why you use one, rather than a heap of JAR files pointed at various Kafka topics.\r\nFor those of you with a heap of JAR files and Kafka topics, this post is for you.\r\n\r\nInformally, consistency speaks to Materialize *appearing* to simply process commands and events in the order they happen in the real world.\r\nWhile the reality is that no scalable data platform does anything nearly so simple, responsible platforms don't let that become your problem.\r\nMaterialize is a responsible platform, and it opts you in to the strongest consistency guarantees we know of: [strict serializability](https://jepsen.io/consistency/models/strict-serializable).\r\nAlthough powerful, these database guarantees needs to be extended from command-response operation (pull) to streaming operation (push), as Materialize supports both concurrently.\r\n\r\nIn this post we will unpack Materialize's consistency guarantees, show them happening in a [playground environment](https://materialize.com/register/), and help you probe and evaluate the consistency properties of other tools you may be using for your operational work.\r\n\r\n\r\n### Consistency a la Databases\r\n\r\nIronically perhaps, the term 'consistency' means many different things to folks in the databases, distributed systems, and big data spaces.\r\nFor a helpful introduction I recommend [the Jepsen page on consistency models](https://jepsen.io/consistency).\r\nThe tl;dr there is that [strict serializable](https://jepsen.io/consistency/models/strict-serializable) is what you wish were the case: all interactions are applied in an order that tracks the order they happened in the real world.\r\nThe other, weaker models introduce semantic anomalies in the interest of avoiding performance anomalies (up to and including database unavailability).\r\nThat doesn't mean the other models are inherently bad, but they are certainly spookier and require more expertise on your part.\r\n\r\nMaterialize supports both [strict serializable]((https://jepsen.io/consistency/models/strict-serializable)) and [serializable](https://jepsen.io/consistency/models/serializable) operation.\r\nSerializability still requires interactions be applied in some order, but the order doesn't need to match the real world;\r\nfor example, you could be served stale results in order to see them faster than if you waited for the results to catch up to their fresh inputs.\r\nWe start you off with strict serializability so that you aren't surprised by the apparent mis-orderings of (non-strict) serializability, and then teach you about the latter if you believe you need to squeeze more performance out of Materialize and can absorb the potential confusion.\r\n\r\nHowever, definitions like strict serializability and serializability only apply to systems that accept commands and provide responses.\r\nThere are other dimensions to consistency as we move into the world of streamed inputs, maintained views, and streamed outputs.\r\nLet's dive into those now!\r\n\r\n### Consistency in Materialize\r\n\r\nAlthough Materialize fits the mold of an interactive SQL database, and provides the guarantees of one, it has additional streaming touchpoints.\r\nInput data can be provided by external sources like Kafka and Postgres, which do not 'transact' against Materialize.\r\nMaterialized views are kept always up to date, as if they are refreshed instantaneously on each data update.\r\nOutput data can be provided to external sinks like Kafka, as streams of events rather than sequences of transactions.\r\nWe need to speak clearly about how Materialize's consistency guarantees integrate with these features.\r\n\r\nThese three concerns lie at the heart of an operational data warehouse, whose outputs and actions must faithfully represent business logic applied to their inputs.\r\nWithout this guarantee, it is not entirely clear what an operational platform will and will not do on your behalf.\r\n\r\n---\r\n\r\nAlthough things sound like they might be about to get more complicated, I think they actually get *easier*, by getting more specific about how we maintain consistency in Materialize.\r\n\r\nMaterialize uses a concurrency control mechanism called [Virtual Time](https://materialize.com/blog/virtual-time-consistency-scalability/).\r\nEvery command and data update get assigned a virtual timestamp, and then Materialize applies these operations in the order of these timestamps. \r\nAlthough there is some subtlety to how we *assign* the timestamps to operations, once that step is done the system behaves in what we think is an largely unsurprising and thoroughly consistent manner.\r\nNot only will Materialize behave as if all operations happen in *some* order, as required by serializability, *we can even show you what that order is*.\r\n\r\n---\r\n\r\nProperly prepared, let's now dive in to each of the three concerns above, which I'll call here input consistency, internal consistency, and output consistency.\r\n\r\n\r\n#### Input Consistency\r\n\r\nMaterialize draws streamed input data from external sources, like Kafka and PostgreSQL.\r\nIdeally, Materialize would assign timestamps to updates that exactly track the moments of change in the upstream data.\r\nIn practice, these sources are often insufficiently specific about their changes, and Materialize instead 'reclocks' their sequence of states into its own virtual time.\r\nWhen it does so, it assigns timestamps that aim to be consistent with the source itself.\r\n\r\nMaterialize durably records its timestamp assignment in auxiliary sources, as changing collections that at each time record the progress through the source so far.\r\n\r\nPostgreSQL sources move forward using a 'log sequence number', and you can see the current time and current log sequence number with the following query, where `pg_source_progress` just happened to be the name of the progress source.\r\n```\r\nmaterialize=> select mz_now(), * from pg_source_progress;\r\n        mz_now |         lsn\r\n---------------+-------------\r\n 1695659907060 | 11695622984\r\n(1 row)\r\n```\r\n\r\nKafka is more complicated. Each topic is comprised of an unbounded number of partitions, each of which moves forward through integer offsets. \r\nRather than a single `lsn`, each time has an association between partition ids and offsets, including a `0` for all partitions that have not yet come into existence.\r\nThe selection reports not a single number, but an offset for ranges of partitions.\r\n```\r\nmaterialize=> select mz_now(), * from kafka_source_progress;\r\n        mz_now | partition |   offset\r\n---------------+-----------+----------\r\n 1695659699912 |     [0,0] | 40166616\r\n 1695659699912 |     [1,1] | 40781940\r\n 1695659699912 |     [2,2] | 40472272\r\n 1695659699912 |      (2,) |        0\r\n(4 rows)\r\n```\r\n\r\nWhen Materialize reclocks these sources into its own timestamps, it aims to maintain consistency with the inputs.\r\nSpecifically, it maintains the order of events in the underlying sources, it respects transaction boundaries when it is aware of them, and it could (but currently does not) transact against the upstream source to ensure that all writes are immediately visible.\r\nLet's explore each of these properties.\r\n\r\nMost streamed sources have a notion of order, in some cases a total order like PostgreSQL's replication log, and in some cases a weaker order like Kafka's partitioned topics.\r\nMaterialize's timestamp assignment should (and does) respect this order, so that you see a plausible database state.\r\nMaterialize records for each virtual timestamp the coordinates in the input order that describe the subset of data available at that timestamp. \r\nA new data update is assigned the first timestamp whose coordinates contain the update.\r\nAs long as the recorded coordinates move forward along the order as times increase, the revealed states of the data also move forward following the order.\r\n\r\nFor PostgreSQL we can verify that repeated inspection of the progress source shows an advancing timestamp and an advancing log sequence number.\r\n```\r\nmaterialize=> select mz_now(), * from pg_source_progress;\r\n        mz_now |         lsn\r\n---------------+-------------\r\n 1695659907060 | 11695622984\r\n(1 row)\r\nmaterialize=> select mz_now(), * from pg_source_progress;\r\n        mz_now |         lsn\r\n---------------+-------------\r\n 1695659910061 | 11695624104\r\n(1 row)\r\nmaterialize=> select mz_now(), * from pg_source_progress;\r\n        mz_now |         lsn\r\n---------------+-------------\r\n 1695659911994 | 11695624568\r\n(1 row)\r\n```\r\n\r\nMany streamed sources reveal transactional boundaries, such as PostgreSQL's replication log.\r\nKafka itself supports 'transactional writes' but does not reveal the transaction boundaries to readers; you would need to use Debezium configured with a transaction topic to provide transaction information with it.\r\nFor PostgreSQL, Materialize assigns identical timestamps to all updates associated with the same transaction.\r\nThis ensures that other operations either see all or none of the updates in any transaction.\r\n\r\nFinally, having written something to an upstream system (and received confirmation) you might like to be certain it is now available and reflected in Materialize.\r\nThis can be achieved by transacting against the upstream system for each timestamp we produce, but is not currently done by Materialize.\r\nWe think we should do it, however, and you should expect systems that can provide this level of fidelity to external data sources.\r\n\r\nTimestamp assignment is the moment Materialize introduces order to its often inconsistent sources of data. \r\nIt is also the moment we are able to be precise about the consistency properties we are able to maintain, and which we will need to invent.\r\n\r\n#### Internal Consistency\r\n\r\nMaterialize has streaming internals, and uses them to continually keep various materialized views up to date.\r\nEven with careful timestamps on input updates, with all the updates in motion through the streaming internals there is the real possibility that Materialize might reveal inconsistent results.\r\nInconsistent or transiently incorrect results are unacceptable for operational work; at best you have to stall your operational plane to sort things out, and at worst you may take irrevocable incorrect actions.\r\n\r\nMany stream processors have the baffling property that their outputs need not correspond to any specific input.\r\nThis comes under the name of [eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency), which allows systems to be transiently incorrect as long as their inputs continue to change.\r\nInputs change pretty much always for stream processors, that's why you use them, leaving several popular systems with no specific consistency properties.\r\nFor an excellent overview, [Jamie Brandon's post on 'internal consistency'](https://www.scattered-thoughts.net/writing/internal-consistency-in-streaming-systems/) evaluates this property for ksqlDB, Flink's Table API, and Materialize (and finds chaos in the non-Materialize entrants).\r\n\r\nMaterialize continually produces **specific** and **correct** outputs for its timestamped inputs.\r\nAnything else is a bug.\r\n\r\nWe can see this in a playground environment using a query like Jamie used in his post.\r\nOur [guided tutorial](https://materialize.com/docs/get-started/quickstart/) sets up a source of auction transactions, with buyers and sellers and bids.\r\nAlthough many things change continually, we would hope that the sum of all credits through sales match the sum of all debits through sales.\r\nThey should always be exactly identical, and if even for a moment they are not that would be a bug in Materialize.\r\n\r\n```sql\r\n-- Maintain the credits due to each account.\r\nCREATE MATERIALIZED VIEW credits AS\r\nSELECT seller, SUM(amount) AS total\r\nFROM winning_bids\r\nGROUP BY seller;\r\n\r\n-- Maintain the credits owed by each account.\r\nCREATE MATERIALIZED VIEW debits AS\r\nSELECT buyer, SUM(amount) AS total\r\nFROM winning_bids\r\nGROUP BY buyer;\r\n\r\n-- Maintain the net balance for each account.\r\nCREATE VIEW balance AS\r\nSELECT \r\n    coalesce(seller, buyer) as id, \r\n    coalesce(credits.total, 0) - coalesce(debits.total, 0) AS total\r\nFROM credits FULL OUTER JOIN debits ON(credits.seller = debits.buyer);\r\n\r\n-- This will always equal zero.\r\nSELECT SUM (total) FROM balance;\r\n```\r\n\r\nImportantly, nothing about the above example relies on the views being created in the same session, by the same person, team, or even running on the same physical hardware.\r\nMaterialize will ensure that `credits`, `debits`, and `balance` always track exactly the correct answer for the timestamped input, and will always have a net balance of zero.\r\n\r\nTo assess internal consistency for systems, Materialize and others, it can help to write views that track *invariants* of your data. \r\nIf there is something you know should always hold, for example that the net balances are zero, then you can observe the results and watch for a result that violates the invariant.\r\n\r\nYou can similarly be certain that when you see a result that it corresponds to the correct answer on a specific input. \r\nFor example, if you want to notify those users whose balance is below 100, the following view is certain to only report users for which it *actually happened*.\r\n\r\n```sql\r\nSELECT mz_now(), * FROM balance WHERE total < -100\r\n```\r\n\r\nThe `mz_now()` column will report the exact time at which the input data yielded a low balance.\r\n\r\nAll results Materialize produces are the specific answers to the query on the input data as it existed at the query time.\r\n\r\n#### Output Consistency\r\n\r\nFinally, having both ingested and maintained results, Materialize needs to speak clearly about its results to external systems.\r\nWe saw just above that a `SELECT` query can use `mz_now()` to learn the specific moment at which query results were correct.\r\nHowever, the full power of Materialize unlocks when you connect its views as streaming outputs onward to downstream applications or systems.\r\nHow does Materialize speak clearly and unambiguously to these streaming consumers?\r\n\r\nMaterialize connects to three different types of downstream consumer, but as we will see it follows identical principles for each.\r\nMaterialize can return streamed changelogs for views in a standard SQL session using its [`SUBSCRIBE`](https://materialize.com/docs/sql/subscribe/) command.\r\nIt can also stream those same changelogs on to external systems, like Kafka and RedPanda, using its [`CREATE SINK`](https://materialize.com/docs/sql/create-sink/) command.\r\nFinally, Materialize also commonly writes data back to *itself*, to fan out to other users and uses, through its [`CREATE MATERIALIZED VIEW`](https://materialize.com/docs/sql/create-materialized-view/) command.\r\nAlthough different types of endpoints, all three communicate the same information: exactly what changed in a view and exactly when did those changes happen.\r\n\r\nTo communicate clearly Materialize follows certain rules for its changelogs.\r\nEach changelog begins at a specific timestamp with the collection snapshot at that timestamp.\r\nEach record changes only once for each timestamp, and that timestamp is explicitly recorded with the change.\r\nEach timestamp is regularly indicated to be complete, even when no changes occur.\r\nThese properties remove ambiguity about what the changes were, when they happened, and whether there are any more coming for any given timestamp.\r\n\r\nLet's take a peek using the `SUBSCRIBE` command, simply watching the count of the number of auctions that have been won.\r\n\r\n```\r\nmaterialize=> copy (\r\n    subscribe (select count(*) from winning_bids) \r\n         with (progress = true)\r\n) to stdout;\r\n```\r\n\r\nI pressed `ENTER` between blocks of returned results to suggest at the live experience, and added comments to these lines that describe the *preceding* block of responses.\r\n\r\n```\r\n1695653291958\tt\t\\N\t\\N\r\n-- Timestamp of initial snapshot\r\n1695653291958\tf\t1\t38549\r\n1695653293090\tf\t-1\t38549\r\n1695653293090\tf\t1\t38550\r\n1695653298001\tt\t\\N\t\\N\r\n-- Initial snapshot and immediate change\r\n1695653299001\tt\t\\N\t\\N\r\n1695653299105\tt\t\\N\t\\N\r\n1695653299105\tf\t-1\t38550\r\n1695653299105\tf\t1\t38551\r\n1695653300001\tt\t\\N\t\\N\r\n-- Brief break before next change\r\n1695653301001\tt\t\\N\t\\N\r\n1695653302001\tt\t\\N\t\\N\r\n1695653303001\tt\t\\N\t\\N\r\n...\r\n-- Nothing happens for a while.\r\n```\r\n\r\nThe columns of each returned row are: first the timestamp in milliseconds since 1970, second 'is this a watermark', third the change in the cardinality of the record, and finally the payload columns of the record itself.\r\nWatermark records indicate only the forward progress of times, that all future timestamps will be at least so large, and have null values for columns other than the timestamp.\r\n\r\nThere are four blocks of output to unpack.\r\n1. The first and immediate block of output is the 'initial snapshot timestamp' progress message, which tells us the time the initial snapshot of the `SUBSCRIBE` will reflect.\r\n2. The second block of output includes the snapshot first. As the snapshot requires spinning up a dataflow (`winning_bids` is a non-materialized view), some additional input changes happen before we have the snapshot, and we report their output changes as well.\r\n3. The next block is now live and reports a new update just as it happens, from `38550` to `38551`, and confirms that there are no further changes at that time.\r\n4. The last block reports multiple seconds proceeding for which the count does not change.\r\n\r\nThese blocks each report the correct `COUNT(*)` output at the exact times the inputs change. \r\nMaterialize will wait until it is certain of the exact updates for a time, including that they are durably committed, before reporting them.\r\n\r\nAlthough other destinations differ from `SUBSCRIBE`, each have access to an ongoing stream of precise information detailing exactly what changed, when it changed, and whether more changes are due.\r\nThis information communicates to consumers the moment a change has certainly occurred, giving them the confidence to act immediately.\r\n\r\n## Consistency and Operational Confidence\r\n\r\nConsistency is critical on operational workflows because there are actions that need to be taken.\r\nMany of these actions have consequences, and if they are directly driven by an inconsistent platform it is up to you to diagnose and debug any resulting glitchy behavior.\r\nThese glitches have consequences too, some of which can be corrected after the fact and some of which cannot.\r\nOperational platforms provide value in part by introducing and maintaining consistency for you, avoiding unintended actions and their consequences.\r\n\r\nMaterialize specifically provides strict serializability, and extends this to its streaming ingestion, transformation, and onward communication.\r\nThis guarantee means Materialize behaves *as if* it applied all commands in an order that matches how they happened in the real world.\r\nIn reality Materialize is massively concurrent, but it absorbs this complexity and presents as a surprisingly capable single operator.\r\n\r\nIf this resonates with you, especially if you have heaps of JAR files and Kafka topics, we invite you to try out Materialize for yourself.\r\nOur [guided tutorial](https://www.materialize.com/docs/get-started/quickstart/) builds up the auction data sources described above, and includes demonstrations of consistency.\r\nIf you'd like to try out Materialize on larger volumes of your own data, reach out about doing a [Proof of Concept](https://materialize.com/trial/) with us!\r\n\r\n<!-- ##{'timestamp':1695099600}## -->\u3002", "top": 0, "createdAt": 1695099600, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2023-09-19", "dateLabelColor": "#A333D0"}, "P8": {"htmlDir": "docs/post/Freshness and Operational Autonomy.html", "labels": ["documentation"], "postTitle": "Freshness and Operational Autonomy", "postUrl": "post/Freshness%20and%20Operational%20Autonomy.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/8", "commentNum": 0, "wordCount": 12705, "description": "\r\nFreshness is one of three components of [Materialize's Trust pillar of product value](https://materialize.com/blog/operational-attributes/#trust), the other two being responsiveness and [consistency](https://materialize.com/blog/operational-consistency/).\r\nOperational work is fundamentally about promptly reacting to and reflecting events in the real world. \r\nAnd the real world, famously, waits for no one.\r\nEvery moment your operational layer isn't up to date represents missed opportunity as the real world moves on.\r\n\r\nAnd believe it or not, staying up to date is only the tip of the operational iceberg.\r\n\r\nMaterialize uses SQL not only to query the present, but also to describe how it should respond to future events.\r\nYour operational work shifts from being a repeated sequence of imperative SQL commands to declarative SQL views that describe your business logic.\r\nThis allows Materialize to accept responsibility for ongoing operational work, and to act autonomously where appropriate.\r\nAnd it allows *you* to declaratively specify much of your operational layer, avoiding a tangle of scripts, cron jobs, and baling twine.\r\n\r\nIn this post we'll unpack how Materialize views freshness, see how it introduces autonomy at different moments, and call out the work you currently do that it can do for you instead.\r\nWe'll build up to an end-to-end demonstration borrowing from our [guided tutorial](https://materialize.com/docs/get-started/quickstart/).\r\n\r\n## Freshness in Materialize\r\n\r\nAt the heart of freshness in Materialize is autonomous proactive work, done in response to the arrival of data rather than waiting for a user command.\r\nUser commands still exist, and Materialize promptly responds to them too, but many of the commands set up ongoing work rather than one-off work.\r\nThe proactive ongoing work spans data ingestion, view and index maintenance, and onward streaming outputs.\r\nAll of this work aims to minimize the time from data updates to their reflection in indexes (for querying) and output streams (for action).\r\n\r\nIn addition to acting proactively, we need to carefully consider the work we choose to do.\r\nOne can't simply re-do all work on each data update; we'll end up continually behind rather than at all ahead.\r\nIdeally, we would do the *same* work as for batch processing, only performed eagerly (as the updates arrive) rather than lazily (once the batch completes).\r\nThis principle ensures that we remain throughput-competitive with batch systems, while minimizing the latency for data updates.\r\n\r\nLet's examine the proactive work across Materialize's ingestion, computation, and output layers.\r\n\r\n### Autonomy in Ingestion\r\n\r\nMaterialize draws input data from [sources](https://materialize.com/docs/sql/create-source/): tables maintained by external systems that Materialize should faithfully reflect.\r\nExamples include PostgreSQL databases (through their replication log) and Kafka topics.\r\nMaterialize continually monitors these external systems, and receives data updates the first moment the systems make them available.\r\n\r\nAs Materialize receives data updates it timestamps them and commits them to its own durable storage.\r\nThe storage layer uses an append-friendly changelog format that does not need to rewrite existing data.\r\nLog compaction happens in the background, off of the critical path and without impeding data ingestion.\r\nUpdates are available to users and their uses as soon as the timestamped data are durably committed to the OLTP database containing Materialize's storage metadata.\r\n\r\nThis ongoing work pulls data in as soon as Materialize has access to it, and attempts to do as little as possible to make it durable and then reveal it to users.\r\nThe result is continual freshness of ingested data, always as current as upstream systems have presented it.\r\n\r\n### Autonomy in Computation\r\n\r\nMany operational systems record data updates promptly, and then invite you to query it.\r\nWhile useful, that invitation stops short of any consequent operational work that needs to be done.\r\nIf you have business logic that depends on those changed data, you'd really like to see the changes in the *outputs* rather than the *inputs*.\r\nYou'd like someone to *maintain* your business logic for you.\r\n\r\nMaterialize's maintenance of views and indexes is driven by [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow), a compute engine specifically designed to minimize the end-to-end latency of data updates.\r\nDifferential dataflow provides carefully implemented data-parallel operators (e.g. `map`, `reduce`, `join`) and Materialize translates your SQL into a dataflow of these operators.\r\nTo read more about the implementation of these atomic operators, and the properties of differential dataflow generally, we recommend [the VLDB paper on Shared Arrangements](http://www.vldb.org/pvldb/vol13/p1793-mcsherry.pdf).\r\n\r\nEven with differential dataflow, Materialize needs to carefully construct dataflows to ensure that updates happen both promptly and efficiently.\r\nA not-uncommon pattern in other systems with shallower incremental view maintenance (IVM) support is that they fall back to expensive implementations when queries stray outside of the range of SQL the system's IVM supports.\r\nMaterialize uses the same engine to both evaluate queries and to incrementally maintain them, so it doesn't have exceptions to its IVM support.\r\n\r\nLet's look at three examples of SQL that can be challenging to maintain in other systems: supporting updates and deletions, correlated subqueries, and recursion.\r\n\r\nSQL aggregations `MIN` and `MAX` are not hard to maintain incrementally when you only insert data, but life gets much harder when you update or delete input data.\r\nYour continued deletions (imagine implementing a priority queue) can eventually make any input record become the correct answer.\r\nMaterialize ensures this happens both correctly and promptly by performing aggregation in a tree, and leaving this tree structure behind as the state to maintain. \r\nThe same construction applies equally well to maintaining views containing `ORDER BY .. LIMIT ..` clauses.\r\n\r\n```sql\r\n-- You can *retract* arbitrary rows from `input_tbl`,\r\n-- and can make any input row become the correct answer.\r\nSELECT key_col, MIN(col1), MAX(col2), ..\r\nFROM input_tbl\r\nGROUP BY key_col;\r\n```\r\nWhen `input_tbl` is append-only, either because its source is append-only or because this is a one-off query, Materialize is able to use the leaner implementation that keeps only the results for each `key_col`.\r\nWhen `input_tbl` can change arbitrarily, Materialize prepares to minimize the update time for any changes, including retractions.\r\n\r\nSQL has the concept of 'correlated subquery' which behave as if you you were to issue a new query for each record in some table.\r\nSimilarly, SQL's `LATERAL` join keyword allows you to manually correlate subqueries. \r\nFor example, \r\n```sql\r\nSELECT * FROM\r\n    input_tbl,\r\n    LATERAL (\r\n        -- As if re-queried for each row in `input_tbl`.\r\n        SELECT col1, col2... FROM other_tbl\r\n        WHERE other_tbl.key_col = input_table.key_col\r\n          AND other_tbl.val_col > input_table.val_col\r\n        ORDER BY other_tbl.ord_col LIMIT k\r\n    )\r\n```\r\nMaterialize rewrites all queries to be free of subqueries in a process called decorrelation ([described here by Neumann and Kemper](https://cs.emis.de/LNI/Proceedings/Proceedings241/383.pdf)).\r\nThis way, Materialize is able to incrementally maintain arbitrary correlated subqueries.\r\n\r\nSQL allows you to write recursive queries with `WITH RECURSIVE`.\r\nThis powerful construct is often vexxing, and we are unaware of other systems that are able to incrementally maintain anything like it for general queries.\r\nFortunately, differential dataflow supports recursive natively, and Materialize supports incremental evaluation and maintenance through its (slightly different) [`WITH MUTUALLY RECURSIVE`](https://materialize.com/docs/sql/recursive-ctes/#details) construct.\r\n\r\nNot all of Materialize's dataflows are flawless.\r\nWindow functions in particular are challenging to support in their full generality, as they allow rich computation and aren't as easily eliminated as are correlated subqueries.\r\nHowever they, like any other limitations, are being actively pursued and should only improve!\r\n\r\nAlthough there is a lot to know here, Materialize's computation layer is continually working to maintain your SQL views and indexes as the underlying data change.\r\nThis is all in pursuit of freshness, pushing data updates through business logic proactively, both to be ready with fresh indexed results and to communicate them onward.\r\n\r\n### Autonomy in Query Serving\r\n\r\nThe most common mode of interaction with a SQL system, the `SELECT` query, isn't great from the perspective of freshness.\r\nYou are required to repeatedly ask the system for results, and when there is a change you need to be the one to notice it.\r\n\r\nMaterialize adds a new command, [`SUBSCRIBE`](https://materialize.com/docs/sql/subscribe/), which like `SELECT` gives you the answer to your query, but then continues with a stream of timestamped updates that tell you about changes to those results as soon as they happen.\r\nThe `SUBSCRIBE` command allows you to build fresh applications without continually hammering the systems with polling `SELECT` statements.\r\n\r\nMaterialize also has the concept of a [SINK](https://materialize.com/docs/sql/create-sink/), which is roughly the output complement to an input `SOURCE`: it pushes the information of a `SUBSCRIBE` on to an external system, such as a Kafka topic.\r\nDownstream systems can listen to these sinks to see updates to maintained views as soon as they happen.\r\n\r\nLet's see `SUBSCRIBE` in action, using an example from our [guided tutorial](https://materialize.com/docs/get-started/quickstart/). \r\nSpecifically, we'll head to ['Step 3: See results change!'](https://materialize.com/docs/get-started/quickstart/#step-3-see-results-change), in case you'd like to follow along.\r\nIn this example we have a large, continually changing view `winning_bids` of auction winners, some of which may correspond to fraudulent accounts.\r\nWe introduce a new table on the side, `fraud_accounts`, and want to monitor the top non-fraudent auction winners, written\r\n```sql\r\nSUBSCRIBE TO (\r\n  SELECT buyer, count(*)\r\n  FROM winning_bids\r\n  WHERE buyer NOT IN (SELECT id FROM fraud_accounts)\r\n  GROUP BY buyer\r\n  ORDER BY 2 DESC LIMIT 5\r\n);\r\n```\r\nWe can look at the output and take any of the top buyers and (perhaps unfairly) flag them as fraudulent by inserting them into `fraud_accounts`.\r\n Perhaps we investigate and clear them, then deleting them from `fraud_accounts`. \r\n Each action results in an immediate update to the `SUBSCRIBE` output.\r\nThe example demonstrates each of the layers, ingesting updates promptly from both tables and sources, moving the updates through an `ORDER BY .. LIMIT` dataflow with a (non-correlated) subquery, and surfacing output updates as soon as they occur.\r\n\r\nThe `SUBSCRIBE` and `SINK` constructs allow Materialize to serve fresh results as soon as they happen.\r\nUsers and applications are not required to anticipate changes, nor poll the system on a tight cadence.\r\n\r\n## Freshness and Operational Autonomy\r\n\r\nAn operational layer wants to be able to connect the dots from input updates and events, through business logic, on to downstream systems that can take the appropriate actions.\r\nTo achieve this one must build autonomy into each of the layers of ingestion, computation, and serving.\r\nIf any of these layers aren't fully autonomous, you or code acting on your behalf will have to poke them into action on some regular basis.\r\nYou'll also likely be responsible for interpreting the results and determining if they merit propagating onward.\r\n\r\nMaterialize specifically allow you to install operational business logic that keeps its results up to date and allows others to take action the moment results change.\r\nIt does this by making its internal components update autonomously and proactively, as updates to data occur.\r\nMaterialize can absorb end-to-end responsibility for this operational work, framed as SQL views.\r\n\r\nIf freshness and operational autonomy sound exciting to you, we invite you to try out Materialize for yourself.\r\nOur [guided tutorial](https://www.materialize.com/docs/get-started/quickstart/) builds up the auction data sources described above, and includes demonstrations of consistency.\r\nIf you'd like to try out Materialize on larger volumes of your own data, reach out about doing a [Proof of Concept](https://materialize.com/trial/) with us!\r\n\r\n<!-- ##{'timestamp':1695963600}## -->\u3002", "top": 0, "createdAt": 1695963600, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2023-09-29", "dateLabelColor": "#A333D0"}, "P7": {"htmlDir": "docs/post/Responsiveness and Operational Agility.html", "labels": ["documentation"], "postTitle": "Responsiveness and Operational Agility", "postUrl": "post/Responsiveness%20and%20Operational%20Agility.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/7", "commentNum": 0, "wordCount": 14832, "description": "\r\nResponsiveness is one of three components of [Materialize's Trust pillar of product value](https://materialize.com/blog/operational-attributes/#trust), the other two being [freshness](https://materialize.com/blog/freshness/) and [consistency](https://materialize.com/blog/operational-consistency/).\r\nWhile being fresh and consistent is fundamental, operational work suffers if each intervention is a 15 minute deployment away.\r\nWe all want to live in world where our operational logic is fully baked, but the reality is that things change and interactivity matters.\r\nMoreover, operational work is often inherently interactive: responding to user or operator queries that are not known ahead of time.\r\nFor these reasons, among others, systems must be responsive to be a trustworthy part of your operational layer.\r\n\r\nDifferent architectures have different visions for how work gets done, which leads to different responsiveness characteristics.\r\nThe conventional cloud data warehouse pulls stale data from cloud storage and re-evaluates your query, each time from scratch and at some cost.\r\nDataflow engines generally re-flow the streams that define their inputs, which happens at high throughput but still takes time to cover the volume of data.\r\nCaches and microservices generally nail responsiveness, though without much to say about consistency or freshness.\r\nThe caveats make none of these alternatives especially satisfying.\r\n\r\nResponsiveness is about more than just promptly providing a response: the response needs to be valuable and actionable.\r\nSystems can trivially respond with inconsistent, stale, or unhelpful results ('nothing yet, boss'), but we understand that this doesn't yet provide value.\r\nThey can promptly respond to interventions with confirmation of initiation ('just starting, boss'), but this doesn't mean any work will soon be done.\r\nResponsiveness provides value when the response has meaning, which we believe is captured by consistency and freshness (which is why we covered them first!).\r\nA responsive system must promptly provide a *meaningful* response; otherwise it is just entertainment.\r\n\r\nIn this post we'll dive into how Materialize makes commands responsive, from the structure it exploits in both data and queries, through the technical underpinnings, up to an example of responsive, fresh, and consistent results for non-trivial operational work involving multi-way joins.\r\n\r\n## Responsiveness in Materialize\r\n\r\nIn Materialize, responsiveness is about minimizing the time between an issued command and Materialize's consistent, fresh responses (to the operator, or to downstream consumers).\r\n\r\nAchieving responsiveness is about much more than just programming hard to make computers go fast. \r\nIt is about preparing and organizing information ahead of time so that when commands arrive we have the answers (nearly) at hand.\r\nWhen `SELECT` commands arrive, from easy `LIMIT 1`s to hard multi-way `JOIN`s, we want to minimize the time required before Materialize can provide the result.\r\nWhen users create indexes, materialized views, and sinks, we want to minimize the time before those assets are operational.\r\nIn each case, we want to identify and exploit structure in the data and the commands to make subsequent work fast.\r\n\r\nWe also try to program rly hard, but the gains really come from the preparation instead.\r\n\r\n### Data Structure: Change Data Capture and Snapshot Roll-ups\r\n\r\nMaterialize uses [change data capture](https://en.wikipedia.org/wiki/Change_data_capture) (CDC) as a way to represent continually changing data.\r\nImportantly, while CDC presents itself as a stream of events, it has the special structure that they always 'roll up' to a snapshot data set.\r\nOne can interpret and operate on CDC data as if a snapshot followed by changes, without needing to retain and review the historical detail of a raw stream.\r\nThis is an example of 'data structure' that will allow us to do something more clever than continually re-evaluating over all data we've ever seen.\r\n\r\nThe CDC structure gives us a guiding principle for how to organize information: organize the snapshot and maintain it as it changes.\r\nMaterialize durably records CDC updates, but continually compacts them to maintain a concise snapshot of input data.\r\nMaterialize builds indexes over both input data and data derived through views, and maintains them as the data change.\r\nMaterialize responds with snapshot data, but follows it with CDC updates that call out the changed data explicitly.\r\nAny tricks we can use for snapshots of data are in scope for Materialize, as long as we can extend them to *maintained* results.\r\n\r\nThe superpower of CDC and roll-ups is that we know that queries have a correct and concise answer, and we can prepare our data to answer them ahead of time.\r\n\r\n### Query Structure: Data Parallelism\r\n\r\nA great deal of the value in SQL's `SELECT` command is how it draws out of complex questions the *independence* of the rows of the data.\r\nA `WHERE` or `HAVING` clause applies row-by-row; the result on one row does not affect the result on another row.\r\nA `JOIN` clause finds rows that match on key columns, whose results are independent of rows that do not match on these columns.\r\nA `GROUP BY` clause produces aggregates for each key, each output independent of rows with other keys.\r\nIt is this query structure, the identified *independence*, that enables much of modern data processing optimization.\r\n\r\nMaterialize's storage plane records CDC streams and maintains them as snapshots and changelogs, serving them up to other parts of the system.\r\nWhen it does serve them up, it does so in response to requests, and these requests usually have valuable context that can improve its performance.\r\nIf a user requires only recent data, e.g. a `WHERE row.time > mz_now()`, the storage layer can return a subset of records that might pass this test.\r\nIf a user requires only a subset of columns, e.g. a projection, the storage layer could (but does not yet) return only those columns\r\nIf a user needs only limited results, e.g. a `LIMIT 1`, the storage layer can stop as soon as the needed number is met.\r\nThese are each techniques from cloud data warehouses on static data, but generalize to changing data for the same SQL idioms. \r\n\r\nMaterialize's compute plane builds and maintains indexes over both input data and data derived from SQL views.\r\nThese indexes are on key columns, or key expressions, and ensure that one can look up all records that match a certain key.\r\nThey allow queries with `WHERE key = literal` or `WHERE key IN (lit1, lit2, lit3)` to dive directly to the relevant results, in milliseconds, rather than scan anything.\r\nThey also enable `JOIN`s that equate the key columns to do so immediately, rather than needing to rescan and reorganize the input.\r\nThese indexes are continually maintained, providing interactive access without sacrificing freshness or consistency as might an independent cache.\r\n\r\nFinally, Materialize's serving plane takes advantage of independence among the SQL commands themselves. \r\nWhile Materialize must put the commands in *some* order, Materialize can see which commands can execute concurrently and does so.\r\nMaterialize tracks the available timestamps for each input and derived view (their 'freshness'), and uses this information in determining the best order.\r\nWhen consistency or freshness is not as important to you as as responsiveness, Materialize provides tools (e.g. `SERIALIZABLE` isolation) to help navigate the trade-offs.\r\n\r\nMaterialize takes advantage of existing SQL idioms you already know and expect, to provide a responsive experience.\r\n\r\n## A Worked Example: Auctions\r\n\r\nLet's take a quick look at a workload that highlights Materialize's *responsiveness* in the face of a non-trivial workload.\r\nWe'll mostly deal with interactive queries, but the implications apply just as well to deployed dataflows into indexes, materialized views, and sinks.\r\n\r\nOur [guided tutorial](https://materialize.com/docs/get-started/quickstart/) is based around an auction load generator, which contains among other things continually evolving auctions and bids.\r\nOne common query you might want to support is 'for each auction I (a user) have bid in, how many other users have outbid me?'\r\nThis both calls out auctions you are currently winning, and gives a sense for the level of competition in other auctions.\r\nHowever, it is not immediately obvious how best to support this sort of query interactively.\r\n\r\nLet's start by writing some views defining the logic we'll want.\r\nAs it turns out, the views themselves will not need to change much as we explore different ways to dial in their responsiveness.\r\n\r\n```sql\r\n-- All bids for auctions that have not closed.\r\nCREATE VIEW active_bids AS\r\nSELECT bids.*\r\nFROM bids, auctions \r\nWHERE bids.auction_id = auctions.id\r\n  AND auctions.end_time > mz_now() \r\n  AND bids.bid_time + INTERVAL '10 seconds' > mz_now();\r\n```\r\n\r\n```sql\r\n-- Number of times each buyer is outbid in each auction.\r\nCREATE VIEW out_bids AS\r\nSELECT a1.buyer, a1.auction_id, COUNT(*)\r\nFROM active_bids AS a1, \r\n     active_bids AS a2\r\nWHERE a1.auction_id = a2.auction_id\r\n  AND a1.amount < a2.amount\r\n  AND a1.buyer != a2.buyer\r\nGROUP BY a1.buyer, a1.auction_id;\r\n```\r\n\r\nA first approach could be to perform the work from scratch each time a user asks.\r\nThis is roughly what would happen if you tried to serve the application out of your data warehouse.\r\nWhile it works, doing so is all sorts of scary, and isn't even all that responsive.\r\n```sql\r\n-- From-scratch evaluation of `out_bids` with a predicate applied.\r\nSELECT * FROM out_bids WHERE buyer = <buyer_id>;\r\n```\r\nMaterialize can push down the `mz_now()` temporal filters to the storage layer, reducing the amount of data that must be processed.\r\nHowever, we still need to collect and organize the data, which is unavoidable work to produce the correct count.\r\nOn the plus side, we have no ongoing cost other than the storage layer maintaining `bids` and `auctions`.\r\nOn Materialize just now, this took between 100 and 300 milliseconds to re-run (with `SERIALIZABLE` isolation).\r\n\r\nA second approach could be to materialize the whole of `out_bids`, maintaining each count for each user and auction.\r\nThis is roughly what you'd get if you set up a stream processor, and produced the results to some serving or caching layer.\r\nWhile it also works, you'll end up spending a fair bit maintaining data you may not need, and you won't even get consistency by the end.\r\n```sql\r\n-- Index `out_bids` by the `buyer` column, for fast look-up.\r\nCREATE INDEX out_bids_idx ON out_bids (buyer);\r\n-- Random access to the index by the buyer id.\r\nSELECT * FROM out_bids WHERE buyer = <buyer_id>;\r\n```\r\nThis approach is very responsive, reading the result directly out of an index. \r\nHowever, there is a maintenance cost: any new bid to an auction means updates for all counts that it exceeds.\r\nOn Materialize just now, this took consistently 20 milliseconds to re-run (with `SERIALIZABLE` isolation).\r\nWere I to increase the input load, I would need to quickly increase the instance size in order to keep up.\r\n\r\nA third approach is to index the intermediate `active_bids`, on both the `buyer` and `auction_id` columns.\r\nThis is neither what you'd get in a cloud data warehouse or in a stream processor; it seems unique to Materialize.\r\n```sql\r\n-- Index `active_bids` by the `buyer` and `auction_id` columns.\r\nCREATE INDEX active_bids_idx1 ON active_bids (buyer);\r\nCREATE INDEX active_bids_idx2 ON active_bids (auction_id);\r\n-- Allow Materialize to cleverly use the indexes in live joins.\r\nSELECT * FROM out_bids WHERE buyer = <buyer_id>;\r\n```\r\nIn this case Materialize will plan a `JOIN` query that uses the indexes and returns in interactive timescales.\r\nInformally, the query plan will start from `<buyer_id>` and pull all relevant auction identifiers from the first index, then use the second index to translate auction identifiers into the bids on those auctions, then count those records that satisfy the predicate on bid values.\r\nWe only touch the records we are interested in, and maintaining indexes on `active_bids` takes much less effort than maintaining all of `out_bids`.\r\nThe counts are instead produced at query time, showing a neat hybridization of pre-computation and query time computation.\r\nOn Materialize just now, this took consistently 30 milliseconds to re-run (with `SERIALIZABLE` isolation).\r\nWere I to increase the input load, I would also need to increase the instance size, but not nearly as much.\r\n\r\n---\r\n\r\nIf you'd like to explore any of these query plans in Materialize, just put an `EXPLAIN` in front of the `SELECT` command.\r\nThe plans of the second and third approaches are very approachable, whereas the first (re-execution) is a whole screenful.\r\nBut actually, taking a moment with each of them is probably very helpful, \r\n\r\n---\r\n\r\nThese three approaches to addressing a task show off several of the ways Materialize provides a responsive experience.\r\nThe storage layer can minimize data retrieved, the compute layer can maintain results in indexes and use them to fuel interactive joins, the adapter layer can choose between them based on available assets.\r\nThese mechanism take advantage of structure in the data and structure in the queries, keeping the right information up to date with input changes.\r\nImportantly, each of them provide identical output, as responsiveness does not come at the expense of consistency or freshness.\r\n\r\n## Responsiveness and Operational Agility\r\n\r\nResponsiveness is about the ability to do new things quickly.\r\nTo answer new questions, or set up new ongoing workflows, quickly.\r\nTo interactively probe and live-diagnose problems, with SQL queries not just key lookups, quickly.\r\nResponsiveness speaks to the *agility* of your operational layer.\r\n\r\nOperational tools that cannot respond quickly with actionable output are inherently clumsy and problematic.\r\nYou, your team, or your users will work around them, giving up on hard-won consistency, freshness, or both.\r\nBy the same token, being *meaningfully responsive* is about more than providing a prompt placeholder response.\r\nOperational systems need to be ready with the information you need, and be poised to correcctly implement the operational work you require.\r\n\r\nIf responsiveness and operational agility sound exciting to you, we invite you to try out Materialize for yourself.\r\nOur [guided tutorial](https://www.materialize.com/docs/get-started/quickstart/) builds up the auction data sources described above, and includes demonstrations of consistency.\r\nIf you'd like to try out Materialize on larger volumes of your own data, reach out about doing a [Proof of Concept](https://materialize.com/trial/) with us!\r\n\r\n<!-- ##{'timestamp':1696914000}## -->\u3002", "top": 0, "createdAt": 1696914000, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2023-10-10", "dateLabelColor": "#A333D0"}, "P6": {"htmlDir": "docs/post/Doing Business with Recursive SQL.html", "labels": ["documentation"], "postTitle": "Doing Business with Recursive SQL", "postUrl": "post/Doing%20Business%20with%20Recursive%20SQL.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/6", "commentNum": 0, "wordCount": 13239, "description": "\r\nLet's take a look at a fundamental problem in economics, with applications to doing business: matching up producers and consumers of some abstract resource, in a way that appeals to all of the participants.\r\n\r\nImagine we have a set of producers and a set of consumers, each of whom wants to be matched to one member of the opposite type, and each of them have some (not neccesarily shared) preference for the other.\r\nThe problem was initially presented in the language of 'stable marriage', but it applies to any pairings where the participants have opinions about those they might be paired with.\r\nThe framing has also been applied to matching hospital residents with hospitals, application clients with server capacity, and in this post hungry engineers and their lunching options.\r\nYou should be able to apply it to a variety of settings, most fruitfully when the matched things come with a rich variety of opinions about each other.\r\n\r\nTo spill the beans, there already is an algorithm for [stable matching](https://en.wikipedia.org/wiki/Stable_marriage_problem), and we're just going to implement it in recursive SQL.\r\nYou might not have thought of SQL as a language for *algorithms*, and conventional SQL is certainly very limited in this respect.\r\nHowever, recursive SQL can be a great fit, and when it is there's no reason not to just lean on the existing approaches!\r\n\r\n### Stable Matching in SQL\r\n\r\nWe will work off of a table `prefs` that will store the mutual preferences between pairs of producer and consumer.\r\nNot every pair needs to be represented here, and any pairs that are absent will just be taken to be non-viable.\r\nWe'll call producers and consumers by `name1` and `name2`, respectively, which aren't very evocative but are easier to type.\r\nEach pair will have integer preferences `pref1` and `pref2` for each other, where smaller numbers mean higher preference (imaging them as a ranking).\r\n\r\n```sql\r\n-- Each entry indicates a potential connection between `name1` and `name2`.\r\n-- Each has a numerical preference for this, where we'll take smaller to be better.\r\n-- The goal is to match up `(name1, name2)` pairs where each prefers the other over\r\n-- any other 'stable' pairing (someone else who likes them back enough not to leave).\r\nCREATE TABLE prefs(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT);\r\n```\r\n\r\nOur goal is to pull out a subset of `prefs` where each `name1` and `name2` occur at most once.\r\nAlso, we shouldn't leave behind any pairing in which each prefers the other more than the pair they were assigned.\r\nThat second part is where the algorithm comes in.\r\n\r\nOf course, we'll want some example preferences to work with.\r\nLet's start with some hungry engineers and food options.\r\nThematically, let's imagine that each human prefers the foods based on their own unaccountable tastes, and the food options (restaurants) prefer the humans based on their distance (because each's price doesn't vary as a function of the human, but the delivery cost does).\r\n\r\nHere's some made up data that will show off what we are trying to do.\r\n\r\n```sql\r\n-- Imagine people have a preference for foods that idk is based on its price.\r\n-- Imagine restaurants have a preference for people based on their distance.\r\nINSERT INTO prefs VALUES\r\n('frank',  4, 'ramen', 1),  -- frank needs food, and ramen likes him best\r\n('arjun',  1, 'ramen', 3),  -- arjun lovel ramen, but it is unrequited.\r\n('arjun',  3, 'sushi', 4),  -- arjun can tolerate sushi; they prefer him to nikhil.\r\n('nikhil', 1, 'sushi', 5);  -- nikhil is too far away to safely enjoy sushi.\r\n```\r\n\r\nIf we study the data (and the comments) we will find that one stable matching is \r\n```\r\n name1 | pref1 | name2 | pref2 \r\n-------+-------+-------+-------\r\n arjun |     3 | sushi |     4\r\n frank |     4 | ramen |     1\r\n(2 rows)\r\n```\r\nNikhil doesn't get lunch in this story, which is too bad, but is a demonstration of the constraints: not everyone gets what they want.\r\nArjun also doesn't get what he wants, which is ramen, because it isn't stable: the ramen-ya would just hit Frank up and they'd do lunch instead.\r\nIt turns out there aren't other stable matchings for this data, but in general there can be many.\r\n\r\nHow do we arrive at a stable matching?\r\nFortunately, way back in 1962, [Gale and Shapley proposed](https://web.archive.org/web/20170925172517/http://www.dtic.mil/get-tr-doc/pdf?AD=AD0251958) an algorithm to do just that.\r\nIn one variant: each producer proposes to satisfy their favorite consumer, each consumer definitively rejects all but the best proposal, and spurned proposers repeat with their next best options, until the rejections stop or they run out of options.\r\n\r\nIt's pretty much recursion, isn't it? \r\nAnd moreover, each of the steps are pretty easy SQL.\r\nLet's write them down!\r\n\r\n```sql\r\n-- Iteratively develop proposals and rejections.\r\nWITH MUTUALLY RECURSIVE\r\n    -- Pairings that have yet not been explicitly rejected.\r\n    active(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT) AS (\r\n        SELECT * FROM prefs\r\n        EXCEPT ALL\r\n        SELECT * FROM rejects\r\n    ),\r\n    -- Each `name1` proposes to its favorite-est `name2`.\r\n    proposals(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT) AS (\r\n        SELECT DISTINCT ON (name1) *\r\n        FROM active\r\n        ORDER BY name1, pref1, name2, pref2\r\n    ),\r\n    -- Each `name2` tentatively accepts the proposal from its favorite-est `name1`\r\n    tentative(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT) AS (\r\n        SELECT DISTINCT ON (name2) *\r\n        FROM proposals\r\n        ORDER BY name2, pref2, name1, pref1\r\n    ),\r\n    -- Proposals that are not accepted become definitively rejected.\r\n    rejects(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT) AS (\r\n        SELECT * FROM rejects\r\n        UNION ALL\r\n        SELECT * FROM proposals\r\n        EXCEPT ALL\r\n        SELECT * FROM tentative\r\n    )\r\n-- The tentative accepts become real accepts!\r\nSELECT * FROM tentative\r\n```\r\n\r\nEach of these steps--proposal, tentative acceptance, and rejection--follow the written description up above.\r\nThe behavior of the `WITH MUTUALLY RECURSIVE` block is to evaluate each term in order, then repeat from the top, until they stop changing.\r\nIt's worth a moment reading and maybe re-reading the SQL to convince yourself that there is at least some relationship to the written plan.\r\n\r\nIf we run the query, we get the result up above.\r\n```\r\n name1 | pref1 | name2 | pref2 \r\n-------+-------+-------+-------\r\n arjun |     3 | sushi |     4\r\n frank |     4 | ramen |     1\r\n(2 rows)\r\n```\r\n\r\nThese results are great to see, but we are here to *maintain* computation, as input data change.\r\nWe can also [`SUBSCRIBE`](https://materialize.com/docs/sql/subscribe/) to the query, and then modify the input to see some output changes.\r\n\r\nEach subscribe starts with a snapshot, and it should be (and is) the answer just up above.\r\n```\r\n1702997600437\t 1\tarjun\t3\tsushi\t4\r\n1702997600437\t 1\tfrank\t4\tramen\t1\r\n```\r\nTo remind you, or introduce you, `SUBSCRIBE` produces output whose first column is the timestamp of some update event, followed by a change in count (here `1` for both records), followed by payload columns matching what you'd see from a `SELECT` query.\r\n\r\nAt this point, let's introduce the possibility that Frank would happily eat a sandwich instead of ramen.\r\n```\r\nmaterialize=> insert into prefs values ('frank', 2, 'sando', 3);\r\n```\r\nAs soon as I press enter, a bunch of changes spill out of the subscription:\r\n```\r\n1702997625810\t 1\tarjun\t1\tramen\t3\r\n1702997625810\t-1\tarjun\t3\tsushi\t4\r\n1702997625810\t 1\tfrank\t2\tsando\t3\r\n1702997625810\t-1\tfrank\t4\tramen\t1\r\n1702997625810\t 1\tnikhil\t1\tsushi\t5\r\n```\r\nHow do we read this? \r\nArjun has a shuffle where he gains a matching with ramen and yields his sushi seat.\r\nFrank switches to a sandwich from ramen.\r\nAnd Nikhil gets lunch! \r\nSushi isn't happy about it, mind you, but lunch occurs for all producers and consumers.\r\n\r\nImportantly, there is one timestamp (`1702997625810`), indicating that all five changes happen atomically, at exactly the same moment.\r\nNeither producer nor consumer will be over-committed, even for a moment, on account of Materialize doesn't screw around with consistency and correctness.\r\n\r\n### Generalizing Stable Matching\r\n\r\nLet's imagine that each restaurant can serve more than one person, and instead has an integer 'capacity'.\r\nWhat do we need to change about our process?\r\nLet's introduce tables `producer_capacity` and `consumer_capacity`, which each hold a name and an integer capacity.\r\n\r\n```sql\r\n-- Each producer and consumer have an integer number of matches they can participate in.\r\nCREATE TABLE producer_capacity(name TEXT, cap INT);\r\nCREATE TABLE consumer_capacity(name TEXT, cap INT);\r\n```\r\n\r\nWhat we need to tweak about the algorithm is that each producer proposes at their top `cap` opportunities, and each consumer tentatively accepts their top `cap` proposals.\r\n\r\nWhere above we have fragments that look like so, to pick the top singular opportunity,\r\n```sql\r\n    -- Each `name1` 'proposes' to its favorite-est `name2`.\r\n    proposals(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT) AS (\r\n        SELECT DISTINCT ON (name1) *\r\n        FROM active\r\n        ORDER BY name1, pref1, name2, pref2\r\n    ),\r\n```\r\nwe'll want to update these to pick the top `cap` opportunities:\r\n```sql\r\n    -- Each `name1` 'proposes' to its `cap` favorite-est `name2`.\r\n    proposals(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT) AS (\r\n        SELECT lat.* FROM producer_capacity, \r\n        LATERAL (\r\n            -- pick out the best `cap` opportunities\r\n            SELECT * FROM active\r\n            WHERE active.name1 = producer_capacity.name\r\n            ORDER BY active.pref1\r\n            LIMIT producer_capacity.cap\r\n        ) lat\r\n    ),\r\n```\r\nThis new SQL is a bit more complicated than the old SQL, but the `LATERAL` join allows us to invoke `LIMIT` with an argument that depends on `cap` rather than a limit of exactly one that `DISTINCT ON` provides.\r\n\r\nWe'll need to do the same thing for our tentative accepts, using `consumer_capacity`.\r\n```sql\r\n    -- Each `name2` tentatively 'accepts' the proposal from its favorite-est `name1`\r\n    tentative(name1 TEXT, pref1 INT, name2 TEXT, pref2 INT) AS (\r\n        SELECT lat.* FROM consumer_capacity, \r\n        LATERAL (\r\n            -- pick out the best `cap` proposals\r\n            SELECT * FROM proposals\r\n            WHERE proposals.name2 = consumer_capacity.name\r\n            ORDER BY proposals.pref2\r\n            LIMIT consumer_capacity.cap\r\n        ) lat\r\n    ),\r\n```\r\n\r\nWith unit capacities we'll see the same results as before. \r\nHowever, let's introduce Nikhil to ramen, which it turns out he likes.\r\n```\r\nmaterialize=> insert into prefs values ('nikhil', 1, 'ramen', 2);\r\n```\r\nThis has some immediate consequences for our subscription to the matching.\r\nI restarted it because we need to pick up the new query with capacities, but the new snapshot put us right back where we were before.\r\n```\r\n1703011622743\t-1\tarjun\t1\tramen\t3\r\n1703011622743\t 1\tarjun\t3\tsushi\t4\r\n1703011622743\t 1\tnikhil\t1\tramen\t2\r\n1703011622743\t-1\tnikhil\t1\tsushi\t5\r\n```\r\nThis dislodges Arjun, who is now back on the sushi plan, because the ramen folks are fully occupied. \r\nBut only because they are occupied.\r\nLet's update their capacity to two, which should give Arjun a seat.\r\n```\r\nmaterialize=> update consumer_capacity set cap = 2 where name = 'ramen';\r\n```\r\n```\r\n1703011679155\t 1\tarjun\t1\tramen\t3\r\n1703011679155\t-1\tarjun\t3\tsushi\t4\r\n```\r\nAnd, to rattle things a bit more let's imagine the sandwich shop is sold out and their capacity drops down to zero.\r\n```\r\nmaterialize=> update consumer_capacity set cap = 0 where name = 'sando';\r\n```\r\n```\r\n1703011883207\t-1\tarjun\t1\tramen\t3\r\n1703011883207\t 1\tarjun\t3\tsushi\t4\r\n1703011883207\t-1\tfrank\t2\tsando\t3\r\n1703011883207\t 1\tfrank\t4\tramen\t1\r\n```\r\nPoor Arjun is just getting bounced around. \r\nHe decides he really wants some ramen, and offers a cash incentive which updates their preference for him dramatically. \r\nWe'll model this by just tweaking their preference directly.\r\n\r\n```\r\nmaterialize=> update prefs set pref2 = 1 where name1 = 'arjun' and name2 = 'ramen';\r\n```\r\n```\r\n1703012011622\t 1\tarjun\t1\tramen\t1\r\n1703012011622\t-1\tarjun\t3\tsushi\t4\r\n1703012011622\t-1\tnikhil\t1\tramen\t2\r\n1703012011622\t 1\tnikhil\t1\tsushi\t5\r\n```\r\nAnd Arjun is back on ramen and Nikhil is back on sushi.\r\n\r\n### Recursive SQL and Doing Business\r\n\r\nThere are lots of changes the input may experience, many of which lead to changed output.\r\nLike in life, the world changes around you and you may need to promptly update your plans for the world.\r\nMaterialize and recursive SQL are here to make sure you are always looking at the correct output, moment by moment.\r\n\r\nWe've seen an example of using SQL for one problem that is fundamental in economics: stable matching (with capacities).\r\nThis certainly isn't the only problem in economics, nor even the most significant business problem you'll have, but it does show off a potentially new use of recursive SQL to solve the problem.\r\nOther problems, similar and different, have natural solutions with recursive SQL that you might not have imagined, and you wouldn't be able to access with vanilla SQL.\r\n\r\n<!-- ##{'timestamp':1702962000}## -->\u3002", "top": 0, "createdAt": 1702962000, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2023-12-19", "dateLabelColor": "#A333D0"}, "P5": {"htmlDir": "docs/post/Materialize and Memory.html", "labels": ["documentation"], "postTitle": "Materialize and Memory", "postUrl": "post/Materialize%20and%20Memory.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/5", "commentNum": 0, "wordCount": 14908, "description": "Materialize keeps your SQL views up to date as the underlying data change.\r\nThe value Materialize provides comes from how promptly it reflects new data, but its *cost* comes from the computer resources needed to achieve this.\r\nWhile we often talk about the value Materialize provides, and work continually to improve it, we are also hard at work continually reducing the cost.\r\nThis work has had marked impact recently, and it felt like a great time to tell you about it, and the reductions in cost. \r\n\r\nMaterialize maintains your source and derived data (e.g. any materialized view), durably in economical cloud storage.\r\nHowever, to promptly maintain views and serve results we want to use much more immediately accessible storage.\r\nThis storage, memory or as we'll see soon local disk, acts as a cache that must be fast, but needn't be durable.\r\nAnd of course, we would all like it to be as economical as possible.\r\n\r\nWe've been dialing down the amount of 'overhead' associated with each intermediate maintained record in Materialize.\r\nWe started some months ago at roughly 96 bytes of overhead (we will explain why), and we are now closing in on between 0 and 16 bytes of overhead, depending.\r\nThis first wave of results have already seen many users memory requirements reduced by nearly 2x.\r\nMoreover, we've laid the groundwork for further improvements, through techniques like spill-to-disk, columnar layout, and compression.\r\nThis further work comes at the cost of CPU cycles, but for the moment CPU cycles are abundant (and elastic) in a way that bytes of memory are not.\r\n\r\nIn this post we'll map out where we started, detail the relatively simple steps we've taken to effectively reduce the overhead, and sketch the future we've opened up with some help from Rust.\r\n\r\n### The Fundemantals of Remembered Things\r\n\r\nMaterialize models all data as relational rows, each of which has some number of columns, each of which contains one of a few different types of data.\r\nOver time the rows come and go, each changing their multiplicity through what we call 'updates': triples `(data, time, diff)`.\r\nEach update indicates a row `data` that at some moment `time` experiences a change `diff` in its multiplicity.\r\nThese changes are often `+1` (insertion) or `-1` (deletion), or a mix of two or more (updates).\r\n\r\nMaterialize maintains *indexed state* by viewing each `data` as a pair `(key, val)`, where `key` are some signified columns and `val` the remaining columns.\r\nWhen you create an index on a collection of data, you specify columns by which you hope to access the data; those columns define `key` and `val` for each `data`.\r\nWe regularly want to fetch the history of some `key`: the associated `val`s and the `(time, diff)` changes they have undergone.\r\n\r\nThe abstract data type we use maps from `key` to `val` to a list of `(time, diff)` pairs.\r\nIn Rust you might use the `HashMap` type to support this abstraction:\r\n```rust\r\n/// Map from key, to value, to a list of times and differences.\r\ntype Indexed<K, V, T, D> = HashMap<K, HashMap<V, Vec<(T, D)>>>;\r\n```\r\n\r\nFor various reasons we won't actually want to use `HashMap` itself, and instead prefer other data structures that provide different performance characteristics.\r\nFor example, we are interested in minimizing the number and size of allocations, and optimizing for both random and sequential read and write throughput.\r\n\r\n### A First Evolution, circa many years ago\r\n\r\nDifferential dataflow's fundamental data structures are thusfar based on sorted lists.\r\nAll of differential dataflow's historical performance, which has been pretty solid, has been based on [the perhaps surprising efficiency of sorted memory access](https://github.com/frankmcsherry/blog/blob/master/posts/2015-08-15.md).\r\nYou may have thought we were going to impress you with exotic improvements on Rust's `HashMap` implementation, but we are going to stay with sorted lists.\r\n\r\nIn the context of space efficiency, sorted lists have a compelling property that Rust's `HashMap` does not have: you can append multiple sorted lists into one larger list, and only need to record the boundaries between them.\r\nThis reduces the per-key, and per-value overhead to something as small as an integer.\r\nYou do miss out on some random access performance, but you also gain on sequential access performance.\r\nFor the moment though, we're interested in space efficiency.\r\n\r\nTo store the map from `key` to `val` to list of `(time, diff)` updates, differential dataflow uses roughly three vectors:\r\n```rust\r\n/// Simplification, for clarity.\r\nstruct Indexed<K, V, T, D> {\r\n    /// key, and the start of its sorted run in `self.vals`.\r\n    keys: Vec<(K, usize)>,\r\n    /// val, and the start of its sorted run in `self.upds`.\r\n    vals: Vec<(V, usize)>,\r\n    /// lots and lots of updates.\r\n    upds: Vec<(T, D)>,\r\n}\r\n```\r\n\r\nEach key is present once, in sorted order. \r\nThe `usize` offset for each key tells you where to start in the `vals` vector, and you continue until the offset of the next key or the end of the vector.\r\nThe `usize` offset for each value tells you where to start in the `upds` vector, and you continue until the offset of the next value or the end of the vector.\r\n\r\nThe data structure supports high throughput sequential reads and writes, random access reads through binary search on keys, and random access writes through a [log-structure merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) idiom (although perhaps 'merge-list' is more appropriate).\r\n\r\nThe overhead is one `usize` for each key, and another `usize` for each distinct `(key, val)` pair.\r\nYou have three allocations, rather than a number proportional to the number of keys or key-value pairs.\r\nThe overhead seems pretty small, until we perform a more thorough accounting.\r\n\r\n### A More Thorough Accounting\r\n\r\nAlthough Materialize maintains only two `usize` (each 8 bytes) beyond the `K`, `V`, `T`, and `D` information it needs for updates, there is more overhead behind the scenes.\r\n\r\nIn Materialize both `K` and `V` are `Row` types, which are variable-length byte sequences encoding column data.\r\nIn Rust a `Vec<u8>` provides a vector of bytes, and takes 24 bytes in addition to the binary data itself.\r\nIn fact we have used a 32 byte version that allows for some amount of in-line allocation, but meant that the minimum sizes of `K` plus `V` is 64 bytes, potentially in addition to the binary row data itself.\r\n\r\nBoth `T` and `D` are each 8 byte integers, because there are many possible times, and many possible copies of the same record.\r\nAdding these together, we get an overhead accounting of\r\n```\r\nkey offset:  8 bytes\r\nval offset:  8 bytes\r\nkey row:    32 bytes\r\nval row:    32 bytes\r\ntime:        8 bytes\r\ndiff:        8 bytes\r\n--------------------\r\noverhead    96 bytes\r\n```\r\nThe minimum buy-in for each update is 96 bytes.\r\nThese 96 bytes may cover no actual row data, and can just be pure overhead.\r\n\r\n### Optimization\r\n\r\nFortunately, the more thorough accounting leads us to a clearer understanding of opportunities.\r\nEvery byte that is not actual binary payload is in play as optimization potential.\r\nLet's discuss a few of the immediate opportunities.\r\n\r\n#### Optimizing `(Time, Diff)` for Snapshots\r\n\r\nMaterialize first computes and then maintains SQL views over your data.\r\nA substantial volume of updates describe the data as it initially exists, an initial 'snapshot', before changes start to happen.\r\nAs changes happen we continually roll them up into the snapshot, so even a live system has a great deal of 'snapshot' updates.\r\n\r\nThe snapshot updates commonly have `(time, diff)` equal to `(now, 1)`.\r\nThat is, each `(key, val)` pair in the snapshot exists 'right now', and just once. \r\nThis provides an opportunity for bespoke compression: if a `(time, diff)` pair repeats we are able to avoid writing it down repeatedly.\r\nIn fact, we can sneak this in at zero overhead by taking advantage of a quirk in our `usize` offsets: they *should* always strictly increase to indicate ranges of updates, because empty ranges should not be recorded, but we can use a repetition (a non-increase) to indicate that the preceding updates should be reused as well.\r\n\r\nThis typically saves 16 bytes per update for the snapshot, and brings us down to 80 bytes of overhead.\r\n```\r\nkey offset:  8 bytes\r\nval offset:  8 bytes\r\nkey row:    32 bytes\r\nval row:    32 bytes\r\n--------------------\r\noverhead:   80 bytes\r\n```\r\n\r\n#### Optimizing `Row` representation\r\n\r\nAlthough we have a 32 byte `Row` we could get by with much less.\r\nJust like we appended lists and stored offsets to track the bounds, we could append lists of bytes into one large `Vec<u8>` and maintain only the `usize` offsets that tell us where each sequence starts and stops.\r\n\r\nThis takes us from 32 bytes with the option for in-line allocation, to 8 bytes without that option.\r\nThis applies twice, once to each of `key` and `val`.\r\nMoreover, we avoid an *allocation* for each `key` and `val`, which evades some further unaccounted overhead in and around memory management.\r\nWe now have four offsets, two for each of `key` and `val`, which will be important next.\r\n```\r\nkey offset:     8 bytes\r\nval offset:     8 bytes\r\nkey row offset: 8 bytes\r\nval row offset: 8 bytes\r\n-----------------------\r\noverhead:      32 bytes\r\n```\r\n\r\n#### Optimizing `usize` Offsets\r\n\r\nOur `usize` offsets take 8 bytes, but rarely get large enough to need more than 4 bytes.\r\nThis is because we end up 'chunking' our data to manageable sizes, and those chunk sizes rarely exceed 4GB, for which a `u32` would be sufficient.\r\nRather than use a `Vec<usize>` to store these offsets, we can first use a `Vec<u32>`, and should we exceed 4 billion-ish we can cut-over new elements to a `Vec<u64>`.\r\n\r\nThis shaves the four `usize` offsets down from 32 bytes to 16 bytes, in most cases.\r\n```\r\nkey offset:     4 bytes\r\nval offset:     4 bytes\r\nkey row offset: 4 bytes\r\nval row offset: 4 bytes\r\n-----------------------\r\noverhead:      16 bytes\r\n```\r\n\r\nGoing even further, these offsets often have very simple structure.\r\nWhen there is exactly one value for each key (e.g. as in a primary key relationship) the key offsets are exactly the sequence 0, 1, 2, ...\r\nWhen considering the snapshot, the value offsets are all zero (recall that repetitions indicate repeated `(time, diff)` pairs).\r\nWhen the binary slices have the same length (e.g. for fixed-width columns) the corresponding row offsets are the integer multiples of this length.\r\nEach of these cases can be encoded by a single 'stride' and a length, using two integers in total rather than any per element.\r\n\r\nThese further optimizations can bring the 16 bytes of overhead down, all the way to zero when stars align.\r\n\r\n### Further Optimization and Future Work\r\n\r\nWith nearly zero overhead, you may be surprised to learn that we are not yet done.\r\nBut in fact, there is still opportunity to further reduce cost!\r\n\r\n#### Paging Binary Data to Disk\r\n\r\nMaterialize, by way of differential dataflow, performs its random accesses in a way that [resembles sequential scans](https://github.com/frankmcsherry/blog/blob/master/posts/2015-08-15.md) (essentially: batching and sorting look-ups before they happen).\r\nThis means that putting binary payload data on secondary storage like disk is not nearly as problematic as it would be were we to access it randomly, as in a hash map.\r\nDisk is obviously substantially cheaper than memory, and it provides the opportunity to trade away peak responsiveness for some cost reduction.\r\n\r\nIn fact we've recently done this, backing in-memory allocations with disk allocations that Linux can spill to if it feels memory pressure.\r\nExpect a post in the near future talking about the design and implementation of this paging layer.\r\n\r\nOur experience so far is that initial snapshot computation experiences almost no degradation (the batch disk accesses are sequential scans), and once up and running update volumes are often low enough volume that local SSD accesses do not prevent timely results.\r\nThe local disks are ephemeral caches, and don't come at the same cost as more durable options like cloud block storage.\r\n\r\n\r\n#### Columnar Compression\r\n\r\nRust has some [handy mechanisms](https://blog.rust-lang.org/2022/10/28/gats-stabilization.html) that allow us to interpose code between the binary data for each row and the SQL logic that will respond to the row data.\r\nOur logic expects each row only as a sequence of `Datum` column values, and doesn't require an actual contiguous `[u8]` binary slab.\r\nThis allows us some flexibility in how we record each row, potentially as a `[u8]` sequence, but also potentially re-ordered, transformed, or compressed.\r\n\r\nCloud Data Warehouses often record their data in [columns](https://en.wikipedia.org/wiki/Column-oriented_DBMS), rather than rows, to improve their space efficiency while sacrificing performance for random access.\r\nWe don't want to sacrifice too much random access, but we can employ several of the same compression tricks.\r\nIn particular, we are able to sneak in various techniques, from [entropy coding](https://en.wikipedia.org/wiki/Entropy_coding) like Huffman and ANS, to [dictionary coding](https://en.wikipedia.org/wiki/Dictionary_coder) which often works well on denormalized relational data.\r\nMoreover, we can apply these techniques column-by-column, as columns often exhibit more commonality than do rows.\r\n\r\nThe benefits of compression depend greatly on the nature of the data itself, and come at a non-trivial CPU overhead, but would unlock significant space savings and further opportunities.\r\n\r\n#### Query Optimization\r\n\r\nA final, evergreen opportunity is to continue to reduce the amount of information we need to maintain, independent of how it is represented.\r\nMaterialize's optimizer pays specific attention to the amount of information maintained, which distinguishes it from most query optimizers that aim primarily to reduce query time.\r\nHow and where we maintain state is very much under our control, and something we still have many opportunities to improve.\r\n\r\n### Wrapping Up\r\n\r\nMaterialize provides value through the information it maintains, at the expense of maintaining intermediate state in scarce and costly storage (at least, relative to cloud blob storage).\r\nThe cost of the storage can't be overlooked, and driving it down makes the provided value net out positive for even more use cases.\r\nIn the limit, we'll get you to expect everything to be always up to date, because why shouldn't it be? \r\n\r\nThe cost optimizations described above are all live in Materialize now.\r\nIt would be interesting to invite you to see the before and after, but actually we'd love to introduce you to the after, and let you see the next tranche of improvements as they happen.\r\nTo try out Materialize, sign up at https://materialize.com/register/!\r\n\r\n<!-- ##{'timestamp':1703048400}## -->\u3002", "top": 0, "createdAt": 1703048400, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2023-12-20", "dateLabelColor": "#A333D0"}, "P4": {"htmlDir": "docs/post/Materialize and Advent of Code.html", "labels": ["documentation"], "postTitle": "Materialize and Advent of Code", "postUrl": "post/Materialize%20and%20Advent%20of%20Code.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/4", "commentNum": 0, "wordCount": 16069, "description": "\r\n\r\nThis past year Team Materialize struck out to do each day of 2023's [Advent of Code](https://adventofcode.com/2023), an annual programming event with thought-provoking problems that you are encouraged to approach from non-standard directions.\r\nWe figured we'd try and use SQL for the whole thing.\r\n\r\nSQL is a bold choice because it is meant for querying data, and not as much for general computation.\r\nSeveral of the problems call for interesting algorithms, specific data structures, and some flexibility.\r\nHowever, Materialize's core thesis is that you can do so much more with SQL that just query your data.\r\nIf you want to move operational logic from bespoke code into SQL, you'll need to be able to express that logic.\r\nAnd so, Advent of Code was a great opportunity to stretch our legs, and fingers, and see just how much logic fits into SQL.\r\n\r\n### Preliminaries\r\n\r\nThere's a lot of content in the month's problems.\r\nThere are 49 problems, and although there is some overlap really there is too much to say about all of them.\r\nWe aren't going to recount each of the problems, the whimsical backstories, and the shape of the problem inputs.\r\nWe'll try and flag some surprising moments, though, and you should dive into those problems if you are keen (they can each be done on their own).\r\n\r\nI (Frank) wrote all of my solutions using Materialize's `WITH MUTUALLY RECURSIVE` even when recursion was not required.\r\nThis just helped me start writing, as the blocks allow you to just start naming subqueries and writing SQL.\r\n\r\nMy solutions all had the same skeletal structure:\r\n```sql\r\nWITH MUTUALLY RECURSIVE\r\n\r\n    -- Parse the problem input into tabular form.\r\n    lines(line TEXT) AS ( .. ),\r\n\r\n    -- SQL leading up to part 1.\r\n    part1(part1 BIGINT) AS ( .. ),\r\n\r\n    -- SQL leading up to part 2.\r\n    part2(part2 BIGINT) AS ( .. ) \r\n\r\nSELECT * FROM part1, part2;\r\n```\r\n\r\nAs mentioned, we won't always need recursion.\r\nHowever, we often do use recursion, and may even need it.\r\nWe'll call this out, as the use (and ease) of recursion in SQL was one of the main unlocks.\r\n\r\n### Week one\r\n\r\n[Day one](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week1/aoc_1201.md) was largely about text manipulation, specifically extracting numbers from text, and was well-addressed by using regular expressions to manipulate and search the text. \r\n\r\n[Day two](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week1/aoc_1202.md) was largely about aggregation: rolling up counts and maxima for games involving numbers of colored cubes; SQL did great here.\r\n\r\n[Day three](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week1/aoc_1203.md) has inputs in grid form, where there can be interaction between multiple lines (with symbols above or below others). \r\nYou are looking for runs of numerals, and I used `WMR` to track these down; reportedly you can also use regular expressions, but I was not clever enough for that!\r\n\r\n[Day four](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week4/aoc_1225.md) introduced scratch cards where each line of input has some winners and losers. \r\nThis was easy SQL until part two, in which winners give you other scratch cards, which have winners that give you other scratch cards, which .. you can see the recursion. \r\nDespite being wordy and complicated, the SQL isn't so bad:\r\n```sql\r\n    -- PART 2\r\n    -- Each card provides a copy of the next `score` cards.\r\n    expanded(card INT, score BIGINT) AS (\r\n        SELECT * FROM matches\r\n        UNION ALL\r\n        SELECT\r\n            matches.card,\r\n            matches.score\r\n        FROM\r\n            expanded,\r\n            matches,\r\n            generate_series(1, expanded.score) as step\r\n        WHERE\r\n            expanded.card + step = matches.card\r\n    ),\r\n    part2(part2 BIGINT) AS ( SELECT COUNT(*) FROM expanded)\r\n```\r\nThis would be tricky to do with non-recursive SQL, as the data itself tells us how to unfold the results.\r\nHooray for recursion!\r\n\r\n[Day five](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week1/aoc_1205.md) was a bit of a bear.\r\nIt was the same day we were doing a Materialize on-site and we were all a bit distracted, but also it was pretty beefy. \r\nYou first have to 'route' various elements through a sequence of remappings, whose length is defined in the data.\r\nYou then have to expand that out to routing whole intervals (rather than elements), and .. there is just lots of potential for error.\r\nI used recursive SQL to handle all the remapping, but other folks just expanded out their SQL for each of the (ten-ish) remappings.\r\n\r\n[Day six](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week1/aoc_1206.md) was about whether you knew (or were willing to learn about) the [quadratic formula](https://en.wikipedia.org/wiki/Quadratic_formula).\r\n\r\n[Day seven](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week1/aoc_1207.md) is about scoring poker hands, using some new rules for tie breaking. \r\nThis was mostly SQL aggregation, as the numbers of each card in each hand largely determine the outcome, other than tie-breaking where I learned about the [`translate`](https://materialize.com/docs/sql/functions/#translate) function.\r\n\r\n### Week two\r\n\r\n[Day eight](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week2/aoc_1208.md) involved some graph navigation (recursion), and some mathematics.\r\nThe mathematics were of the form 'notice that various things are relatively prime', and it was important to rely on SQL as a tool to support reasoning, as opposed to directly attacking the specified computation.\r\nIn this case, my problem called for 14,935,034,899,483 steps, and no tool is going to make direct simulation be the right answer.\r\n\r\n[Day nine](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week2/aoc_1209.md) was a refreshing introduction to polynomials, and how if you take enough derivatives of them they end up at zero.\r\nThe task was to do this, repeatedly difference adjacent measurements, or adjacent differences, etc., until you get all zeros.\r\nThen, integrate back up to get projections in the forward and reverse direction.\r\nI used recursion here to accommodate the unknown degree of the polynomial (somewhere in the twenties).\r\n\r\n[Day ten](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week2/aoc_1210.md) presents you with a grid of pipe (symbols `|`, `-`, `J`, `7`, `F`, and `L`), and questions about how long a loop of pipe is, and then how many cells are contained within it. The first part involved recursion, and I used it again for a dynamic programming solution to the second part.\r\n\r\n[Day eleven](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week2/aoc_1211.md) presents a grid of 'galaxies' and has you calculate the distance between pairs (the L1 or 'Manhattan' distance, always the sum of absolute values of coordinate differences). \r\nParts one and two were the same, but with different magnitudes of numbers.\r\nNo recursion here!\r\n\r\n[Day twelve](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week2/aoc_1211.md) was about sequence alignment, matching partial observations with hard constraints.\r\nDynamic programming was a great solution here, using recursion.\r\n\r\n[Day thirteen](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week2/aoc_1213.md) had grids of observations with the hypothesis that each is mirrored, horizontally or vertically, at some point that you need to find.\r\nSQL and subqueries were a great way to validate hypothetical mirroring axes.\r\n\r\n[Day fourteen](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week2/aoc_1214.md) was a treat, in that it used *nested* recursion: a `WMR` block within a `WMR` block.\r\nThe problem was simulation of rocks that roll in cardinal directions, changing the direction ninety degrees, and repeating.\r\nEach simulation was recursive (rocks roll until they stop), and we were meant to repeat the larger progress a great many times (1,000,000,000 cycles).\r\nThe only bummer here was the amount of copy/paste re-use, as each of the four cardinal directions had different subqueries.\r\n\r\n### Week three\r\n\r\n[Day fifteen](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week3/aoc_1215.md) has you implement a hash function, and then a hash map.\r\nRecursion was a handy way to walk through the input to be hashed, though the hash function was simple enough that you could have used math directly instead. \r\nThe second part (the hash map) did not require recursion, as rather than simulate the operations you could leap to the final state you were looking for.\r\n\r\n[Day sixteen](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week3/aoc_1216.md) was about bouncing light around in a grid, and seeing how many grid cells are illuminated.\r\nThe illumination process was classic recursive SQL, where you keep expanding `(row, col, dir)` triples until the set reaches a fixed point.\r\nIn the second part the light sources had an origin, which is just a fourth column to add, tracking the source of each ray of light.\r\n\r\n[Day seventeen](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week3/aoc_1217.md) is a pathfinding problem, with constraints on how you move around the path (not too short or too long in any direction at once).\r\nClassic recursive SQL to implement [Bellman-Ford](https://en.wikipedia.org/wiki/Bellman\u2013Ford_algorithm).\r\n\r\n[Day eighteen](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week3/aoc_1218.md) provides instructions of how a digger will move around, excavating earth, and asks you to calculate the area.\r\nThis is an opportunity to learn about the [Trapezoid formula](https://en.wikipedia.org/wiki/Shoelace_formula#Trapezoid_formula) for computing the area as the addition and subtraction of trapezoid areas.\r\n\r\n[Day nineteen](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week3/aoc_1219.md) sneakily introduces you to [binary space partitioning](https://en.wikipedia.org/wiki/Binary_space_partitioning), where rules based on inequality tests route you to new rules, until eventually you reach some rule that says 'accept' or 'reject'.\r\nThis was all pretty easy, except for a substantial amount of SQL overhead related to the various symbols and characters and coordinates all of which required their own columns.\r\n\r\n[Day twenty](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week3/aoc_1220.md) presents you with the simulation of an asynchronous circuit, and this is the day that almost broke me.\r\nMechanically the SQL isn't that complicated, but *debugging* the SQL was a real challenge.\r\nIt got done over the course of a quite long train ride into the evening.\r\n\r\n[Day twenty-one](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week3/aoc_1221.md) was another example of some (recursive) SQL for grid exploration, followed by some mathematics.\r\nIn this case the grid exploration was standard, determining reachable locations on the grid, and then the math was quadratic extrapolation from a sequence of measurements (to something too large to actually evaluate, an answer of 621,289,922,886,149 reachable states).\r\n\r\n### Week four\r\n\r\nThe last week was shorter, but also culminated in some pretty exciting problems and techniques.\r\n\r\n[Day twenty-two](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week4/aoc_1222.md) had shapes made of cubes falling into a well, and coming to rest on others (or the ground).\r\nThere were then questions about how many pieces are load bearing, and also for each load bearing piece how many others would fall if they were removed.\r\nDropping the pieces used recursive SQL, determining the load bearing pieces did not, but then scoring the load bearing pieces again required recursion.\r\n\r\n[Day twenty-three](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week4/aoc_1223.md) is a classic example of finding the 'longest path' in a directed graph.\r\nThis is a relatively easy problem when the input is acyclic (part one), and it is NP-hard when the input may have cycles (part two).\r\nPart one was a mostly vanilla recursive SQL query, and part two encoded the 32 prior state options in a large integer and just did a lot of work.\r\n\r\n[Day twenty-four](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week4/aoc_1224.md) had most folks reach for a numerical solver, something like Mathematica or z3.\r\nThat is less easy in SQL, and I needed to learn some math instead (specifically how to find the intersection of two line segments).\r\nAlthough part two seemed quite complex, it ended up being relatively easy when you realize a few simplifications (an added dimension that can be ignored until the end, allowing you to re-use part one).\r\n\r\n[Day twenty-five](https://github.com/MaterializeInc/advent-of-code-2023/blob/main/week4/aoc_1225.md) asked for a minimum graph cut (of three edges).\r\nThis is a standard optimization problem, but rather than try to implement the [Stoer-Wagner algorithm](https://en.wikipedia.org/wiki/Stoer\u2013Wagner_algorithm) I went with something from my PhD thesis: partitioning the graph based on the [Fiedler vector](https://en.wikipedia.org/wiki/Algebraic_connectivity#Fiedler_vector).\r\nIt turns out this gave the right answer on the first try, and the holidays were saved!\r\n\r\n## Conclusions\r\n\r\nThe exercise was certainly helpful and informative, on multiple levels.\r\n\r\nFirst, it really reinforced for me that `WITH MUTUALLY RECURSIVE` is a very valuable tool to have access to when faced with a new problem.\r\nOften your problem is a bunch of joins and reductions, but when it isn't you are immediately in a bit of a pickle.\r\nIn most cases, algorithmic challenges immediately gave way to recursive SQL.\r\n\r\nThat being said, there's clearly an accessibility gap when reaching for recursive SQL.\r\nI find the idioms approachable, but I've spent a while working with data-parallel algorithms, and have seen several of the tricks.\r\nThere's still plenty of work to do before the casual SQL author feels comfortable with recursive SQL.\r\n\r\nSecond, the majority of my time was spent *debugging* rather than authoring.\r\nThis is a classic challenge with declaritive languages, who go from input program to output data in often inscrutable ways.\r\nI borrowed some techniques from [debugging Datalog](https://yanniss.github.io/DeclarativeDebugging.pdf), but ideally the system itself would help me with this (and several research systems do provide integrated lineage).\r\n\r\nDebugging the logic of SQL queries only gets harder when the data are changing underneath you.\r\nTechniques like spot checking data become infeasible when the data changes faster than you can observe records that are meant to line up.\r\nMaterialize should help in these cases, with maintained diagnostic views that represent assertions, or better violations thereof, whose contents spell out records that at some moment violated something that was meant to be true.\r\nMaterialize's `SUBSCRIBE` provides a full account of these views, reporting records that existed even for a moment, where anything other than 'always empty' represents an error in your SQL (or your assertions).\r\n\r\nThird, using Materialize in new and weird ways shook out several bugs.\r\nWe've already fixed them.\r\nDogfooding your own product, especially in surprising contexts, is a great way to forcibly increase your test coverage.\r\nIssues ranged from the silly ('why would you name a table `count`?') to the abstruse (doubly nested recursive SQL blocks), but they spilled out in the early days and became less frequent as the weeks went on.\r\n\r\nFinally, the main conclusion was that it was all possible.\r\nDespite substantial anxiety about whether and when we would need to bail out, defeated, the whole project did work out.\r\nWe were able to express a rich variety of computational tasks as data-driven SQL both expressed and maintained by Materialize.\r\n<!-- ##{'timestamp':1704171600}## -->\u3002", "top": 0, "createdAt": 1704171600, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-01-02", "dateLabelColor": "#bc4c00"}, "P3": {"htmlDir": "docs/post/Computing and Maintaining Weird (Outer) Joins.html", "labels": ["documentation"], "postTitle": "Computing and Maintaining Weird (Outer) Joins", "postUrl": "post/Computing%20and%20Maintaining%20Weird%20%28Outer%29%20Joins.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/3", "commentNum": 0, "wordCount": 24914, "description": "[Differential dataflow](https://github.com/TimelyDataflow/differential-dataflow) has a single join operator: `join`.\r\nIt takes two input collections, and for each `(key, val1)` and `(key, val2)` in the inputs it produces `(key, (val1, val2))` in the output.\r\nThis makes `join` a 'binary equijoin', where it fishes out exactly the exact matches on `key`.\r\nThis restriction is important, and powerful: when either input experiences a change, the `key` of the change is what directs us to the (other) input records that will help us produce the appropriate output change.\r\nHowever, there are other 'joins' in the larger relational data world, and we need to support them as well.\r\n\r\nIn this post we'll build up an implementation of a **multi-way outer equijoin**.\r\nWe'll start small, but arrive at the best way I know how to build these beasts out of existing parts.\r\nAlong the way, we'll \r\n    get an introduction to how differential dataflow works, \r\n    develop several ways to use it to implement joins of various stripes, and\r\n    deploy these techniques together to take on the outer-est of (equi-)joins.\r\n\r\nAmazingly, to me at least, we end up needing to understand how to efficiently implement multi-way joins of sums of terms.\r\nThat is, how to efficiently implement\r\n```\r\n(A0 + A1 + A2) \u22c8 (B0 + B1 + B2) \u22c8 (C0 + ...) \u22c8 ...\r\n```\r\nTo be honest, I can't recall this pattern from my database education (such as it was), and I'd love any tips or pointers about where else this shows up.\r\nIf you get to the end and it all checks out as old-hat for you, I'd love to know about it!\r\n\r\n### Differential Dataflow and the Binary Equijoin `join`\r\n\r\nDifferential dataflow is a framework for computing and then maintaining functions over continually changing volumes of data.\r\nIt manipulates *updates* to data, written as triples `(data, time, diff)` and indicating that at `time` the number of occurrences of `data` changes by `diff`.\r\nDifferential dataflow provides primitive operators like `map`, `filter`, `join`, `reduce`, and `iterate`, which users compose to build more complex functions.\r\nEach operator translates input updates into the output updates that would result from continually re-evaluating the operator at every time. \r\nSimilarly, the composed dataflow of operators similarly produces output updates that correspond exactly to continual reevaluation on the changing inputs.\r\n\r\nThe `join` operator applies to two input collections, for which their `data` have the shape `(key, _)`: pairs of some common 'key' type and potentially unrelated 'value' types.\r\nThe intended output is a tuple `(key, (val1, val2))` for each pair of inputs that have a matching `key`.\r\nThe output updates can be derived from first principles, but with enough head-scratching one can conclude that each pair of updates with matching key produces one output update:\r\n\r\n```   \r\n    update1: ((key, val1), time1, diff1)     -- First input\r\n    update2: ((key, val2), time2, diff2)     -- Second input\r\n-> \r\n    ((key, (val1, val2)),  max(time1, time2),  diff1 * diff2)\r\n     \\-- output data --/   \\----  time ----/   \\--  diff --/\r\n```\r\n\r\nWe can respond to each input update by iterating over the updates in the *other* input with the same key, and use the rule above.\r\nThere are smarter ways to do this, consider for example the second update introducing and retracting a record before `time1`: we would produce two outputs that exactly cancel.\r\nIn any case, we'll need to retain *some* information about each input, ideally arranged by `key` so that these updates can be efficiently retrieved.\r\n\r\nDifferential dataflow has a primitive called an 'arrangement', which is both a stream of updates and a maintained indexed form of their accumulation.\r\nAn arrangement translates a stream of updates into a sequence of indexed 'batches' of updates, each of which are indexed by `key`.\r\nIt also maintains a collection of these batches that serve as an indexed roll-up of the accumulated updates, using a structure analogous to a [log-structured merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree).\r\nArrangements are the primary mechanism to maintain 'state' as a dataflow runs, and specifically are what `join` uses: each input to `join` must be an arrangement, and if they are not then they will be arranged for you.\r\n\r\n### Technique 1: Shared Arrangements\r\n\r\nA key advantage to using arrangements is that they can be [*shared*](http://www.vldb.org/pvldb/vol13/p1793-mcsherry.pdf).\r\nArranged data can be used by any number of dataflows, avoiding the cost of an additional redundant arrangement.\r\nAs an example, imagine we have a collection of link data `(source, target)`, and we would like to compute and maintain those identifiers within three steps of some query set `query`.\r\nIf the data are arranged, say in an arrangement named `links`, we could write\r\n```rust\r\n// Join `query` against `links` three times, giving\r\n// the identifiers three steps away from each query.\r\nquery.map(|query| (query, query))\r\n     .join(links).map(|(step0, (query, step1))| (step1, query))\r\n     .join(links).map(|(step1, (query, step2))| (step2, query))\r\n     .join(links).map(|(step2, (query, step3))| (step3, query))\r\n```\r\nThis fragment would naively require six arrangements, two for each `join` invocation.\r\nHowever, we are able to re-use the `links` arrangement at no cost, and instead only introduce three arrangements, corresponding to the number of steps (0, 1, and 2) out from `query`.\r\nThese new arrangements can be substantially smaller than `links`, and the amount of work required to compute and maintain the results can be trivial even when `links` is enormous.\r\n\r\n### Technique 2: Functional Joins\r\n\r\nThis one is a bit of cheat, in that by the end of it you may not be sure it is even a join.\r\n\r\nThere are times, and we will see them coming up, where we want to join not against *data* but against a *function*.\r\nFor example, perhaps we have a collection of `(line, text)` of pairs of integers and strings, and we would like to split each `text` into the words it contains.\r\nOne way to do this is with `join`: the first input is our lines of text, and the second input is the quite large collection of pairs `(text, (pos, word))` each indicating a word that can be found in `text`.\r\n\r\nRather than hope to implement this with `join`, because we couldn't hope to maintain the second collection, we could implement this with the `flat_map` operator instead.\r\n```rust\r\n// Convert each `text` into the words it contains.\r\nlines.flat_map(|(line, text)| \r\n    text.split_whitespace()\r\n        .enumerate()\r\n        .map(|(pos, word)| (line, text.clone(), (pos, word.to_owned())))\r\n)\r\n```\r\n\r\nAt this point you may be wondering why we have called this a 'functional join' rather than a 'flat map'.\r\nYou are not wrong that `flat_map` is the best way to implement this.\r\nHowever, we will need to prepare ourselves to see this pattern in joins, and understand that it is one way to implement something that may present as a `join`.\r\nEach input record results in zero or many output records, determined by some key fields in the record.\r\n\r\n### Technique 3: Multi-way Joins\r\n\r\nEven managing a single join can be challenging, but invariably folks actually want to perform multiple joins at once.\r\nRecall our `query` and `links` example, from just up above\r\n```rust\r\n// Join `query` against `links` three times, giving\r\n// the identifiers three steps away from each query.\r\nquery.map(|query| (query, query))\r\n     .join(links).map(|(step0, (query, step1))| (step1, query))\r\n     .join(links).map(|(step1, (query, step2))| (step2, query))\r\n     .join(links).map(|(step2, (query, step3))| (step3, query))\r\n```\r\nThis performs three joins, and introduces new arrangements for the left inputs of each of the three `join` calls.\r\nWe argued that this could be small if `query` is small, and also if each of the intermediate results are small.\r\nBut if this isn't the case, then they might be large, and we might end up maintaining quite a lot of information.\r\n\r\nLet's take a different example that might not be so easy.\r\nImagine you start with a collection `facts` of raw data, and you want to enrich it using dimesion tables that translate foreign keys like 'user id' into further detail.\r\nThe additional detail may result in further keys you want to unpack, like addresses, zipcodes, and the sales agents they map to.\r\n```rust\r\n// Enrich facts with user, address, and sales agent information.\r\nfacts.map(|fact| (fact.user_id, fact)).join(users).map( .. )\r\n     .map(|fact| (fact.addr_id, fact)).join(addrs).map( .. )\r\n     .map(|fact| (fact.zipcode, fact)).join(agent).map( .. )\r\n```\r\nLots and lots of data pipelines have this sort of enrichment in them, in part because 'normalized' database best practices are to factor apart this information.\r\nUnfortunately, stitching it back together efficiently is an important part of these best practices.\r\n\r\nFor this query, we may have arrangements of `users`, `addrs`, `agent`.\r\nHowever, we are unlikely to have arrangements of the left inputs to each of the `join`s.\r\nThe very first left input, `facts` keyed by `user_id`, is plausibly something we might have pre-arranged, but the other two result from the query itself.\r\nNaively implemented, we'll create second and third arrangements of enriched `fact` data, which can be really quite large.\r\n\r\nFortunately, there is a trick for multiway joins that I have no better name for than ['delta joins'](https://github.com/TimelyDataflow/differential-dataflow/tree/master/dogsdogsdogs).\r\nThe gist is that rather than plan a multiway join as a sequence (or tree) of binary joins, as done in System R, you describe how the whole join will vary as a function of each input.\r\nYou can get this derivation by expanding out our derivation for binary joins, in terms of input updates, to multiple inputs.\r\nYou then independently implement each of these response functions for each input as best as you can and then compose their results.\r\n\r\nFor example, our query above joins four relations: `facts`, `users`, `addrs`, and `agent`, subject to some equality constraints.\r\nWhen `facts` changes, we need to look up enrichments in `users`, then `addrs`, then `agent` to find the change to enriched facts.\r\nWhen `agent` changes, we need to find the affected `addrs`, then `users`, then `facts`, in order to update the enrichment of existing facts.\r\n\r\n```\r\n-- rules for how to react to an update to each input.\r\n-- elided: equality constraints for each join (\u22c8).\r\nd_query/d_facts = d_facts \u22c8 users \u22c8 addrs \u22c8 agent\r\nd_query/d_users = d_users \u22c8 addrs \u22c8 agent \u22c8 facts\r\nd_query/d_addrs = d_addrs \u22c8 agent \u22c8 users \u22c8 facts\r\nd_query/d_agent = d_agent \u22c8 addrs \u22c8 users \u22c8 facts\r\n```\r\nThe overall changes to `query` result from adding together these update rules.\r\n\r\nWhat's different above is that each of the `d_term \u22c8` joins are *ephemeral*: no one needs to remember the `d_` part of the input.\r\nEach of these rules are implementable with what differential dataflow calls a `half_join`: an operator that responds to records in one input by look-ups into a second, and which does not respond to changes to the second input.\r\nThe `half_join` operator needs an arrangement of its second input, but not of its first input.\r\n\r\nThis pattern has different arrangement requirements than the sequence of binary `join` operators.\r\nEach collection needs an arrangement by those attributes by which it may be interrogated.\r\nIn the example above, the required arrangements end up being:\r\n\r\n1. input `facts` arranged by `user_id`,\r\n2. input `users` arranged by `user_id` and also by `addr_id`,\r\n3. input `addrs` arranged by `addr_id` and also by `zipcode`,\r\n4. input `agent` arranged by `zipcode`.\r\n\r\nThis ends up being six arrangements, just like before, but they are all arrangements we might reasonably have ahead of time.\r\nThe *incremental* arrangement cost of the query can be zero, if these arrangement are all pre-built.\r\n\r\n### Boss Battle: Left Outer Joins\r\n\r\nAn 'outer' join is a SQL construct that is much like an standard ('inner') join except that any records that 'miss', i.e. do not match any other records, are still produced as output but with `NULL` values in columns we hoped to populate.\r\nOuter joins are helpful in best-effort joins, where you hope to enrich some data, but can't be certain you'll find the enrichment and don't want to lose the input data if you cannot.\r\n\r\nFor example, consider our `facts`, `users`, `addrs`, and `agent` scenario just above.\r\nWhat would happen if there is a `user_id` that does not exist in `users`, or a `addr_id` that does not exist in `addrs`, or a `zipcode` that does not exist in `agent`?\r\nWritten as a conventional join, we would simply drop such records on the floor and never speak of them.\r\nSometimes that is the right thing to do, but often you want to see the data along with any *failures* to find the enrichments.\r\n\r\nIf we take our example from above but use `LEFT JOIN` instead of `join`, we will keep even facts that do not match `users`, `addrs`, or `agent`.\r\n```sql\r\n-- Enrich facts with more data, but don't lose any.\r\nfacts LEFT JOIN users ON (facts.user_id = users.id)\r\n      LEFT JOIN addrs ON (users.addr_id = addrs.id)\r\n      LEFT JOIN agent ON (addrs.zipcode = agent.zc)\r\n```\r\n\r\nThere are also `RIGHT` and `FULL` joins, which respectively go in the other direction (e.g. output users that match no facts, with null fact columns) and in both directions (all bonus records that would be added to a `LEFT` or `RIGHT` join).\r\nWe are only going to noodle on `LEFT` joins, though the noodling should generalize just fine.\r\n\r\nTo implement left joins, we'll need to find a way to express them in terms of the tools we have.\r\nThose tools are .. the operators differential dataflow provides; things like `map`, `filter`, `join`, and `reduce` (no `iterate`. NO!).\r\n\r\n### Step one: turn LEFT JOINs into JOINs\r\n\r\nWhen we left join two collections, some records match perfectly as in an inner join, and some do not.\r\nWhat do we have to add to the results of the inner join to get the correct answer?\r\nSpecifically, any keys that might be present in the first input, but are not present in the second input, could just be added to the second input with `NULL` values.\r\n\r\n```sql\r\n-- Some facts exactly match some entry in users.\r\nSELECT * FROM facts INNER JOIN users ON (facts.user_id = users.id)\r\n-- Some facts totally miss, but need to match something.\r\nUNION ALL\r\nSELECT facts.*, NULL FROM facts\r\nWHERE facts.user_id NOT IN (SELECT id FROM users)\r\n```\r\nThis construction keeps the `INNER JOIN` pristine, but adds in `facts` extended by `NULL`s for any fact whose `user_id` is not found in `users`.\r\nAlthough not totally clear, `NOT IN` results in a join between `facts` and distinct `users.id`.\r\nThis approach feels good, re-uses arrangements on `facts` and `users`, and is pretty close to what Materialize does for you at the moment.\r\n\r\nHowever, this technique is not great for multiway outer joins.\r\nWe need access to the left input (here: `facts`) to complete the outer join, and generally that input is the result of the outer join just before this one.\r\nIf we need to have that answer to form this query fragment, we don't have a story for how they all become one multiway inner join.\r\nLikewise, Materialize currently plans a multiway outer join as a *sequence* of fragments like above that *involve* inner joins, but are not *an* inner join.\r\n\r\n### Step two: Multiway LEFT JOINS into Multiway JOINs\r\n\r\nLet's take the intuition above and see if we can preserve the join structure.\r\nWe want to produce a SQL fragment that is at its root just an inner join.\r\nWe will need to be careful that it should rely on base tables, not its direct inputs (what?).\r\n\r\nLet's start and we'll see where we get.\r\n\r\nFirst, let's rewrite the above fragment in a way that looks more like *one* inner join.\r\nOne one side we have `facts`, and on the other side .. at least `users` but also some other stuff?\r\nFor a first cut, that 'other stuff' is .. the `user_id`s in `facts` but not in `users`?\r\nWe could add those rows to `users`, with `NULL` values in missing columns, and see what we get!\r\n\r\nAs it turns out we get totally the wrong answer. \r\nBest intentions, of course, but the wrong answer.\r\nI believe the right answer is expressed roughly this way, in SQL:\r\n\r\n```sql\r\n-- Some facts exactly match some entry in users.\r\nSELECT facts.*, users.* \r\nFROM facts INNER JOIN users ON (facts.user_id = users.id)\r\n-- Some facts totally miss, but could match something.\r\nUNION ALL\r\nWITH absent(id) AS (\r\n    SELECT user_id FROM facts \r\n    EXCEPT \r\n    SELECT id FROM users\r\n)\r\nSELECT facts.*, NULL \r\nFROM facts INNER JOIN absent ON (facts.user_id = absent.id)\r\n-- Some facts have NULL `user_id` and refuse to be joined.\r\nUNION ALL\r\nSELECT facts.*, NULL\r\nFROM facts WHERE facts.user_id IS NULL\r\n```\r\n\r\nWe do grab the `absent` keys, but importantly we produce `NULL` in their key columns.\r\nWe also need to deal with potentially null `user_id` values, which we do in the third clause, because SQL's `NULL` values do not equal themselves.\r\nAgain, best intentions, I'm sure.\r\n\r\nThe good news is that we have framed the SQL in a way that looks like (taking some notational liberties):\r\n```\r\n  facts \u22c8 users\r\n+ facts \u22c8 absent    -- with null outputs\r\n+ facts \u22c8 NULLs     -- only for null user_id\r\n```\r\nEach of the three joins are slightly different, but they all have the property that `facts` arranged by `user_id` is enough for them.\r\nWe can and will now factor out `facts` from these three terms, which puts us in a position to write our multiway left join as:\r\n\r\n```\r\n-- Left join of facts, users, addrs, and agent.\r\nfacts \u22c8 (users + absent(users) + NULL)\r\n      \u22c8 (addrs + absent(addrs) + NULL)\r\n      \u22c8 (agent + absent(agent) + NULL)\r\n```\r\nThis is starting to look a bit more like the joins over sums of terms advertised in the beginning of the post.\r\nFor the moment, we are just going to add together the terms, though.\r\n\r\nThere is quite a lot unsaid here, and the nature of the \u22c8 varies a bit for each of the terms in parentheses.\r\nYou do have to populate the `absent(foo)` collections with values from base relations, rather than their immediate inputs.\r\nAnd fortunately, SQL notwithstanding, differential dataflow *does* equate NULL with itself, and everything works out just fine.\r\nMaterialize [recently merged](https://github.com/MaterializeInc/materialize/pull/24345) an approach that looks like this for multiway outer joins.\r\nIt's early days, but we'll soon start exploring how this work for folks with stacks of left joins.\r\n\r\nBut the story doesn't end here. \r\nSomewhat stressfully, this approach takes existing inputs `facts`, `users`, `addrs`, and `agent` and .. fails to use any of their pre-existing arrangements.\r\nIt has some other performance issues as well.\r\n\r\n### Step three: Rendering JOINs of UNIONs\r\n\r\nThe last step, or next step at least .. perhaps not the last, is to render these query plans efficiently.\r\nAt the moment we have no better plan than to treat the augmented collections as new collections, arrange them, and join them.\r\nRoughly like so:\r\n```\r\n-- Left join of facts, users, addrs, and agent.\r\nwith users_aug as (users + absent(users) + NULL)\r\nwith addrs_aug as (addrs + absent(users) + NULL)\r\nwith agent_aug as (agent + absent(agent) + NULL)\r\nfacts \u22c8 users_aug \u22c8 addrs_aug \u22c8 agent_aug\r\n```\r\n\r\nThese `_aug` collections are as big (somewhat bigger) than their unaugmented counterparts, and it feels somewhat bad to re-arrange them.\r\nIt feels bad that despite pre-arranging `users`, `addrs`, and `agent` we can re-use none of them.\r\nIt feels bad that all `NULL` values will be routed to a single worker just to find out that they map to `NULL`; lots of work for no surprise.\r\n\r\nHowever, we can get around all of these bad feels with some dataflow shenanigans.\r\nUnfortunately, they are shenanigans that as far as I can tell neither Materialize nor SQL can describe.\r\n\r\nWe have two strategies for evaluating multiway joins: as a sequence of binary joins, and using delta join rules.\r\nThe shenanigans are easier with the sequence of binary joins, so let's start there.\r\nWe are going to do something as simple as re-distributing over the `\u22c8` operator, performing each join the way we want.\r\nWe then add up the results of each step of the join rather than adding up the inputs to each step of the join.\r\n\r\n```\r\n-- Left join of facts, users, addrs, and agent.\r\nstep0 = facts;\r\nstep1 = step0 \u22c8 users + step0 \u22c8 absent(users) + step0 \u22c8 NULL;\r\nstep2 = step1 \u22c8 addrs + step1 \u22c8 absent(addrs) + step1 \u22c8 NULL;\r\nstep3 = step2 \u22c8 agent + step2 \u22c8 absent(agent) + step2 \u22c8 NULL;\r\nstep3\r\n```\r\nEach of these \u22c8 operators are slightly different. \r\nThe first \u22c8 in each row is the traditional equijoin.\r\nThe second \u22c8 in each row is an equijoin that projects away matched keys and puts `NULL` in their place.\r\nThe third \u22c8 in each row only matches nulls.\r\n\r\nHowever, in each line we only have to arrange non-null `stepx` and determine and arrange `absent(foo)`. \r\nWe can re-use existing arrangements of `users`, `addrs`, and `agent`.\r\nThe join with `NULL` can be implemented as a `flat_map` rather than by co-locating all null records for a `join` (omg finally explained).\r\n\r\nIn actual fact, we can implement this in both SQL and Materialize, but in doing so we'll lose the multiway join planning benefit of avoiding intermediate arrangements.\r\nWe will need to arrange `stepx` for each `x`, and the nice folks with stack of left joins 30+ deep (yes, seriously) will be sitting on 30x as much data as they feel they should.\r\n\r\nTo recover the benefits, let's grab the delta join construction from way up above. \r\nI'll use `_aug` suffixes to remind us that it isn't going to be as easy as joining against the pre-arranged collections.\r\n```\r\n-- rules for how to react to an update to each input.\r\n-- elided: equality constraints for each join (\u22c8).\r\nd_query/d_facts     = d_facts     \u22c8 users_aug \u22c8 addrs_aug \u22c8 agent_aug\r\nd_query/d_users_aug = d_users_aug \u22c8 addrs_aug \u22c8 agent_aug \u22c8 facts\r\nd_query/d_addrs_aug = d_addrs_aug \u22c8 agent_aug \u22c8 users_aug \u22c8 facts\r\nd_query/d_agent_aug = d_agent_aug \u22c8 addrs_aug \u22c8 users_aug \u22c8 facts\r\n```\r\nIgnore for the moment the fact that `d_users_aug` is complicated (an update to `users` may induce the opposite update to `absent(users)`).\r\nEach line up above describes a sequence of `half_join` applications, which like `join` also distributes over `+`.\r\n\r\n```\r\n  d_query/d_facts    \r\n= d_facts \u22c8 users_aug \u22c8 addrs_aug \u22c8 agent_aug\r\n= ( \r\n    d_step0 = d_facts;\r\n    d_step1 = d_step0 \u22c8 users + d_step0 \u22c8 absent(users) + d_step0 \u22c8 NULL;\r\n    d_step2 = d_step1 \u22c8 addrs + d_step1 \u22c8 absent(addrs) + d_step1 \u22c8 NULL;\r\n    d_step3 = d_step2 \u22c8 agent + d_step2 \u22c8 absent(agent) + d_step2 \u22c8 NULL;\r\n    d_step3\r\n)\r\n```\r\nEach time we need to do a `half_join`, we can unpack the right argument to it and conduct the half join as we see fit.\r\nWe can either `half_join` with a pre-existing arrangement, `half_join` with a new arrangement of absent values, or `flat_map` some `NULL` values into place.\r\n\r\nWriting the whole thing out is exhausting, especially for 30-deep stacks of left joins.\r\nFortunately this is something computers are good at.\r\nMuch like it now seems that they may be good at computing and maintaining deep stacks of left equijoins.\r\n\r\n### What's next?\r\n\r\nThere is an expressivity gap to close between SQL/Materialize and differential dataflow.\r\nI'm not aware of a SQL-to-SQL rewrite that gets us the desired implementation, because we cannot afford to distribute the joins out across the unions, and SQL does not have a `half_join` operator.\r\nWe're pondering options now, including expanding our lowest level IR to reflect e.g. half joins, and tweaking the renderer to recognize the idiom of joins between sums of terms.\r\nThere will certainly be some amount of measurement as we try and assess the remaining gap, and draw down the amount of time and resources spent on outer joins.\r\n\r\nI have a concurrent effort to spread the gospel of [referential integrity](https://en.wikipedia.org/wiki/Referential_integrity) so that we can turn those outer joins to inner joins.\r\nYou can understand how in weakly consistent systems you'd need the outer joins to cover for inconsistencies, but do you need it in a strongly consistent system like Materialize?\r\n\r\nOf course, if you've read this far you have an obligation to fill me in on what I've missed about all of this.\r\nIs there an easier transform, one that doesn't end up joining terms that are themselves sums of useful constituents?\r\nDo you have an exciting use case for maintaining stacks of outer joins, and you've been burned before?\r\nDo reach out in these cases, and [take Materialize for a spin](https://materialize.com/register/) (though, if you have stacks of 30+ left joins, please reach out for some personal attention).\r\n\r\n<!-- ##{'timestamp':1710738000}## -->\u3002", "top": 0, "createdAt": 1710738000, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-03-18", "dateLabelColor": "#bc4c00"}, "P2": {"htmlDir": "docs/post/Demonstrating Operational Data with SQL.html", "labels": ["documentation"], "postTitle": "Demonstrating Operational Data with SQL", "postUrl": "post/Demonstrating%20Operational%20Data%20with%20SQL.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/2", "commentNum": 0, "wordCount": 39220, "description": "Databases, Big Data, and Stream Processors have long had the property that it can be hard to *demonstrate* their value, like in a demo setting.\r\nDatabases coordinate the work of multiple teams of independent workers, and don't shine when there is just one user.\r\nBig Data systems introduce scalable patterns that can be purely overhead when the data fit on a single laptop.\r\nStream Processors aim to get the lowest of end-to-end latencies, but do nothing of any consequence on static data.\r\nThese systems demonstrate value when you have variety, volume, and velocity, and most demo data sets have none of these.\r\n\r\nMaterialize, an operational data warehouse backed by scalable streaming systems, has all three of these challenges!\r\n\r\nFortunately, Materialize is powerful enough to synthesize its own operational data for demonstration purposes.\r\nIn this post, we'll build a recipe for a generic live data source using standard SQL primitives and some Materialize magic.\r\nWe'll then add various additional flavors: distributions over keys, irregular validity, foreign key relationships.\r\nIt's all based off of Materialize's own [auction load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction), but it's written entirely in SQL and something that I can customize as my needs evolve.\r\n\r\nThe thing I find most amazing here is that with just SQL you can create *live* data. \r\nData that comes and goes, changes, and respects invariants as it does.\r\nAnd that the gap between your idea for live data and making it happen is just typing some SQL.\r\n\r\n### My Motivation: Materialize\r\n\r\nMaterialize has a few product beats it wants to hit when we demo it, derived from our product principles.\r\n\r\n* **Responsiveness**: Materialize should be able to get back to you ASAP, even with lots of data involved.\r\n* **Freshness**: Materialize should reflect arbitrary updates almost immediately, even through complex logic.\r\n* **Consistency**: Materialize's outputs should always reflect a consistent state, even across multiple users and views.\r\n\r\nWe want to get folks to that 'aha!' moment where they realize that Materialize is like no other technology they know of.\r\nUntil that moment, Materialize could just be a trenchcoat containing Postgres, Spark, and Flink stacked according to your preferences.\r\n\r\nOf course, different contexts connect for different users.\r\nSome folks think about transactions and fraud and want to see how to get in front of that.\r\nOthers have users of their own, and know that sluggish, stale, inconsistent results are how they lose their users, and want to feel the lived experience.\r\nMany users won't believe a thing until the data looks like their data, with the same schemas and data distributions, and the same business logic.\r\nThese are all legitimate concerns, and to me they speak to the inherent *heterogeneity* involved in demonstrating something.\r\n\r\nI want to be able to demonstrate Materialize more **effectively**, which is some amount tied up in demonstrating it more **flexibly**.\r\n\r\nAs a personal first, I'm going to try telling the story in reverse order, Memento-style.\r\nWe'll start with the outcomes, which I hope will make sense, and then figure out how we got there, and eventually arrive at the wall of SQL that makes it happen.\r\nIt does mean we'll need some suspension of disbelief as we go, though; bear with me!\r\nI do hope that whichever prefix you can tolerate makes sense and is engaging, and am only certain that if we started with the SQL it would not be.\r\n\r\nThe outine is, roughly:\r\n\r\n1.  [Demonstrating Materialize with auction data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#demonstrating-materialize)\r\n\r\n    We'll work through Materialize's quick start to show off `auctions` and `bids` data, and give a feel for what we need to have our live data do.\r\n    We're going to hit the beats of responsiveness, freshness, and consistency along the way.\r\n\r\n2.  [Building an Auction loadgen from unrelated live data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#auction-data-from-changing-moments)\r\n\r\n    Here we'll build live views that define `auctions` and `bids`, starting from a live view that just contains recent timestamps.\r\n    We'll see how to turn largely nonsense data into plausible auctions and bids, through the magic of pseudorandomness.\r\n\r\n3.  [Building live random data from just SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#operational-data-from-thin-air)\r\n\r\n    Starting from nothing more than SQL, we'll create a live view that Materialize can maintain containing recent moments as timestamps.\r\n    As time continually moves forward, those moments continually change.\r\n\r\n4.  [All the SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#appendix-all-the-sql) Really, just SQL.\r\n\r\nFeel more than welcome to leap to the sections that interest you most.\r\nI recommend starting at the beginning, though!\r\n\r\n### Demonstrating Materialize\r\n\r\nLet's sit down with Materialize and some live auction data and see if we can't hit the beats of responsiveness, freshness, and consistency.\r\nThe story is borrowed from our own quickstart, but by the end of it we'll find we've swapped out the quickstart's built-in load generator.\r\n\r\nMaterialize's [`AUCTION` load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction) populates `auctions` and `bids` tables.\r\nTheir contents look roughly like so:\r\n```\r\nmaterialize=> select * from auctions;\r\n id | seller |        item        |          end_time          \r\n----+--------+--------------------+----------------------------\r\n  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00\r\n  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00\r\n  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00\r\n  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00\r\n  ...\r\n(4 rows)\r\n```\r\n```\r\nmaterialize=> select * from bids;\r\n id | buyer | auction_id | amount |          bid_time          \r\n----+-------+------------+--------+----------------------------\r\n 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00\r\n 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00\r\n 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00\r\n 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00\r\n ...\r\n```\r\n\r\nWe will root around in this data, as it changes, and show off Materialize as something unlike other data tools.\r\nSpecifically we'll want to show off responsiveness, freshness, and consistency, which we'll do in that order.\r\nHowever, the point is that you get them all at the same time, rather than one at a time, and by the end we should be able to see all three at once.\r\n\r\n#### Beat 1: Responsiveness\r\n\r\nMaterialize is able to respond immediately, even to complex queries over large volumes of data.\r\nLet's start by looking at the data, counting the number of auctions and the number of bids.\r\n```\r\nmaterialize=> select count(*) from auctions;\r\n count \r\n-------\r\n 86400\r\n(1 row)\r\n\r\nTime: 52.580 ms\r\n```\r\n```\r\nmaterialize=> select count(*) from bids;\r\n  count   \r\n----------\r\n 10994252\r\n(1 row)\r\n\r\nTime: 8139.897 ms (00:08.140)\r\n```\r\nIt's almost 100k auctions, and over 10M bids across them.\r\nThe specific numbers will make more sense when we get to the generator, but some of you may already recognize 86,400.\r\nTen seconds to count ten million things is not great, but this is running on our smallest instance (`25cc`; roughly 1/4 of a core).\r\nAlso, we aren't yet using Materialize's super-power to *maintain* results.\r\n\r\nMaterialize maintains computed results in indexes, created via the `CREATE INDEX` command.\r\n```sql\r\n-- Maintain bids indexed by id.\r\nCREATE INDEX bids_id ON bids (id);\r\n```\r\n\r\nWhen we want to find a specific bid by id, this can be very fast .\r\n```\r\nmaterialize=> select * from bids where id = 4;\r\n id | buyer | auction_id | amount |        bid_time        \r\n----+-------+------------+--------+------------------------\r\n  4 |   228 |    6492730 |    149 | 2024-06-19 13:57:50+00\r\n(1 row)\r\n\r\nTime: 19.711 ms\r\n```\r\nInspecting the query history (a feature in Materialize's console) we can see it only took 5ms for the DB, and the additional latency is between NYC and AWS's us-east-1.\r\nThis really is just a look-up into a maintained index, admittedly only on `bids` rather than some sophisticated query.\r\n\r\nYou can build indexes on any collection of data, not just raw data like `bids`.\r\nWe could build an index on `SELECT COUNT(*) FROM bids` to make that fast too, for example.\r\nInstead, let's go straight to the good stuff.\r\n\r\nHere's a view that determines which auctions are won by which bids.\r\n```sql\r\n-- Determine auction winners: the greatest bid before expiration.\r\nCREATE VIEW winning_bids AS\r\n  SELECT DISTINCT ON (auctions.id) bids.*,\r\n    auctions.item,\r\n    auctions.seller\r\n  FROM auctions, bids\r\n  WHERE auctions.id = bids.auction_id\r\n    AND bids.bid_time < auctions.end_time\r\n    AND mz_now() >= auctions.end_time\r\n  ORDER BY auctions.id,\r\n    bids.amount DESC,\r\n    bids.bid_time,\r\n    bids.buyer;\r\n```\r\n\r\nDirectly querying this view results in a not-especially-responsive experience\r\n```\r\nmaterialize=> select auction_id, buyer, amount from winning_bids limit 5;\r\n auction_id | buyer | amount \r\n------------+-------+--------\r\n        217 |    41 |    252\r\n       3328 |   209 |     55\r\n      19201 |   147 |    255\r\n      18947 |    34 |    254\r\n       7173 |   143 |      5\r\n(5 rows)\r\n\r\nTime: 87428.589 ms (01:27.429)\r\n```\r\nWe are grinding through all the bids from scratch when you select from a view, because the view only explains what query you want to run.\r\nA view by itself doesn't cause any work to be done ahead of time.\r\n\r\nHowever, we can create indexes on `winning_bids`, and once they are up and running everything gets better.\r\nWe are going to create two indexes, on the columns `buyer` and `seller`, for future storytelling reasons.\r\n```sql\r\n-- Compute and maintain winning bids, indexed two ways.\r\nCREATE INDEX wins_by_buyer ON winning_bids (buyer);\r\nCREATE INDEX wins_by_seller ON winning_bids (seller);\r\n```\r\nThe auctions aren't faster to magic in to existence than the original query was, so we'll have to wait a moment for them to hydrate.\r\nOnce this has happened, you get responsive interactions with the view.\r\n```\r\nmaterialize=> select auction_id, buyer, amount from winning_bids limit 5;\r\n auction_id | buyer | amount \r\n------------+-------+--------\r\n    7647534 |     0 |    254\r\n    6568079 |     0 |    239\r\n   10578840 |     0 |    254\r\n   14208479 |     0 |    249\r\n   15263465 |     0 |    199\r\n(5 rows)\r\n\r\nTime: 61.283 ms\r\n```\r\nRather than grind over the ten million or so bids to find winners, the ~80,000 results are maintained and its easy to read the first five.\r\nMoreover, the results are all immediately up to date, rather than being fast-but-stale.\r\nLet's hit that **freshness** beat now!\r\n\r\n<!-- \r\nIn addition, our indexes set us up for responsive ad-hoc queries.\r\nHere's an example where we look for 'auction flippers': folks who are both buyers and sellers of the same item at increased amounts:\r\n```sql\r\n-- Look for users who re-sell their winnings\r\nCREATE VIEW potential_flips AS\r\n  SELECT w2.seller,\r\n         w2.item AS item,\r\n         w2.amount AS seller_amount,\r\n         w1.amount AS buyer_amount\r\n  FROM winning_bids w1,\r\n       winning_bids w2\r\n  WHERE w1.buyer = w2.seller\r\n    AND w2.amount > w1.amount\r\n    AND w1.item = w2.item;\r\n```\r\n\r\nWe have enough auctions that some folks will be both buyers and sellers, and for some fraction of them its the same item for an increased price.\r\n```\r\nmaterialize=> select count(*) from potential_flips;\r\n count \r\n-------\r\n  9755\r\n(1 row)\r\n\r\nTime: 602.481 ms\r\n```\r\n```\r\nmaterialize=> select seller, count(*) from potential_flips group by seller order by count(*) desc limit 5;\r\n seller | count \r\n--------+-------\r\n  42091 |     7\r\n  42518 |     6\r\n  10529 |     6\r\n  39840 |     6\r\n  49317 |     6\r\n(5 rows)\r\n\r\nTime: 678.330 ms\r\n```\r\n\r\nThis is now pretty interactive, using scant resources, over enough data and through complex views that to start from scratch would be exhausting.\r\nHowever, maintained indexes keep intermediate results up to date, and you get the same results as if re-run from scratch, just without the latency. -->\r\n\r\n#### Beat 2: Freshness\r\n\r\nAll of this auction data is synthetic, and while it changes often the show is pretty clearly on rails.\r\nThat is, Materialize knows ahead of time what the changes will be.\r\nYou want to know that Materialize can respond fast to *arbitrary* changes, including ones that Materialize doesn't anticipate.\r\n\r\nWe need **interaction**!\r\n\r\nLet's create a table we can modify, through our own whims and fancies.\r\nOur modifications to this table, not part of the load generator, will be how we demonstrate the speed at which Materialize updates results as data change.\r\n```sql\r\n-- Accounts that we might flag for fraud.\r\nCREATE TABLE fraud_accounts (id bigint);\r\n```\r\n\r\nLet's look at a query that calls out the top five accounts that win auctions.\r\nWe'll subscribe to it, meaning we get to watch the updates as they happen.\r\n```sql\r\n-- Top five non-fraud accounts, by auction wins.\r\nCOPY (SUBSCRIBE TO (\r\n  SELECT buyer, count(*)\r\n  FROM winning_bids\r\n  WHERE buyer NOT IN (SELECT id FROM fraud_accounts)\r\n  GROUP BY buyer\r\n  ORDER BY count(*) DESC, buyer LIMIT 5\r\n)) TO STDOUT;\r\n```\r\nThis produces first a snapshot and then a continual stream of updates.\r\nIn our case, the updates are going to derive from our manipulation of `fraud_accounts`.\r\n```\r\n1718981380562\t1\t7247\t7\r\n1718981380562\t1\t17519\t7\r\n1718981380562\t1\t27558\t7\r\n1718981380562\t1\t20403\t7\r\n1718981380562\t1\t16584\t7\r\n```\r\nThe data are not really changing much, on account of the winners all having the same counts.\r\nBut, this is actually good for us, because we can see what happens when we force a change.\r\n\r\nAt this point, let's insert the record `17519` into `fraud_accounts`.\r\n```\r\n-- Mark 17519 as fraudulent\r\n1718981387841\t-1\t17519\t7\r\n1718981387841\t1\t32134\t7\r\n```\r\nWe can do the same with `16584`, and then `34985`.\r\n```\r\n-- Mark 16584 as fraudulent\r\n1718981392977\t1\t34985\t7\r\n1718981392977\t-1\t16584\t7\r\n-- Mark 34985 as fraudulent\r\n1718981398158\t1\t35131\t7\r\n1718981398158\t-1\t34985\t7\r\n```\r\nFinally, let's remove all records from `fraud_accounts` and we can see that we return back to the original state.\r\n```\r\n-- Remove all fraud indicators.\r\n1718981403087\t-1\t35131\t7\r\n1718981403087\t1\t17519\t7\r\n1718981403087\t-1\t32134\t7\r\n1718981403087\t1\t16584\t7\r\n...\r\n```\r\nThat `34985` record isn't mention here because it only showed up due to our other removals.\r\nWe don't hear about a change because there is no moment when it is in the top five, even transiently.\r\nThat is a great lead-in to Materailize's **consistency** properties!\r\n\r\n#### Beat 3: Consistency\r\n\r\nAll the freshness and responsiveness in the world doesn't mean much if the results are incoherent.\r\nMaterialize only ever presents actual results that actually happened, with no transient errors.\r\nWhen you see results, you can confidently act on them knowing that they are real, and don't need further second to bake.\r\n\r\nLet's take a look at consistency through the lens of account balances as auctions close and winning buyers must pay sellers.\r\n```sql\r\n-- Account ids, with credits and debits from auctions sold and won.\r\nCREATE VIEW funds_movement AS\r\n  SELECT id,\r\n         SUM(credits) AS credits,\r\n         SUM(debits) AS debits\r\n  FROM (\r\n    SELECT seller AS id, amount AS credits, 0 AS debits\r\n    FROM winning_bids\r\n    UNION ALL\r\n    SELECT buyer AS id, 0 AS credits, amount AS debits\r\n    FROM winning_bids\r\n  )\r\n  GROUP BY id;\r\n```\r\n\r\nThese balances derive from the same source: `winning_bids`, and although they'll vary from account to account, they should all add up.\r\nSpecifically, if we get the total credits and the total debits, they should 100% of the time be exactly equal.\r\n```sql\r\n-- Discrepancy between credits and debits.\r\nSELECT SUM(credits) - SUM(debits) \r\nFROM funds_movement;\r\n```\r\nThis query reports zero, 100% of the time.\r\nWe can `SUBSCRIBE` to the query to be notified of any change.\r\n```\r\nmaterialize=> COPY (SUBSCRIBE (\r\n    SELECT SUM(credits) - SUM(debits) \r\n    FROM funds_movement\r\n)) TO STDOUT;\r\n\r\n1716312983129\t1\t0\r\n```\r\nThis tells us that starting at time `1716312983129`, there was `1` record, and it was `0`.\r\nYou can sit there a while, and there will be no changes.\r\nYou could also add the `WITH (PROGRESS)` option, and it will provide regular heartbeats confirming that second-by-second it is still zero.\r\nThe credits and debits always add up, and aren't for a moment inconsistent.\r\n\r\nWe can set up similar views for other assertions.\r\nFor example, every account that has sold or won an auction should have a balance.\r\nA SQL query can look for violations of this, and we can monitor it to see that it is always empty.\r\nIf it is ever non-empty, perhaps there are bugs in the query logic, its contents are immediately actionable: \r\nthere is a specific time where the inputs evaluated to an invariant-violating output, and if you return to that moment you'll see the inputs that produce the bad output.\r\n\r\nThe consistency extends across multiple independent sessions.\r\nThe moment you get confirmation that the insert into `fraud_accounts`, you can be certain that no one will see that account in the top five non-fraudulent auction winners.\r\nThis guarantee is called 'strict serializability', that the system behaves as if every event occurred at a specific time between its start and end, and is the strongest guarantee that databases provide.\r\n\r\n#### Demo over!\r\n\r\nThat's it!\r\nWe've completed the introduction to Materialize, and used auction data to show off responsiveness, freshness, and consistency.\r\nThere's a lot more to show off, of course, and if any of this sounded fascinating you should swing by https://materialize.com/register/ to spin up a trial environment.\r\n\r\nHowever, in this post we will continue to unpack how we got all of that `auctions` and `bids` data in the first place!\r\n\r\n### Auction Data from Changing Moments\r\n\r\nWhere do the `auctions` and `bids` data come from?\r\nYou can get them from our load generator, but we're going to try and coax them out of raw SQL.\r\nWe're going to start with something we haven't introduced yet, but it's a view whose content looks like this\r\n```sql\r\n-- All seconds within the past 24 hours.\r\nCREATE VIEW moments AS\r\nSELECT generate_series(\r\n    now() - '1 day'::interval + '1 second'::interval,\r\n    now(),\r\n    '1 second'\r\n) moment;\r\n``` \r\n\r\nUnpacking this, `moments` contains rows with a single column containing a timestamp.\r\nWhenever we look at it, the view contains those timestamps at most one day less than `now()`.\r\nIt should have at any moment exactly 86,400 records present, as many as `auctions` up above.\r\n\r\nImportantly, this view definition will not actually work for us.\r\nYou are welcome to try it out, but you'll find out that while it can be *inspected*, it cannot be *maintained*.\r\nWe'll fix that by the end of the post, but it will need to wait until the next section.\r\nFor the moment, let's assume we have this view and the magical ability to keep it up to date.\r\n\r\nThese 'moments' are not auction data, though.\r\nHow do we get from moments to auctions and bids?\r\n\r\nThe `auctions` and `bids` collections look roughly like so:\r\n```\r\nmaterialize=> select * from auctions;\r\n id | seller |        item        |          end_time          \r\n----+--------+--------------------+----------------------------\r\n  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00\r\n  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00\r\n  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00\r\n  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00\r\n  ...\r\n(4 rows)\r\n```\r\n```\r\nmaterialize=> select * from bids;\r\n id | buyer | auction_id | amount |          bid_time          \r\n----+-------+------------+--------+----------------------------\r\n 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00\r\n 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00\r\n 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00\r\n 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00\r\n ...\r\n```\r\n\r\nAuctions have a unique id, a seller id, an item description, and an end time.\r\nBids have a unique id (no relation), a buyer id, an auction id, the amount of the bid, and the time of the bid.\r\n\r\nThe `seller`, `item`, `buyer`, and `amount` fields are all random, within some bounds.\r\nAs a first cut, we'll think about just using random values for each of the columns.\r\nWhere might we get randomness, you ask?\r\nWell, if *pseudo*-randomness is good enough (it will be), we can use cryptographic hashes of the moments.\r\n```sql\r\n-- Extract pseudorandom bytes from each moment.\r\nCREATE VIEW random AS\r\nSELECT moment, digest(moment::text, 'md5') as random\r\nFROM moments;\r\n```\r\nLet's start with bytes from `random` to populate columns, and we'd have a first cut at random data.\r\nColumns like `auctions.item` are populated by joining with a constant collection (part of the generator), but `id` and `seller` could just be random.\r\nThe `end_time` we'll pick to be a random time up to 256 minutes after the auction starts.\r\n```sql\r\n-- Totally accurate auction generator.\r\nCREATE VIEW auctions_core AS\r\nSELECT \r\n    moment,\r\n    random,\r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 as seller,\r\n    get_byte(random, 5) as item,\r\n    -- Have each auction expire after up to 256 minutes.\r\n    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time\r\nFROM random;\r\n```\r\nWe've clearly made some calls about how random each of these should be, and those calls influence what we'll see in the data.\r\nFor example, we've established at most 65,536 sellers, which lines up fine with our 86,400 auctions at any moment; some sellers will have multiple auctions and many will not.\r\nAuctions are open for a few hours on average, close out but linger, and then vanish after 24 hours.\r\nIf we want to change any of these, perhaps to add more distinct items, or keep auctions running longer, or to skew the distribution over sellers, we can!\r\n\r\nSimilarly, the columns of `bids` are also pretty random, but columns like `auction_id` and `bid_time` do need to have some relationship to `auctions` and the referenced auction.\r\nWe'll build those out in just a moment, but have a bit more tidying to do for `auctions` first.\r\n\r\n#### Adding Custom Expiration\r\n\r\nOur auctions wind down after some random amount of time, but they are not removed from `auctions` for three hours.\r\nThematically we can think of this as auctions whose winners have been locked in, but whose accounts have not yet been settled.\r\n\r\nIf we want the auction to vanish from `auctions` at this time it closed, we could accomplish this with a temporal filter:\r\n```sql\r\nWHERE mz_now() < end_time\r\n```\r\nAs soon as we reach `end_time` the auction would vanish from `auctions`.\r\n\r\nThis is a very helpful pattern for load generators that want to control when data arrive and when it departs, in finer detail than 'a twenty four hour window'.\r\nFor example, one could randomly generate `insert_ts` and `delete_ts`, and then use\r\n```sql\r\n-- Create an event that is live for the interval `[insert_ts, delete_ts]`.\r\nWHERE mz_now() BETWEEN insert_ts AND delete_ts\r\n```\r\nThis pattern allows careful control of when events *appear* to occur, by holding them back until `mz_now()` reaches a value, and then retracting them when it reaches a later value. \r\n\r\n#### Making More Realistic Data\r\n\r\nOur random numbers for `item` aren't nearly as nice as what the existing load generator produces.\r\nHowever, we can get the same results by putting those nice values in a view and using our integer `item` to join against the view.\r\n```sql\r\n-- A static view giving names to items.\r\nCREATE VIEW items (id, item) AS VALUES\r\n    (0, 'Signed Memorabilia'),\r\n    (1, 'City Bar Crawl'),\r\n    (2, 'Best Pizza in Town'),\r\n    (3, 'Gift Basket'),\r\n    (4, 'Custom Art');\r\n```\r\n\r\nNow when we want to produce an actual auction record, we can join against items like so\r\n```sql\r\n-- View that mirrors the `auctions` table from our load generator.\r\nCREATE VIEW auctions AS\r\nSELECT id, seller, items.item, end_time\r\nFROM auctions_core, items\r\nWHERE auction.item = items.id;\r\n```\r\n\r\nWe've now got a view `auctions` that mirrors what Materialize's load generator produces, at least superficially.\r\n\r\n#### Introducing Foreign Key Constraints\r\n\r\nEach bid in `bids` references an auction, and we are unlikely to find an extant auction if we just use random numbers for `auction_id`.\r\nWe'd like to base our `bids` on the available auctions, and have them occur at times that make sense for the auction.\r\n\r\nWe can accomplish this by deriving the bids for an auction from `auctions` itself.\r\nWe will use some available pseudorandomness to propose a number of bids, and then create further pseudorandomness to determine the details of each bid.\r\n```sql\r\nCREATE VIEW bids AS\r\n-- Establish per-bid records and pseudorandomness.\r\nWITH prework AS (\r\n    -- Create `get_byte(random, 6)` many bids for each auction, \r\n    -- each with their own freshly generated pseudorandomness.\r\n    SELECT \r\n        id as auction_id,\r\n        moment as auction_start,\r\n        end_time as auction_end,\r\n        digest(random::text || generate_series(1, get_byte(random, 6))::text, 'md5') as random\r\n    FROM auctions_core\r\n)\r\nSELECT\r\n    get_byte(random, 0) +\r\n    get_byte(random, 1) * 256 +\r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) AS buyer,\r\n    auction_id,\r\n    get_byte(random, 4)::numeric AS amount,\r\n    auction_start + (get_byte(random, 5)::text || ' seconds')::interval as bid_time\r\nFROM prework;\r\n```\r\n\r\nWe now have a pile of bids for each auction, with the compelling property that when the auction goes away so too do its bids.\r\nThis gives us 'referential integrity', the property of foreign keys (`bids.auction_id`) that their referent (`auction.id`) is always valid.\r\n\r\nAnd with this, we have generated the `auctions` and `bids` data that continually change, but always make sense.\r\n\r\nThere are several other changes you might want to make!\r\nFor example, random bids means that auctions stop changing as they go on, because new random bids are unlikely to beat all prior bids.\r\nYou could instead have the bids trend up with time, to keep the data interesting.\r\nBut, the changes are pretty easy to roll out, and just amount to editing the SQL that defines them.\r\n\r\nLet's pause for now on noodling on ways we could make the data even more realistic.\r\nUp next we have to unpack how we got that `moments` view in the first place.\r\nOnce we've done that, you are welcome to go back to playing around with load generator novelties and variations!\r\n\r\n### Operational Data from Thin Air\r\n\r\nOur `auctions` and `bids` data was based on a view `moments` that showed us all timestamps within the past three hours.\r\nWe saw how we could go from that to pretty much anything, through extracted pseudorandomness.\r\n\r\nWe used a view that seemed maybe too easy, that looked roughly like so:\r\n```sql\r\n-- Generate a sliding window over timestamp data.\r\n-- Arguments: <volume>, <velocity>\r\nSELECT moment,\r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    <velocity>\r\n) moment\r\nWHERE now() BETWEEN moment AND moment + <volume>;\r\n```\r\n\r\nThis example uses `generate_series` to produce moments at which events will occur.\r\nThe `<velocity>` argument chooses the step size of the `generate_series` call, and locks in the cadence of updates.\r\nThe `<volume>` argument controls for how long each record lingers, and sets the steady state size.\r\nThe result is a sliding window over random data, where you get to control the volume and velocity.\r\n\r\nWe used `'1 second'` for the velocity and `'1 day'` for the volume.\r\n\r\nNow, while you can *type* the above, it won't actually run properly if you press enter.\r\nThe query describes 130 years of data, probably at something like a one second update frequency (because you wanted live data, right?).\r\nI don't even know how to determine how many records this is accurately based on all the leap-action that occurs.\r\nMoreover, you won't be able to materialize this view, because `now()` prevents materializations.\r\n\r\nTo actually get this to work, we'll have to use some clever tricks.\r\nThe coming subsections are a sequence of such tricks, and the punchline will be 'it works!', in case that saves you any time.\r\n\r\n#### Clever trick 1: using `mz_now()`\r\n\r\nOur first clever trick is to move from `now()` to `mz_now()`.\r\nThese are very similar functions, where the `now()` function gets you the contents of the system clock, and `mz_now()` gets you the transaction time of your command.\r\nThe main difference between the two is that we can materialize some queries containing `mz_now()`, unlike any query containing `now()`.\r\n\r\n```sql\r\n-- Generate a sliding window over timestamp data.\r\nSELECT moment,\r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 second'\r\n) moment\r\n--    /------\\---- LOOK HERE!\r\nWHERE mz_now() BETWEEN moment AND moment + '1 day';\r\n```\r\nThis very simple change means that Materialize now has the ability to keep the query up to date.\r\nMaterialize has a feature called ['temporal filters'](https://materialize.com/docs/transform-data/patterns/temporal-filters/) that allows `mz_now()` in `WHERE` clauses, because we are able to invert the clause and see the moment (Materialize time) at which changes will occur.\r\n\r\nUnfortunately, the implementation strategy for keeping this view up to date still involves first producing all the data, and then filtering it (we don't have any magical insight into `generate_series` that allows us to invert its implementation).\r\nBut fortunately, we have other clever tricks available to us.\r\n\r\n#### Clever trick 2: Hierachical Generation\r\n\r\nThe problem above is that we generate all the data at once, and then filter it.\r\nWe could instead generate the years of interest, from them the days of interest, from them the hours of interest, then minutes of interest, then seconds of interest, and finally milliseconds of interest.\r\nIn a sense we are generating *intervals* rather than *moments*, and then producing moments from the intervals.\r\n\r\nLet's start by generating all the years we might be interested in.\r\nWe start with all the years we might reasonably need, and a `WHERE` clause that checks for intersection of the interval (`+ '1 year'`) and the extension by volume (`+ '1 day'`).\r\n```sql\r\n-- Each year-long interval of interest\r\nCREATE VIEW years AS\r\nSELECT * \r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 year') year\r\nWHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';\r\n```\r\nThis view does not have all that many years in it. \r\nRoughly 130 of them.\r\nFew enough that we can filter them down, and get to work on days.\r\n\r\nAt this point, we'll repeatedly refine the intervals by subdividing into the next granularity.\r\nWe'll do this for years into days, but you'll have to use your imagination for the others.\r\nWe have all the SQL at the end, so don't worry that you'll miss out on that.\r\n```sql\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(\r\n        year, \r\n        year + '1 year' - '1 day'::interval, \r\n        '1 day') as day\r\n    FROM years\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day';\r\n```\r\nWe'll repeat this on to a view `seconds`, and stop there.\r\n\r\nAlthough we could continue to milliseconds, experience has been that it's hard to demo things changing that quickly through SQL.\r\nLines of text flow past like the Matrix, and all you can really see is that there is change, not what the change is.\r\n\r\nUnfortunately, there is a final gotcha.\r\nMaterialize is too clever by half, and if you materialize the `seconds` view, it will see that it is able to determine the entire 130 year timeline of the view, history and future, and record it for you.\r\nAt great expense.\r\nThese declarative systems are sometimes just too smart.\r\n\r\n#### Clever trick 3: An empty table\r\n\r\nWe can fix everything by introducing an empty table.\r\n\r\nThe empty table is only present to ruin Materialize's ability to be certain it already knows the right answer about the future.\r\nWe'll introduce it to each of our views in the same place, and its only function is to menace Materialize with the possibility that it *could* contain data.\r\nBut it won't.\r\nBut we wont tell Materialize that.\r\n\r\n```sql\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(\r\n        year, \r\n        year + '1 year' - '1 day'::interval, \r\n        '1 day') as day\r\n    FROM years\r\n    -- THIS NEXT LINE IS NEW!!\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day';\r\n```\r\n\r\nWith these tricks in hand, we now have the ability to spin it up and see what it looks like.\r\n\r\n```sql\r\nCREATE DEFAULT INDEX ON days;\r\n```\r\n\r\nWe'll want to create the same default indexes on our other views: `hours`, `minutes`, and `seconds`.\r\nImportantly, we want to create them in this order, also, to make sure that each relies on the one before it.\r\nIf they did not, we would be back in the world of the previous section, where each would read ahead until the end of time (the year 2099, in this example).\r\n\r\n#### Finishing touches\r\n\r\nAs a final bit of housekeeping, we'll want to go from intervals back to moments, with some additional inequalities.\r\n```sql\r\n-- The final view we'll want to use.\r\nCREATE VIEW moments AS\r\nSELECT second AS moment FROM seconds\r\nWHERE mz_now() >= second\r\n  AND mz_now() < second + '1 day';\r\n```\r\nThe only change here is the `mz_now()` inequality, which now avoids `BETWEEN` because it has inclusive upper bounds.\r\nThe result is now a view that always has exactly 24 * 60 * 60 = 86400 elements in it.\r\nWe can verify this by subscribing to the changelog of the count query:\r\n```sql\r\n-- Determine the count and monitor its changes.\r\nCOPY (\r\n    SUBSCRIBE (SELECT COUNT(*) FROM moments) \r\n    WITH (progress = true)\r\n)\r\nTO stdout;\r\n```\r\nThis reports an initial value of 86400, and then repeatedly reports (second by second) that there are no additional changes.\r\n```\r\nmaterialize=> COPY (\r\n    SUBSCRIBE (SELECT COUNT(*) FROM moments) \r\n    WITH (progress = true)\r\n)\r\nTO stdout;\r\n1716210913609\tt\t\\N\t\\N\r\n1716210913609\tf\t1\t86400\r\n1716210914250\tt\t\\N\t\\N\r\n1716210914264\tt\t\\N\t\\N\r\n1716210914685\tt\t\\N\t\\N\r\n1716210915000\tt\t\\N\t\\N\r\n1716210915684\tt\t\\N\t\\N\r\n1716210916000\tt\t\\N\t\\N\r\n1716210916248\tt\t\\N\t\\N\r\n1716210916288\tt\t\\N\t\\N\r\n1716210916330\tt\t\\N\t\\N\r\n1716210916683\tt\t\\N\t\\N\r\n^CCancel request sent\r\nERROR:  canceling statement due to user request\r\nmaterialize=> \r\n```\r\nAll rows with a second column of `t` are 'progress' statements rather than data updates.\r\nThe second row, the only one with a `f`, confirms a single record (`1`) with a value of `86400`.\r\n\r\nYeah, that's it! The only thing left is to read a wall of text containing all the SQL.\r\nActually, I recommend bouncing up to the start of the post again, and confirming that the pieces fit together for you.\r\nIt's also a fine time to [try out Materialize](https://materialize.com/register/), the only system that can run all of these views. \r\n\r\n### Appendix: All the SQL\r\n\r\n```sql\r\nCREATE TABLE empty (e TIMESTAMP);\r\n\r\n-- Supporting view to translate ids into text.\r\nCREATE VIEW items (id, item) AS VALUES\r\n    (0, 'Signed Memorabilia'),\r\n    (1, 'City Bar Crawl'),\r\n    (2, 'Best Pizza in Town'),\r\n    (3, 'Gift Basket'),\r\n    (4, 'Custom Art');\r\n\r\n-- Each year-long interval of interest\r\nCREATE VIEW years AS\r\nSELECT * \r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 year') year\r\nWHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';\r\n\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(year, year + '1 year' - '1 day'::interval, '1 day') as day\r\n    FROM years\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day' + '1 day';\r\n\r\n-- Each hour-long interval of interest\r\nCREATE VIEW hours AS\r\nSELECT * FROM (\r\n    SELECT generate_series(day, day + '1 day' - '1 hour'::interval, '1 hour') as hour\r\n    FROM days\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN hour AND hour + '1 hour' + '1 day';\r\n\r\n-- Each minute-long interval of interest\r\nCREATE VIEW minutes AS\r\nSELECT * FROM (\r\n    SELECT generate_series(hour, hour + '1 hour' - '1 minute'::interval, '1 minute') AS minute\r\n    FROM hours\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN minute AND minute + '1 minute' + '1 day';\r\n\r\n-- Any second-long interval of interest\r\nCREATE VIEW seconds AS\r\nSELECT * FROM (\r\n    SELECT generate_series(minute, minute + '1 minute' - '1 second'::interval, '1 second') as second\r\n    FROM minutes\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN second AND second + '1 second' + '1 day';\r\n\r\n-- Indexes are important to ensure we expand intervals carefully.\r\nCREATE DEFAULT INDEX ON years;\r\nCREATE DEFAULT INDEX ON days;\r\nCREATE DEFAULT INDEX ON hours;\r\nCREATE DEFAULT INDEX ON minutes;\r\nCREATE DEFAULT INDEX ON seconds;\r\n\r\n-- The final view we'll want to use .\r\nCREATE VIEW moments AS\r\nSELECT second AS moment FROM seconds\r\nWHERE mz_now() >= second\r\n  AND mz_now() < second + '1 day';\r\n\r\n-- Extract pseudorandom bytes from each moment.\r\nCREATE VIEW random AS\r\nSELECT moment, digest(moment::text, 'md5') as random\r\nFROM moments;\r\n\r\n-- Present as auction \r\nCREATE VIEW auctions_core AS\r\nSELECT \r\n    moment,\r\n    random,\r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 as seller,\r\n    get_byte(random, 5) as item,\r\n    -- Have each auction expire after up to 256 minutes.\r\n    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time\r\nFROM random;\r\n\r\n-- Refine and materialize auction data.\r\nCREATE MATERIALIZED VIEW auctions AS\r\nSELECT auctions_core.id, seller, items.item, end_time\r\nFROM auctions_core, items\r\nWHERE auctions_core.item % 5 = items.id;\r\n\r\n-- Create and materialize bid data.\r\nCREATE MATERIALIZED VIEW bids AS\r\n-- Establish per-bid records and randomness.\r\nWITH prework AS (\r\n    SELECT \r\n        id AS auction_id,\r\n        moment as auction_start,\r\n        end_time as auction_end,\r\n        digest(random::text || generate_series(1, get_byte(random, 5))::text, 'md5') as random\r\n    FROM auctions_core\r\n)\r\nSELECT \r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id, \r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 AS buyer,\r\n    auction_id,\r\n    get_byte(random, 5)::numeric AS amount,\r\n    auction_start + (get_byte(random, 6)::text || ' minutes')::interval as bid_time\r\nFROM prework;\r\n```\r\n<!-- ##{'timestamp':1716094800}## -->\r\n\r\n\u3002", "top": 0, "createdAt": 1728537546, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-10-10", "dateLabelColor": "#bc4c00"}}, "singeListJson": {}, "labelColorDict": {"bug": "#d73a4a", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "question": "#d876e3", "wontfix": "#ffffff"}, "displayTitle": "Computer Scientist", "faviconUrl": "https://github.githubassets.com/favicons/favicon.svg", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://nuowoo.github.io/blog", "prevUrl": "disabled", "nextUrl": "disabled"}