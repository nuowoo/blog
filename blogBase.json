{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "Blog Title", "subTitle": "Blog description", "avatarUrl": "https://github.githubassets.com/favicons/favicon.svg", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/Demonstrating Operational Data with SQL.html", "labels": ["documentation"], "postTitle": "Demonstrating Operational Data with SQL", "postUrl": "post/Demonstrating%20Operational%20Data%20with%20SQL.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/1", "commentNum": 0, "wordCount": 39181, "description": "\r\n\r\nDatabases, Big Data, and Stream Processors have long had the property that it can be hard to *demonstrate* their value, like in a demo setting.\r\nDatabases coordinate the work of multiple teams of independent workers, and don't shine when there is just one user.\r\nBig Data systems introduce scalable patterns that can be purely overhead when the data fit on a single laptop.\r\nStream Processors aim to get the lowest of end-to-end latencies, but do nothing of any consequence on static data.\r\nThese systems demonstrate value when you have variety, volume, and velocity, and most demo data sets have none of these.\r\n\r\nMaterialize, an operational data warehouse backed by scalable streaming systems, has all three of these challenges!\r\n\r\nFortunately, Materialize is powerful enough to synthesize its own operational data for demonstration purposes.\r\nIn this post, we'll build a recipe for a generic live data source using standard SQL primitives and some Materialize magic.\r\nWe'll then add various additional flavors: distributions over keys, irregular validity, foreign key relationships.\r\nIt's all based off of Materialize's own [auction load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction), but it's written entirely in SQL and something that I can customize as my needs evolve.\r\n\r\nThe thing I find most amazing here is that with just SQL you can create *live* data. \r\nData that comes and goes, changes, and respects invariants as it does.\r\nAnd that the gap between your idea for live data and making it happen is just typing some SQL.\r\n\r\n### My Motivation: Materialize\r\n\r\nMaterialize has a few product beats it wants to hit when we demo it, derived from our product principles.\r\n\r\n* **Responsiveness**: Materialize should be able to get back to you ASAP, even with lots of data involved.\r\n* **Freshness**: Materialize should reflect arbitrary updates almost immediately, even through complex logic.\r\n* **Consistency**: Materialize's outputs should always reflect a consistent state, even across multiple users and views.\r\n\r\nWe want to get folks to that 'aha!' moment where they realize that Materialize is like no other technology they know of.\r\nUntil that moment, Materialize could just be a trenchcoat containing Postgres, Spark, and Flink stacked according to your preferences.\r\n\r\nOf course, different contexts connect for different users.\r\nSome folks think about transactions and fraud and want to see how to get in front of that.\r\nOthers have users of their own, and know that sluggish, stale, inconsistent results are how they lose their users, and want to feel the lived experience.\r\nMany users won't believe a thing until the data looks like their data, with the same schemas and data distributions, and the same business logic.\r\nThese are all legitimate concerns, and to me they speak to the inherent *heterogeneity* involved in demonstrating something.\r\n\r\nI want to be able to demonstrate Materialize more **effectively**, which is some amount tied up in demonstrating it more **flexibly**.\r\n\r\nAs a personal first, I'm going to try telling the story in reverse order, Memento-style.\r\nWe'll start with the outcomes, which I hope will make sense, and then figure out how we got there, and eventually arrive at the wall of SQL that makes it happen.\r\nIt does mean we'll need some suspension of disbelief as we go, though; bear with me!\r\nI do hope that whichever prefix you can tolerate makes sense and is engaging, and am only certain that if we started with the SQL it would not be.\r\n\r\nThe outine is, roughly:\r\n\r\n1.  [Demonstrating Materialize with auction data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#demonstrating-materialize)\r\n\r\n    We'll work through Materialize's quick start to show off `auctions` and `bids` data, and give a feel for what we need to have our live data do.\r\n    We're going to hit the beats of responsiveness, freshness, and consistency along the way.\r\n\r\n2.  [Building an Auction loadgen from unrelated live data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#auction-data-from-changing-moments)\r\n\r\n    Here we'll build live views that define `auctions` and `bids`, starting from a live view that just contains recent timestamps.\r\n    We'll see how to turn largely nonsense data into plausible auctions and bids, through the magic of pseudorandomness.\r\n\r\n3.  [Building live random data from just SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#operational-data-from-thin-air)\r\n\r\n    Starting from nothing more than SQL, we'll create a live view that Materialize can maintain containing recent moments as timestamps.\r\n    As time continually moves forward, those moments continually change.\r\n\r\n4.  [All the SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#appendix-all-the-sql) Really, just SQL.\r\n\r\nFeel more than welcome to leap to the sections that interest you most.\r\nI recommend starting at the beginning, though!\r\n\r\n### Demonstrating Materialize\r\n\r\nLet's sit down with Materialize and some live auction data and see if we can't hit the beats of responsiveness, freshness, and consistency.\r\nThe story is borrowed from our own quickstart, but by the end of it we'll find we've swapped out the quickstart's built-in load generator.\r\n\r\nMaterialize's [`AUCTION` load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction) populates `auctions` and `bids` tables.\r\nTheir contents look roughly like so:\r\n```\r\nmaterialize=> select * from auctions;\r\n id | seller |        item        |          end_time          \r\n----+--------+--------------------+----------------------------\r\n  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00\r\n  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00\r\n  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00\r\n  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00\r\n  ...\r\n(4 rows)\r\n```\r\n```\r\nmaterialize=> select * from bids;\r\n id | buyer | auction_id | amount |          bid_time          \r\n----+-------+------------+--------+----------------------------\r\n 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00\r\n 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00\r\n 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00\r\n 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00\r\n ...\r\n```\r\n\r\nWe will root around in this data, as it changes, and show off Materialize as something unlike other data tools.\r\nSpecifically we'll want to show off responsiveness, freshness, and consistency, which we'll do in that order.\r\nHowever, the point is that you get them all at the same time, rather than one at a time, and by the end we should be able to see all three at once.\r\n\r\n#### Beat 1: Responsiveness\r\n\r\nMaterialize is able to respond immediately, even to complex queries over large volumes of data.\r\nLet's start by looking at the data, counting the number of auctions and the number of bids.\r\n```\r\nmaterialize=> select count(*) from auctions;\r\n count \r\n-------\r\n 86400\r\n(1 row)\r\n\r\nTime: 52.580 ms\r\n```\r\n```\r\nmaterialize=> select count(*) from bids;\r\n  count   \r\n----------\r\n 10994252\r\n(1 row)\r\n\r\nTime: 8139.897 ms (00:08.140)\r\n```\r\nIt's almost 100k auctions, and over 10M bids across them.\r\nThe specific numbers will make more sense when we get to the generator, but some of you may already recognize 86,400.\r\nTen seconds to count ten million things is not great, but this is running on our smallest instance (`25cc`; roughly 1/4 of a core).\r\nAlso, we aren't yet using Materialize's super-power to *maintain* results.\r\n\r\nMaterialize maintains computed results in indexes, created via the `CREATE INDEX` command.\r\n```sql\r\n-- Maintain bids indexed by id.\r\nCREATE INDEX bids_id ON bids (id);\r\n```\r\n\r\nWhen we want to find a specific bid by id, this can be very fast .\r\n```\r\nmaterialize=> select * from bids where id = 4;\r\n id | buyer | auction_id | amount |        bid_time        \r\n----+-------+------------+--------+------------------------\r\n  4 |   228 |    6492730 |    149 | 2024-06-19 13:57:50+00\r\n(1 row)\r\n\r\nTime: 19.711 ms\r\n```\r\nInspecting the query history (a feature in Materialize's console) we can see it only took 5ms for the DB, and the additional latency is between NYC and AWS's us-east-1.\r\nThis really is just a look-up into a maintained index, admittedly only on `bids` rather than some sophisticated query.\r\n\r\nYou can build indexes on any collection of data, not just raw data like `bids`.\r\nWe could build an index on `SELECT COUNT(*) FROM bids` to make that fast too, for example.\r\nInstead, let's go straight to the good stuff.\r\n\r\nHere's a view that determines which auctions are won by which bids.\r\n```sql\r\n-- Determine auction winners: the greatest bid before expiration.\r\nCREATE VIEW winning_bids AS\r\n  SELECT DISTINCT ON (auctions.id) bids.*,\r\n    auctions.item,\r\n    auctions.seller\r\n  FROM auctions, bids\r\n  WHERE auctions.id = bids.auction_id\r\n    AND bids.bid_time < auctions.end_time\r\n    AND mz_now() >= auctions.end_time\r\n  ORDER BY auctions.id,\r\n    bids.amount DESC,\r\n    bids.bid_time,\r\n    bids.buyer;\r\n```\r\n\r\nDirectly querying this view results in a not-especially-responsive experience\r\n```\r\nmaterialize=> select auction_id, buyer, amount from winning_bids limit 5;\r\n auction_id | buyer | amount \r\n------------+-------+--------\r\n        217 |    41 |    252\r\n       3328 |   209 |     55\r\n      19201 |   147 |    255\r\n      18947 |    34 |    254\r\n       7173 |   143 |      5\r\n(5 rows)\r\n\r\nTime: 87428.589 ms (01:27.429)\r\n```\r\nWe are grinding through all the bids from scratch when you select from a view, because the view only explains what query you want to run.\r\nA view by itself doesn't cause any work to be done ahead of time.\r\n\r\nHowever, we can create indexes on `winning_bids`, and once they are up and running everything gets better.\r\nWe are going to create two indexes, on the columns `buyer` and `seller`, for future storytelling reasons.\r\n```sql\r\n-- Compute and maintain winning bids, indexed two ways.\r\nCREATE INDEX wins_by_buyer ON winning_bids (buyer);\r\nCREATE INDEX wins_by_seller ON winning_bids (seller);\r\n```\r\nThe auctions aren't faster to magic in to existence than the original query was, so we'll have to wait a moment for them to hydrate.\r\nOnce this has happened, you get responsive interactions with the view.\r\n```\r\nmaterialize=> select auction_id, buyer, amount from winning_bids limit 5;\r\n auction_id | buyer | amount \r\n------------+-------+--------\r\n    7647534 |     0 |    254\r\n    6568079 |     0 |    239\r\n   10578840 |     0 |    254\r\n   14208479 |     0 |    249\r\n   15263465 |     0 |    199\r\n(5 rows)\r\n\r\nTime: 61.283 ms\r\n```\r\nRather than grind over the ten million or so bids to find winners, the ~80,000 results are maintained and its easy to read the first five.\r\nMoreover, the results are all immediately up to date, rather than being fast-but-stale.\r\nLet's hit that **freshness** beat now!\r\n\r\n<!-- \r\nIn addition, our indexes set us up for responsive ad-hoc queries.\r\nHere's an example where we look for 'auction flippers': folks who are both buyers and sellers of the same item at increased amounts:\r\n```sql\r\n-- Look for users who re-sell their winnings\r\nCREATE VIEW potential_flips AS\r\n  SELECT w2.seller,\r\n         w2.item AS item,\r\n         w2.amount AS seller_amount,\r\n         w1.amount AS buyer_amount\r\n  FROM winning_bids w1,\r\n       winning_bids w2\r\n  WHERE w1.buyer = w2.seller\r\n    AND w2.amount > w1.amount\r\n    AND w1.item = w2.item;\r\n```\r\n\r\nWe have enough auctions that some folks will be both buyers and sellers, and for some fraction of them its the same item for an increased price.\r\n```\r\nmaterialize=> select count(*) from potential_flips;\r\n count \r\n-------\r\n  9755\r\n(1 row)\r\n\r\nTime: 602.481 ms\r\n```\r\n```\r\nmaterialize=> select seller, count(*) from potential_flips group by seller order by count(*) desc limit 5;\r\n seller | count \r\n--------+-------\r\n  42091 |     7\r\n  42518 |     6\r\n  10529 |     6\r\n  39840 |     6\r\n  49317 |     6\r\n(5 rows)\r\n\r\nTime: 678.330 ms\r\n```\r\n\r\nThis is now pretty interactive, using scant resources, over enough data and through complex views that to start from scratch would be exhausting.\r\nHowever, maintained indexes keep intermediate results up to date, and you get the same results as if re-run from scratch, just without the latency. -->\r\n\r\n#### Beat 2: Freshness\r\n\r\nAll of this auction data is synthetic, and while it changes often the show is pretty clearly on rails.\r\nThat is, Materialize knows ahead of time what the changes will be.\r\nYou want to know that Materialize can respond fast to *arbitrary* changes, including ones that Materialize doesn't anticipate.\r\n\r\nWe need **interaction**!\r\n\r\nLet's create a table we can modify, through our own whims and fancies.\r\nOur modifications to this table, not part of the load generator, will be how we demonstrate the speed at which Materialize updates results as data change.\r\n```sql\r\n-- Accounts that we might flag for fraud.\r\nCREATE TABLE fraud_accounts (id bigint);\r\n```\r\n\r\nLet's look at a query that calls out the top five accounts that win auctions.\r\nWe'll subscribe to it, meaning we get to watch the updates as they happen.\r\n```sql\r\n-- Top five non-fraud accounts, by auction wins.\r\nCOPY (SUBSCRIBE TO (\r\n  SELECT buyer, count(*)\r\n  FROM winning_bids\r\n  WHERE buyer NOT IN (SELECT id FROM fraud_accounts)\r\n  GROUP BY buyer\r\n  ORDER BY count(*) DESC, buyer LIMIT 5\r\n)) TO STDOUT;\r\n```\r\nThis produces first a snapshot and then a continual stream of updates.\r\nIn our case, the updates are going to derive from our manipulation of `fraud_accounts`.\r\n```\r\n1718981380562\t1\t7247\t7\r\n1718981380562\t1\t17519\t7\r\n1718981380562\t1\t27558\t7\r\n1718981380562\t1\t20403\t7\r\n1718981380562\t1\t16584\t7\r\n```\r\nThe data are not really changing much, on account of the winners all having the same counts.\r\nBut, this is actually good for us, because we can see what happens when we force a change.\r\n\r\nAt this point, let's insert the record `17519` into `fraud_accounts`.\r\n```\r\n-- Mark 17519 as fraudulent\r\n1718981387841\t-1\t17519\t7\r\n1718981387841\t1\t32134\t7\r\n```\r\nWe can do the same with `16584`, and then `34985`.\r\n```\r\n-- Mark 16584 as fraudulent\r\n1718981392977\t1\t34985\t7\r\n1718981392977\t-1\t16584\t7\r\n-- Mark 34985 as fraudulent\r\n1718981398158\t1\t35131\t7\r\n1718981398158\t-1\t34985\t7\r\n```\r\nFinally, let's remove all records from `fraud_accounts` and we can see that we return back to the original state.\r\n```\r\n-- Remove all fraud indicators.\r\n1718981403087\t-1\t35131\t7\r\n1718981403087\t1\t17519\t7\r\n1718981403087\t-1\t32134\t7\r\n1718981403087\t1\t16584\t7\r\n...\r\n```\r\nThat `34985` record isn't mention here because it only showed up due to our other removals.\r\nWe don't hear about a change because there is no moment when it is in the top five, even transiently.\r\nThat is a great lead-in to Materailize's **consistency** properties!\r\n\r\n#### Beat 3: Consistency\r\n\r\nAll the freshness and responsiveness in the world doesn't mean much if the results are incoherent.\r\nMaterialize only ever presents actual results that actually happened, with no transient errors.\r\nWhen you see results, you can confidently act on them knowing that they are real, and don't need further second to bake.\r\n\r\nLet's take a look at consistency through the lens of account balances as auctions close and winning buyers must pay sellers.\r\n```sql\r\n-- Account ids, with credits and debits from auctions sold and won.\r\nCREATE VIEW funds_movement AS\r\n  SELECT id,\r\n         SUM(credits) AS credits,\r\n         SUM(debits) AS debits\r\n  FROM (\r\n    SELECT seller AS id, amount AS credits, 0 AS debits\r\n    FROM winning_bids\r\n    UNION ALL\r\n    SELECT buyer AS id, 0 AS credits, amount AS debits\r\n    FROM winning_bids\r\n  )\r\n  GROUP BY id;\r\n```\r\n\r\nThese balances derive from the same source: `winning_bids`, and although they'll vary from account to account, they should all add up.\r\nSpecifically, if we get the total credits and the total debits, they should 100% of the time be exactly equal.\r\n```sql\r\n-- Discrepancy between credits and debits.\r\nSELECT SUM(credits) - SUM(debits) \r\nFROM funds_movement;\r\n```\r\nThis query reports zero, 100% of the time.\r\nWe can `SUBSCRIBE` to the query to be notified of any change.\r\n```\r\nmaterialize=> COPY (SUBSCRIBE (\r\n    SELECT SUM(credits) - SUM(debits) \r\n    FROM funds_movement\r\n)) TO STDOUT;\r\n\r\n1716312983129\t1\t0\r\n```\r\nThis tells us that starting at time `1716312983129`, there was `1` record, and it was `0`.\r\nYou can sit there a while, and there will be no changes.\r\nYou could also add the `WITH (PROGRESS)` option, and it will provide regular heartbeats confirming that second-by-second it is still zero.\r\nThe credits and debits always add up, and aren't for a moment inconsistent.\r\n\r\nWe can set up similar views for other assertions.\r\nFor example, every account that has sold or won an auction should have a balance.\r\nA SQL query can look for violations of this, and we can monitor it to see that it is always empty.\r\nIf it is ever non-empty, perhaps there are bugs in the query logic, its contents are immediately actionable: \r\nthere is a specific time where the inputs evaluated to an invariant-violating output, and if you return to that moment you'll see the inputs that produce the bad output.\r\n\r\nThe consistency extends across multiple independent sessions.\r\nThe moment you get confirmation that the insert into `fraud_accounts`, you can be certain that no one will see that account in the top five non-fraudulent auction winners.\r\nThis guarantee is called 'strict serializability', that the system behaves as if every event occurred at a specific time between its start and end, and is the strongest guarantee that databases provide.\r\n\r\n#### Demo over!\r\n\r\nThat's it!\r\nWe've completed the introduction to Materialize, and used auction data to show off responsiveness, freshness, and consistency.\r\nThere's a lot more to show off, of course, and if any of this sounded fascinating you should swing by https://materialize.com/register/ to spin up a trial environment.\r\n\r\nHowever, in this post we will continue to unpack how we got all of that `auctions` and `bids` data in the first place!\r\n\r\n### Auction Data from Changing Moments\r\n\r\nWhere do the `auctions` and `bids` data come from?\r\nYou can get them from our load generator, but we're going to try and coax them out of raw SQL.\r\nWe're going to start with something we haven't introduced yet, but it's a view whose content looks like this\r\n```sql\r\n-- All seconds within the past 24 hours.\r\nCREATE VIEW moments AS\r\nSELECT generate_series(\r\n    now() - '1 day'::interval + '1 second'::interval,\r\n    now(),\r\n    '1 second'\r\n) moment;\r\n``` \r\n\r\nUnpacking this, `moments` contains rows with a single column containing a timestamp.\r\nWhenever we look at it, the view contains those timestamps at most one day less than `now()`.\r\nIt should have at any moment exactly 86,400 records present, as many as `auctions` up above.\r\n\r\nImportantly, this view definition will not actually work for us.\r\nYou are welcome to try it out, but you'll find out that while it can be *inspected*, it cannot be *maintained*.\r\nWe'll fix that by the end of the post, but it will need to wait until the next section.\r\nFor the moment, let's assume we have this view and the magical ability to keep it up to date.\r\n\r\nThese 'moments' are not auction data, though.\r\nHow do we get from moments to auctions and bids?\r\n\r\nThe `auctions` and `bids` collections look roughly like so:\r\n```\r\nmaterialize=> select * from auctions;\r\n id | seller |        item        |          end_time          \r\n----+--------+--------------------+----------------------------\r\n  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00\r\n  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00\r\n  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00\r\n  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00\r\n  ...\r\n(4 rows)\r\n```\r\n```\r\nmaterialize=> select * from bids;\r\n id | buyer | auction_id | amount |          bid_time          \r\n----+-------+------------+--------+----------------------------\r\n 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00\r\n 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00\r\n 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00\r\n 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00\r\n ...\r\n```\r\n\r\nAuctions have a unique id, a seller id, an item description, and an end time.\r\nBids have a unique id (no relation), a buyer id, an auction id, the amount of the bid, and the time of the bid.\r\n\r\nThe `seller`, `item`, `buyer`, and `amount` fields are all random, within some bounds.\r\nAs a first cut, we'll think about just using random values for each of the columns.\r\nWhere might we get randomness, you ask?\r\nWell, if *pseudo*-randomness is good enough (it will be), we can use cryptographic hashes of the moments.\r\n```sql\r\n-- Extract pseudorandom bytes from each moment.\r\nCREATE VIEW random AS\r\nSELECT moment, digest(moment::text, 'md5') as random\r\nFROM moments;\r\n```\r\nLet's start with bytes from `random` to populate columns, and we'd have a first cut at random data.\r\nColumns like `auctions.item` are populated by joining with a constant collection (part of the generator), but `id` and `seller` could just be random.\r\nThe `end_time` we'll pick to be a random time up to 256 minutes after the auction starts.\r\n```sql\r\n-- Totally accurate auction generator.\r\nCREATE VIEW auctions_core AS\r\nSELECT \r\n    moment,\r\n    random,\r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 as seller,\r\n    get_byte(random, 5) as item,\r\n    -- Have each auction expire after up to 256 minutes.\r\n    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time\r\nFROM random;\r\n```\r\nWe've clearly made some calls about how random each of these should be, and those calls influence what we'll see in the data.\r\nFor example, we've established at most 65,536 sellers, which lines up fine with our 86,400 auctions at any moment; some sellers will have multiple auctions and many will not.\r\nAuctions are open for a few hours on average, close out but linger, and then vanish after 24 hours.\r\nIf we want to change any of these, perhaps to add more distinct items, or keep auctions running longer, or to skew the distribution over sellers, we can!\r\n\r\nSimilarly, the columns of `bids` are also pretty random, but columns like `auction_id` and `bid_time` do need to have some relationship to `auctions` and the referenced auction.\r\nWe'll build those out in just a moment, but have a bit more tidying to do for `auctions` first.\r\n\r\n#### Adding Custom Expiration\r\n\r\nOur auctions wind down after some random amount of time, but they are not removed from `auctions` for three hours.\r\nThematically we can think of this as auctions whose winners have been locked in, but whose accounts have not yet been settled.\r\n\r\nIf we want the auction to vanish from `auctions` at this time it closed, we could accomplish this with a temporal filter:\r\n```sql\r\nWHERE mz_now() < end_time\r\n```\r\nAs soon as we reach `end_time` the auction would vanish from `auctions`.\r\n\r\nThis is a very helpful pattern for load generators that want to control when data arrive and when it departs, in finer detail than 'a twenty four hour window'.\r\nFor example, one could randomly generate `insert_ts` and `delete_ts`, and then use\r\n```sql\r\n-- Create an event that is live for the interval `[insert_ts, delete_ts]`.\r\nWHERE mz_now() BETWEEN insert_ts AND delete_ts\r\n```\r\nThis pattern allows careful control of when events *appear* to occur, by holding them back until `mz_now()` reaches a value, and then retracting them when it reaches a later value. \r\n\r\n#### Making More Realistic Data\r\n\r\nOur random numbers for `item` aren't nearly as nice as what the existing load generator produces.\r\nHowever, we can get the same results by putting those nice values in a view and using our integer `item` to join against the view.\r\n```sql\r\n-- A static view giving names to items.\r\nCREATE VIEW items (id, item) AS VALUES\r\n    (0, 'Signed Memorabilia'),\r\n    (1, 'City Bar Crawl'),\r\n    (2, 'Best Pizza in Town'),\r\n    (3, 'Gift Basket'),\r\n    (4, 'Custom Art');\r\n```\r\n\r\nNow when we want to produce an actual auction record, we can join against items like so\r\n```sql\r\n-- View that mirrors the `auctions` table from our load generator.\r\nCREATE VIEW auctions AS\r\nSELECT id, seller, items.item, end_time\r\nFROM auctions_core, items\r\nWHERE auction.item = items.id;\r\n```\r\n\r\nWe've now got a view `auctions` that mirrors what Materialize's load generator produces, at least superficially.\r\n\r\n#### Introducing Foreign Key Constraints\r\n\r\nEach bid in `bids` references an auction, and we are unlikely to find an extant auction if we just use random numbers for `auction_id`.\r\nWe'd like to base our `bids` on the available auctions, and have them occur at times that make sense for the auction.\r\n\r\nWe can accomplish this by deriving the bids for an auction from `auctions` itself.\r\nWe will use some available pseudorandomness to propose a number of bids, and then create further pseudorandomness to determine the details of each bid.\r\n```sql\r\nCREATE VIEW bids AS\r\n-- Establish per-bid records and pseudorandomness.\r\nWITH prework AS (\r\n    -- Create `get_byte(random, 6)` many bids for each auction, \r\n    -- each with their own freshly generated pseudorandomness.\r\n    SELECT \r\n        id as auction_id,\r\n        moment as auction_start,\r\n        end_time as auction_end,\r\n        digest(random::text || generate_series(1, get_byte(random, 6))::text, 'md5') as random\r\n    FROM auctions_core\r\n)\r\nSELECT\r\n    get_byte(random, 0) +\r\n    get_byte(random, 1) * 256 +\r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) AS buyer,\r\n    auction_id,\r\n    get_byte(random, 4)::numeric AS amount,\r\n    auction_start + (get_byte(random, 5)::text || ' seconds')::interval as bid_time\r\nFROM prework;\r\n```\r\n\r\nWe now have a pile of bids for each auction, with the compelling property that when the auction goes away so too do its bids.\r\nThis gives us 'referential integrity', the property of foreign keys (`bids.auction_id`) that their referent (`auction.id`) is always valid.\r\n\r\nAnd with this, we have generated the `auctions` and `bids` data that continually change, but always make sense.\r\n\r\nThere are several other changes you might want to make!\r\nFor example, random bids means that auctions stop changing as they go on, because new random bids are unlikely to beat all prior bids.\r\nYou could instead have the bids trend up with time, to keep the data interesting.\r\nBut, the changes are pretty easy to roll out, and just amount to editing the SQL that defines them.\r\n\r\nLet's pause for now on noodling on ways we could make the data even more realistic.\r\nUp next we have to unpack how we got that `moments` view in the first place.\r\nOnce we've done that, you are welcome to go back to playing around with load generator novelties and variations!\r\n\r\n### Operational Data from Thin Air\r\n\r\nOur `auctions` and `bids` data was based on a view `moments` that showed us all timestamps within the past three hours.\r\nWe saw how we could go from that to pretty much anything, through extracted pseudorandomness.\r\n\r\nWe used a view that seemed maybe too easy, that looked roughly like so:\r\n```sql\r\n-- Generate a sliding window over timestamp data.\r\n-- Arguments: <volume>, <velocity>\r\nSELECT moment,\r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    <velocity>\r\n) moment\r\nWHERE now() BETWEEN moment AND moment + <volume>;\r\n```\r\n\r\nThis example uses `generate_series` to produce moments at which events will occur.\r\nThe `<velocity>` argument chooses the step size of the `generate_series` call, and locks in the cadence of updates.\r\nThe `<volume>` argument controls for how long each record lingers, and sets the steady state size.\r\nThe result is a sliding window over random data, where you get to control the volume and velocity.\r\n\r\nWe used `'1 second'` for the velocity and `'1 day'` for the volume.\r\n\r\nNow, while you can *type* the above, it won't actually run properly if you press enter.\r\nThe query describes 130 years of data, probably at something like a one second update frequency (because you wanted live data, right?).\r\nI don't even know how to determine how many records this is accurately based on all the leap-action that occurs.\r\nMoreover, you won't be able to materialize this view, because `now()` prevents materializations.\r\n\r\nTo actually get this to work, we'll have to use some clever tricks.\r\nThe coming subsections are a sequence of such tricks, and the punchline will be 'it works!', in case that saves you any time.\r\n\r\n#### Clever trick 1: using `mz_now()`\r\n\r\nOur first clever trick is to move from `now()` to `mz_now()`.\r\nThese are very similar functions, where the `now()` function gets you the contents of the system clock, and `mz_now()` gets you the transaction time of your command.\r\nThe main difference between the two is that we can materialize some queries containing `mz_now()`, unlike any query containing `now()`.\r\n\r\n```sql\r\n-- Generate a sliding window over timestamp data.\r\nSELECT moment,\r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 second'\r\n) moment\r\n--    /------\\---- LOOK HERE!\r\nWHERE mz_now() BETWEEN moment AND moment + '1 day';\r\n```\r\nThis very simple change means that Materialize now has the ability to keep the query up to date.\r\nMaterialize has a feature called ['temporal filters'](https://materialize.com/docs/transform-data/patterns/temporal-filters/) that allows `mz_now()` in `WHERE` clauses, because we are able to invert the clause and see the moment (Materialize time) at which changes will occur.\r\n\r\nUnfortunately, the implementation strategy for keeping this view up to date still involves first producing all the data, and then filtering it (we don't have any magical insight into `generate_series` that allows us to invert its implementation).\r\nBut fortunately, we have other clever tricks available to us.\r\n\r\n#### Clever trick 2: Hierachical Generation\r\n\r\nThe problem above is that we generate all the data at once, and then filter it.\r\nWe could instead generate the years of interest, from them the days of interest, from them the hours of interest, then minutes of interest, then seconds of interest, and finally milliseconds of interest.\r\nIn a sense we are generating *intervals* rather than *moments*, and then producing moments from the intervals.\r\n\r\nLet's start by generating all the years we might be interested in.\r\nWe start with all the years we might reasonably need, and a `WHERE` clause that checks for intersection of the interval (`+ '1 year'`) and the extension by volume (`+ '1 day'`).\r\n```sql\r\n-- Each year-long interval of interest\r\nCREATE VIEW years AS\r\nSELECT * \r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 year') year\r\nWHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';\r\n```\r\nThis view does not have all that many years in it. \r\nRoughly 130 of them.\r\nFew enough that we can filter them down, and get to work on days.\r\n\r\nAt this point, we'll repeatedly refine the intervals by subdividing into the next granularity.\r\nWe'll do this for years into days, but you'll have to use your imagination for the others.\r\nWe have all the SQL at the end, so don't worry that you'll miss out on that.\r\n```sql\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(\r\n        year, \r\n        year + '1 year' - '1 day'::interval, \r\n        '1 day') as day\r\n    FROM years\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day';\r\n```\r\nWe'll repeat this on to a view `seconds`, and stop there.\r\n\r\nAlthough we could continue to milliseconds, experience has been that it's hard to demo things changing that quickly through SQL.\r\nLines of text flow past like the Matrix, and all you can really see is that there is change, not what the change is.\r\n\r\nUnfortunately, there is a final gotcha.\r\nMaterialize is too clever by half, and if you materialize the `seconds` view, it will see that it is able to determine the entire 130 year timeline of the view, history and future, and record it for you.\r\nAt great expense.\r\nThese declarative systems are sometimes just too smart.\r\n\r\n#### Clever trick 3: An empty table\r\n\r\nWe can fix everything by introducing an empty table.\r\n\r\nThe empty table is only present to ruin Materialize's ability to be certain it already knows the right answer about the future.\r\nWe'll introduce it to each of our views in the same place, and its only function is to menace Materialize with the possibility that it *could* contain data.\r\nBut it won't.\r\nBut we wont tell Materialize that.\r\n\r\n```sql\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(\r\n        year, \r\n        year + '1 year' - '1 day'::interval, \r\n        '1 day') as day\r\n    FROM years\r\n    -- THIS NEXT LINE IS NEW!!\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day';\r\n```\r\n\r\nWith these tricks in hand, we now have the ability to spin it up and see what it looks like.\r\n\r\n```sql\r\nCREATE DEFAULT INDEX ON days;\r\n```\r\n\r\nWe'll want to create the same default indexes on our other views: `hours`, `minutes`, and `seconds`.\r\nImportantly, we want to create them in this order, also, to make sure that each relies on the one before it.\r\nIf they did not, we would be back in the world of the previous section, where each would read ahead until the end of time (the year 2099, in this example).\r\n\r\n#### Finishing touches\r\n\r\nAs a final bit of housekeeping, we'll want to go from intervals back to moments, with some additional inequalities.\r\n```sql\r\n-- The final view we'll want to use.\r\nCREATE VIEW moments AS\r\nSELECT second AS moment FROM seconds\r\nWHERE mz_now() >= second\r\n  AND mz_now() < second + '1 day';\r\n```\r\nThe only change here is the `mz_now()` inequality, which now avoids `BETWEEN` because it has inclusive upper bounds.\r\nThe result is now a view that always has exactly 24 * 60 * 60 = 86400 elements in it.\r\nWe can verify this by subscribing to the changelog of the count query:\r\n```sql\r\n-- Determine the count and monitor its changes.\r\nCOPY (\r\n    SUBSCRIBE (SELECT COUNT(*) FROM moments) \r\n    WITH (progress = true)\r\n)\r\nTO stdout;\r\n```\r\nThis reports an initial value of 86400, and then repeatedly reports (second by second) that there are no additional changes.\r\n```\r\nmaterialize=> COPY (\r\n    SUBSCRIBE (SELECT COUNT(*) FROM moments) \r\n    WITH (progress = true)\r\n)\r\nTO stdout;\r\n1716210913609\tt\t\\N\t\\N\r\n1716210913609\tf\t1\t86400\r\n1716210914250\tt\t\\N\t\\N\r\n1716210914264\tt\t\\N\t\\N\r\n1716210914685\tt\t\\N\t\\N\r\n1716210915000\tt\t\\N\t\\N\r\n1716210915684\tt\t\\N\t\\N\r\n1716210916000\tt\t\\N\t\\N\r\n1716210916248\tt\t\\N\t\\N\r\n1716210916288\tt\t\\N\t\\N\r\n1716210916330\tt\t\\N\t\\N\r\n1716210916683\tt\t\\N\t\\N\r\n^CCancel request sent\r\nERROR:  canceling statement due to user request\r\nmaterialize=> \r\n```\r\nAll rows with a second column of `t` are 'progress' statements rather than data updates.\r\nThe second row, the only one with a `f`, confirms a single record (`1`) with a value of `86400`.\r\n\r\nYeah, that's it! The only thing left is to read a wall of text containing all the SQL.\r\nActually, I recommend bouncing up to the start of the post again, and confirming that the pieces fit together for you.\r\nIt's also a fine time to [try out Materialize](https://materialize.com/register/), the only system that can run all of these views. \r\n\r\n### Appendix: All the SQL\r\n\r\n```sql\r\nCREATE TABLE empty (e TIMESTAMP);\r\n\r\n-- Supporting view to translate ids into text.\r\nCREATE VIEW items (id, item) AS VALUES\r\n    (0, 'Signed Memorabilia'),\r\n    (1, 'City Bar Crawl'),\r\n    (2, 'Best Pizza in Town'),\r\n    (3, 'Gift Basket'),\r\n    (4, 'Custom Art');\r\n\r\n-- Each year-long interval of interest\r\nCREATE VIEW years AS\r\nSELECT * \r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 year') year\r\nWHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';\r\n\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(year, year + '1 year' - '1 day'::interval, '1 day') as day\r\n    FROM years\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day' + '1 day';\r\n\r\n-- Each hour-long interval of interest\r\nCREATE VIEW hours AS\r\nSELECT * FROM (\r\n    SELECT generate_series(day, day + '1 day' - '1 hour'::interval, '1 hour') as hour\r\n    FROM days\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN hour AND hour + '1 hour' + '1 day';\r\n\r\n-- Each minute-long interval of interest\r\nCREATE VIEW minutes AS\r\nSELECT * FROM (\r\n    SELECT generate_series(hour, hour + '1 hour' - '1 minute'::interval, '1 minute') AS minute\r\n    FROM hours\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN minute AND minute + '1 minute' + '1 day';\r\n\r\n-- Any second-long interval of interest\r\nCREATE VIEW seconds AS\r\nSELECT * FROM (\r\n    SELECT generate_series(minute, minute + '1 minute' - '1 second'::interval, '1 second') as second\r\n    FROM minutes\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN second AND second + '1 second' + '1 day';\r\n\r\n-- Indexes are important to ensure we expand intervals carefully.\r\nCREATE DEFAULT INDEX ON years;\r\nCREATE DEFAULT INDEX ON days;\r\nCREATE DEFAULT INDEX ON hours;\r\nCREATE DEFAULT INDEX ON minutes;\r\nCREATE DEFAULT INDEX ON seconds;\r\n\r\n-- The final view we'll want to use .\r\nCREATE VIEW moments AS\r\nSELECT second AS moment FROM seconds\r\nWHERE mz_now() >= second\r\n  AND mz_now() < second + '1 day';\r\n\r\n-- Extract pseudorandom bytes from each moment.\r\nCREATE VIEW random AS\r\nSELECT moment, digest(moment::text, 'md5') as random\r\nFROM moments;\r\n\r\n-- Present as auction \r\nCREATE VIEW auctions_core AS\r\nSELECT \r\n    moment,\r\n    random,\r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 as seller,\r\n    get_byte(random, 5) as item,\r\n    -- Have each auction expire after up to 256 minutes.\r\n    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time\r\nFROM random;\r\n\r\n-- Refine and materialize auction data.\r\nCREATE MATERIALIZED VIEW auctions AS\r\nSELECT auctions_core.id, seller, items.item, end_time\r\nFROM auctions_core, items\r\nWHERE auctions_core.item % 5 = items.id;\r\n\r\n-- Create and materialize bid data.\r\nCREATE MATERIALIZED VIEW bids AS\r\n-- Establish per-bid records and randomness.\r\nWITH prework AS (\r\n    SELECT \r\n        id AS auction_id,\r\n        moment as auction_start,\r\n        end_time as auction_end,\r\n        digest(random::text || generate_series(1, get_byte(random, 5))::text, 'md5') as random\r\n    FROM auctions_core\r\n)\r\nSELECT \r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id, \r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 AS buyer,\r\n    auction_id,\r\n    get_byte(random, 5)::numeric AS amount,\r\n    auction_start + (get_byte(random, 6)::text || ' minutes')::interval as bid_time\r\nFROM prework;\r\n```\u3002", "top": 0, "createdAt": 1728536911, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-10-10", "dateLabelColor": "#bc4c00"}, "P2": {"htmlDir": "docs/post/Demonstrating Operational Data with SQL.html", "labels": ["documentation"], "postTitle": "Demonstrating Operational Data with SQL", "postUrl": "post/Demonstrating%20Operational%20Data%20with%20SQL.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/2", "commentNum": 0, "wordCount": 39179, "description": "\r\nDatabases, Big Data, and Stream Processors have long had the property that it can be hard to *demonstrate* their value, like in a demo setting.\r\nDatabases coordinate the work of multiple teams of independent workers, and don't shine when there is just one user.\r\nBig Data systems introduce scalable patterns that can be purely overhead when the data fit on a single laptop.\r\nStream Processors aim to get the lowest of end-to-end latencies, but do nothing of any consequence on static data.\r\nThese systems demonstrate value when you have variety, volume, and velocity, and most demo data sets have none of these.\r\n\r\nMaterialize, an operational data warehouse backed by scalable streaming systems, has all three of these challenges!\r\n\r\nFortunately, Materialize is powerful enough to synthesize its own operational data for demonstration purposes.\r\nIn this post, we'll build a recipe for a generic live data source using standard SQL primitives and some Materialize magic.\r\nWe'll then add various additional flavors: distributions over keys, irregular validity, foreign key relationships.\r\nIt's all based off of Materialize's own [auction load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction), but it's written entirely in SQL and something that I can customize as my needs evolve.\r\n\r\nThe thing I find most amazing here is that with just SQL you can create *live* data. \r\nData that comes and goes, changes, and respects invariants as it does.\r\nAnd that the gap between your idea for live data and making it happen is just typing some SQL.\r\n\r\n### My Motivation: Materialize\r\n\r\nMaterialize has a few product beats it wants to hit when we demo it, derived from our product principles.\r\n\r\n* **Responsiveness**: Materialize should be able to get back to you ASAP, even with lots of data involved.\r\n* **Freshness**: Materialize should reflect arbitrary updates almost immediately, even through complex logic.\r\n* **Consistency**: Materialize's outputs should always reflect a consistent state, even across multiple users and views.\r\n\r\nWe want to get folks to that 'aha!' moment where they realize that Materialize is like no other technology they know of.\r\nUntil that moment, Materialize could just be a trenchcoat containing Postgres, Spark, and Flink stacked according to your preferences.\r\n\r\nOf course, different contexts connect for different users.\r\nSome folks think about transactions and fraud and want to see how to get in front of that.\r\nOthers have users of their own, and know that sluggish, stale, inconsistent results are how they lose their users, and want to feel the lived experience.\r\nMany users won't believe a thing until the data looks like their data, with the same schemas and data distributions, and the same business logic.\r\nThese are all legitimate concerns, and to me they speak to the inherent *heterogeneity* involved in demonstrating something.\r\n\r\nI want to be able to demonstrate Materialize more **effectively**, which is some amount tied up in demonstrating it more **flexibly**.\r\n\r\nAs a personal first, I'm going to try telling the story in reverse order, Memento-style.\r\nWe'll start with the outcomes, which I hope will make sense, and then figure out how we got there, and eventually arrive at the wall of SQL that makes it happen.\r\nIt does mean we'll need some suspension of disbelief as we go, though; bear with me!\r\nI do hope that whichever prefix you can tolerate makes sense and is engaging, and am only certain that if we started with the SQL it would not be.\r\n\r\nThe outine is, roughly:\r\n\r\n1.  [Demonstrating Materialize with auction data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#demonstrating-materialize)\r\n\r\n    We'll work through Materialize's quick start to show off `auctions` and `bids` data, and give a feel for what we need to have our live data do.\r\n    We're going to hit the beats of responsiveness, freshness, and consistency along the way.\r\n\r\n2.  [Building an Auction loadgen from unrelated live data](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#auction-data-from-changing-moments)\r\n\r\n    Here we'll build live views that define `auctions` and `bids`, starting from a live view that just contains recent timestamps.\r\n    We'll see how to turn largely nonsense data into plausible auctions and bids, through the magic of pseudorandomness.\r\n\r\n3.  [Building live random data from just SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#operational-data-from-thin-air)\r\n\r\n    Starting from nothing more than SQL, we'll create a live view that Materialize can maintain containing recent moments as timestamps.\r\n    As time continually moves forward, those moments continually change.\r\n\r\n4.  [All the SQL](https://github.com/frankmcsherry/blog/blob/master/posts/2024-05-19.md#appendix-all-the-sql) Really, just SQL.\r\n\r\nFeel more than welcome to leap to the sections that interest you most.\r\nI recommend starting at the beginning, though!\r\n\r\n### Demonstrating Materialize\r\n\r\nLet's sit down with Materialize and some live auction data and see if we can't hit the beats of responsiveness, freshness, and consistency.\r\nThe story is borrowed from our own quickstart, but by the end of it we'll find we've swapped out the quickstart's built-in load generator.\r\n\r\nMaterialize's [`AUCTION` load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction) populates `auctions` and `bids` tables.\r\nTheir contents look roughly like so:\r\n```\r\nmaterialize=> select * from auctions;\r\n id | seller |        item        |          end_time          \r\n----+--------+--------------------+----------------------------\r\n  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00\r\n  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00\r\n  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00\r\n  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00\r\n  ...\r\n(4 rows)\r\n```\r\n```\r\nmaterialize=> select * from bids;\r\n id | buyer | auction_id | amount |          bid_time          \r\n----+-------+------------+--------+----------------------------\r\n 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00\r\n 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00\r\n 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00\r\n 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00\r\n ...\r\n```\r\n\r\nWe will root around in this data, as it changes, and show off Materialize as something unlike other data tools.\r\nSpecifically we'll want to show off responsiveness, freshness, and consistency, which we'll do in that order.\r\nHowever, the point is that you get them all at the same time, rather than one at a time, and by the end we should be able to see all three at once.\r\n\r\n#### Beat 1: Responsiveness\r\n\r\nMaterialize is able to respond immediately, even to complex queries over large volumes of data.\r\nLet's start by looking at the data, counting the number of auctions and the number of bids.\r\n```\r\nmaterialize=> select count(*) from auctions;\r\n count \r\n-------\r\n 86400\r\n(1 row)\r\n\r\nTime: 52.580 ms\r\n```\r\n```\r\nmaterialize=> select count(*) from bids;\r\n  count   \r\n----------\r\n 10994252\r\n(1 row)\r\n\r\nTime: 8139.897 ms (00:08.140)\r\n```\r\nIt's almost 100k auctions, and over 10M bids across them.\r\nThe specific numbers will make more sense when we get to the generator, but some of you may already recognize 86,400.\r\nTen seconds to count ten million things is not great, but this is running on our smallest instance (`25cc`; roughly 1/4 of a core).\r\nAlso, we aren't yet using Materialize's super-power to *maintain* results.\r\n\r\nMaterialize maintains computed results in indexes, created via the `CREATE INDEX` command.\r\n```sql\r\n-- Maintain bids indexed by id.\r\nCREATE INDEX bids_id ON bids (id);\r\n```\r\n\r\nWhen we want to find a specific bid by id, this can be very fast .\r\n```\r\nmaterialize=> select * from bids where id = 4;\r\n id | buyer | auction_id | amount |        bid_time        \r\n----+-------+------------+--------+------------------------\r\n  4 |   228 |    6492730 |    149 | 2024-06-19 13:57:50+00\r\n(1 row)\r\n\r\nTime: 19.711 ms\r\n```\r\nInspecting the query history (a feature in Materialize's console) we can see it only took 5ms for the DB, and the additional latency is between NYC and AWS's us-east-1.\r\nThis really is just a look-up into a maintained index, admittedly only on `bids` rather than some sophisticated query.\r\n\r\nYou can build indexes on any collection of data, not just raw data like `bids`.\r\nWe could build an index on `SELECT COUNT(*) FROM bids` to make that fast too, for example.\r\nInstead, let's go straight to the good stuff.\r\n\r\nHere's a view that determines which auctions are won by which bids.\r\n```sql\r\n-- Determine auction winners: the greatest bid before expiration.\r\nCREATE VIEW winning_bids AS\r\n  SELECT DISTINCT ON (auctions.id) bids.*,\r\n    auctions.item,\r\n    auctions.seller\r\n  FROM auctions, bids\r\n  WHERE auctions.id = bids.auction_id\r\n    AND bids.bid_time < auctions.end_time\r\n    AND mz_now() >= auctions.end_time\r\n  ORDER BY auctions.id,\r\n    bids.amount DESC,\r\n    bids.bid_time,\r\n    bids.buyer;\r\n```\r\n\r\nDirectly querying this view results in a not-especially-responsive experience\r\n```\r\nmaterialize=> select auction_id, buyer, amount from winning_bids limit 5;\r\n auction_id | buyer | amount \r\n------------+-------+--------\r\n        217 |    41 |    252\r\n       3328 |   209 |     55\r\n      19201 |   147 |    255\r\n      18947 |    34 |    254\r\n       7173 |   143 |      5\r\n(5 rows)\r\n\r\nTime: 87428.589 ms (01:27.429)\r\n```\r\nWe are grinding through all the bids from scratch when you select from a view, because the view only explains what query you want to run.\r\nA view by itself doesn't cause any work to be done ahead of time.\r\n\r\nHowever, we can create indexes on `winning_bids`, and once they are up and running everything gets better.\r\nWe are going to create two indexes, on the columns `buyer` and `seller`, for future storytelling reasons.\r\n```sql\r\n-- Compute and maintain winning bids, indexed two ways.\r\nCREATE INDEX wins_by_buyer ON winning_bids (buyer);\r\nCREATE INDEX wins_by_seller ON winning_bids (seller);\r\n```\r\nThe auctions aren't faster to magic in to existence than the original query was, so we'll have to wait a moment for them to hydrate.\r\nOnce this has happened, you get responsive interactions with the view.\r\n```\r\nmaterialize=> select auction_id, buyer, amount from winning_bids limit 5;\r\n auction_id | buyer | amount \r\n------------+-------+--------\r\n    7647534 |     0 |    254\r\n    6568079 |     0 |    239\r\n   10578840 |     0 |    254\r\n   14208479 |     0 |    249\r\n   15263465 |     0 |    199\r\n(5 rows)\r\n\r\nTime: 61.283 ms\r\n```\r\nRather than grind over the ten million or so bids to find winners, the ~80,000 results are maintained and its easy to read the first five.\r\nMoreover, the results are all immediately up to date, rather than being fast-but-stale.\r\nLet's hit that **freshness** beat now!\r\n\r\n<!-- \r\nIn addition, our indexes set us up for responsive ad-hoc queries.\r\nHere's an example where we look for 'auction flippers': folks who are both buyers and sellers of the same item at increased amounts:\r\n```sql\r\n-- Look for users who re-sell their winnings\r\nCREATE VIEW potential_flips AS\r\n  SELECT w2.seller,\r\n         w2.item AS item,\r\n         w2.amount AS seller_amount,\r\n         w1.amount AS buyer_amount\r\n  FROM winning_bids w1,\r\n       winning_bids w2\r\n  WHERE w1.buyer = w2.seller\r\n    AND w2.amount > w1.amount\r\n    AND w1.item = w2.item;\r\n```\r\n\r\nWe have enough auctions that some folks will be both buyers and sellers, and for some fraction of them its the same item for an increased price.\r\n```\r\nmaterialize=> select count(*) from potential_flips;\r\n count \r\n-------\r\n  9755\r\n(1 row)\r\n\r\nTime: 602.481 ms\r\n```\r\n```\r\nmaterialize=> select seller, count(*) from potential_flips group by seller order by count(*) desc limit 5;\r\n seller | count \r\n--------+-------\r\n  42091 |     7\r\n  42518 |     6\r\n  10529 |     6\r\n  39840 |     6\r\n  49317 |     6\r\n(5 rows)\r\n\r\nTime: 678.330 ms\r\n```\r\n\r\nThis is now pretty interactive, using scant resources, over enough data and through complex views that to start from scratch would be exhausting.\r\nHowever, maintained indexes keep intermediate results up to date, and you get the same results as if re-run from scratch, just without the latency. -->\r\n\r\n#### Beat 2: Freshness\r\n\r\nAll of this auction data is synthetic, and while it changes often the show is pretty clearly on rails.\r\nThat is, Materialize knows ahead of time what the changes will be.\r\nYou want to know that Materialize can respond fast to *arbitrary* changes, including ones that Materialize doesn't anticipate.\r\n\r\nWe need **interaction**!\r\n\r\nLet's create a table we can modify, through our own whims and fancies.\r\nOur modifications to this table, not part of the load generator, will be how we demonstrate the speed at which Materialize updates results as data change.\r\n```sql\r\n-- Accounts that we might flag for fraud.\r\nCREATE TABLE fraud_accounts (id bigint);\r\n```\r\n\r\nLet's look at a query that calls out the top five accounts that win auctions.\r\nWe'll subscribe to it, meaning we get to watch the updates as they happen.\r\n```sql\r\n-- Top five non-fraud accounts, by auction wins.\r\nCOPY (SUBSCRIBE TO (\r\n  SELECT buyer, count(*)\r\n  FROM winning_bids\r\n  WHERE buyer NOT IN (SELECT id FROM fraud_accounts)\r\n  GROUP BY buyer\r\n  ORDER BY count(*) DESC, buyer LIMIT 5\r\n)) TO STDOUT;\r\n```\r\nThis produces first a snapshot and then a continual stream of updates.\r\nIn our case, the updates are going to derive from our manipulation of `fraud_accounts`.\r\n```\r\n1718981380562\t1\t7247\t7\r\n1718981380562\t1\t17519\t7\r\n1718981380562\t1\t27558\t7\r\n1718981380562\t1\t20403\t7\r\n1718981380562\t1\t16584\t7\r\n```\r\nThe data are not really changing much, on account of the winners all having the same counts.\r\nBut, this is actually good for us, because we can see what happens when we force a change.\r\n\r\nAt this point, let's insert the record `17519` into `fraud_accounts`.\r\n```\r\n-- Mark 17519 as fraudulent\r\n1718981387841\t-1\t17519\t7\r\n1718981387841\t1\t32134\t7\r\n```\r\nWe can do the same with `16584`, and then `34985`.\r\n```\r\n-- Mark 16584 as fraudulent\r\n1718981392977\t1\t34985\t7\r\n1718981392977\t-1\t16584\t7\r\n-- Mark 34985 as fraudulent\r\n1718981398158\t1\t35131\t7\r\n1718981398158\t-1\t34985\t7\r\n```\r\nFinally, let's remove all records from `fraud_accounts` and we can see that we return back to the original state.\r\n```\r\n-- Remove all fraud indicators.\r\n1718981403087\t-1\t35131\t7\r\n1718981403087\t1\t17519\t7\r\n1718981403087\t-1\t32134\t7\r\n1718981403087\t1\t16584\t7\r\n...\r\n```\r\nThat `34985` record isn't mention here because it only showed up due to our other removals.\r\nWe don't hear about a change because there is no moment when it is in the top five, even transiently.\r\nThat is a great lead-in to Materailize's **consistency** properties!\r\n\r\n#### Beat 3: Consistency\r\n\r\nAll the freshness and responsiveness in the world doesn't mean much if the results are incoherent.\r\nMaterialize only ever presents actual results that actually happened, with no transient errors.\r\nWhen you see results, you can confidently act on them knowing that they are real, and don't need further second to bake.\r\n\r\nLet's take a look at consistency through the lens of account balances as auctions close and winning buyers must pay sellers.\r\n```sql\r\n-- Account ids, with credits and debits from auctions sold and won.\r\nCREATE VIEW funds_movement AS\r\n  SELECT id,\r\n         SUM(credits) AS credits,\r\n         SUM(debits) AS debits\r\n  FROM (\r\n    SELECT seller AS id, amount AS credits, 0 AS debits\r\n    FROM winning_bids\r\n    UNION ALL\r\n    SELECT buyer AS id, 0 AS credits, amount AS debits\r\n    FROM winning_bids\r\n  )\r\n  GROUP BY id;\r\n```\r\n\r\nThese balances derive from the same source: `winning_bids`, and although they'll vary from account to account, they should all add up.\r\nSpecifically, if we get the total credits and the total debits, they should 100% of the time be exactly equal.\r\n```sql\r\n-- Discrepancy between credits and debits.\r\nSELECT SUM(credits) - SUM(debits) \r\nFROM funds_movement;\r\n```\r\nThis query reports zero, 100% of the time.\r\nWe can `SUBSCRIBE` to the query to be notified of any change.\r\n```\r\nmaterialize=> COPY (SUBSCRIBE (\r\n    SELECT SUM(credits) - SUM(debits) \r\n    FROM funds_movement\r\n)) TO STDOUT;\r\n\r\n1716312983129\t1\t0\r\n```\r\nThis tells us that starting at time `1716312983129`, there was `1` record, and it was `0`.\r\nYou can sit there a while, and there will be no changes.\r\nYou could also add the `WITH (PROGRESS)` option, and it will provide regular heartbeats confirming that second-by-second it is still zero.\r\nThe credits and debits always add up, and aren't for a moment inconsistent.\r\n\r\nWe can set up similar views for other assertions.\r\nFor example, every account that has sold or won an auction should have a balance.\r\nA SQL query can look for violations of this, and we can monitor it to see that it is always empty.\r\nIf it is ever non-empty, perhaps there are bugs in the query logic, its contents are immediately actionable: \r\nthere is a specific time where the inputs evaluated to an invariant-violating output, and if you return to that moment you'll see the inputs that produce the bad output.\r\n\r\nThe consistency extends across multiple independent sessions.\r\nThe moment you get confirmation that the insert into `fraud_accounts`, you can be certain that no one will see that account in the top five non-fraudulent auction winners.\r\nThis guarantee is called 'strict serializability', that the system behaves as if every event occurred at a specific time between its start and end, and is the strongest guarantee that databases provide.\r\n\r\n#### Demo over!\r\n\r\nThat's it!\r\nWe've completed the introduction to Materialize, and used auction data to show off responsiveness, freshness, and consistency.\r\nThere's a lot more to show off, of course, and if any of this sounded fascinating you should swing by https://materialize.com/register/ to spin up a trial environment.\r\n\r\nHowever, in this post we will continue to unpack how we got all of that `auctions` and `bids` data in the first place!\r\n\r\n### Auction Data from Changing Moments\r\n\r\nWhere do the `auctions` and `bids` data come from?\r\nYou can get them from our load generator, but we're going to try and coax them out of raw SQL.\r\nWe're going to start with something we haven't introduced yet, but it's a view whose content looks like this\r\n```sql\r\n-- All seconds within the past 24 hours.\r\nCREATE VIEW moments AS\r\nSELECT generate_series(\r\n    now() - '1 day'::interval + '1 second'::interval,\r\n    now(),\r\n    '1 second'\r\n) moment;\r\n``` \r\n\r\nUnpacking this, `moments` contains rows with a single column containing a timestamp.\r\nWhenever we look at it, the view contains those timestamps at most one day less than `now()`.\r\nIt should have at any moment exactly 86,400 records present, as many as `auctions` up above.\r\n\r\nImportantly, this view definition will not actually work for us.\r\nYou are welcome to try it out, but you'll find out that while it can be *inspected*, it cannot be *maintained*.\r\nWe'll fix that by the end of the post, but it will need to wait until the next section.\r\nFor the moment, let's assume we have this view and the magical ability to keep it up to date.\r\n\r\nThese 'moments' are not auction data, though.\r\nHow do we get from moments to auctions and bids?\r\n\r\nThe `auctions` and `bids` collections look roughly like so:\r\n```\r\nmaterialize=> select * from auctions;\r\n id | seller |        item        |          end_time          \r\n----+--------+--------------------+----------------------------\r\n  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00\r\n  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00\r\n  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00\r\n  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00\r\n  ...\r\n(4 rows)\r\n```\r\n```\r\nmaterialize=> select * from bids;\r\n id | buyer | auction_id | amount |          bid_time          \r\n----+-------+------------+--------+----------------------------\r\n 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00\r\n 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00\r\n 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00\r\n 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00\r\n ...\r\n```\r\n\r\nAuctions have a unique id, a seller id, an item description, and an end time.\r\nBids have a unique id (no relation), a buyer id, an auction id, the amount of the bid, and the time of the bid.\r\n\r\nThe `seller`, `item`, `buyer`, and `amount` fields are all random, within some bounds.\r\nAs a first cut, we'll think about just using random values for each of the columns.\r\nWhere might we get randomness, you ask?\r\nWell, if *pseudo*-randomness is good enough (it will be), we can use cryptographic hashes of the moments.\r\n```sql\r\n-- Extract pseudorandom bytes from each moment.\r\nCREATE VIEW random AS\r\nSELECT moment, digest(moment::text, 'md5') as random\r\nFROM moments;\r\n```\r\nLet's start with bytes from `random` to populate columns, and we'd have a first cut at random data.\r\nColumns like `auctions.item` are populated by joining with a constant collection (part of the generator), but `id` and `seller` could just be random.\r\nThe `end_time` we'll pick to be a random time up to 256 minutes after the auction starts.\r\n```sql\r\n-- Totally accurate auction generator.\r\nCREATE VIEW auctions_core AS\r\nSELECT \r\n    moment,\r\n    random,\r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 as seller,\r\n    get_byte(random, 5) as item,\r\n    -- Have each auction expire after up to 256 minutes.\r\n    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time\r\nFROM random;\r\n```\r\nWe've clearly made some calls about how random each of these should be, and those calls influence what we'll see in the data.\r\nFor example, we've established at most 65,536 sellers, which lines up fine with our 86,400 auctions at any moment; some sellers will have multiple auctions and many will not.\r\nAuctions are open for a few hours on average, close out but linger, and then vanish after 24 hours.\r\nIf we want to change any of these, perhaps to add more distinct items, or keep auctions running longer, or to skew the distribution over sellers, we can!\r\n\r\nSimilarly, the columns of `bids` are also pretty random, but columns like `auction_id` and `bid_time` do need to have some relationship to `auctions` and the referenced auction.\r\nWe'll build those out in just a moment, but have a bit more tidying to do for `auctions` first.\r\n\r\n#### Adding Custom Expiration\r\n\r\nOur auctions wind down after some random amount of time, but they are not removed from `auctions` for three hours.\r\nThematically we can think of this as auctions whose winners have been locked in, but whose accounts have not yet been settled.\r\n\r\nIf we want the auction to vanish from `auctions` at this time it closed, we could accomplish this with a temporal filter:\r\n```sql\r\nWHERE mz_now() < end_time\r\n```\r\nAs soon as we reach `end_time` the auction would vanish from `auctions`.\r\n\r\nThis is a very helpful pattern for load generators that want to control when data arrive and when it departs, in finer detail than 'a twenty four hour window'.\r\nFor example, one could randomly generate `insert_ts` and `delete_ts`, and then use\r\n```sql\r\n-- Create an event that is live for the interval `[insert_ts, delete_ts]`.\r\nWHERE mz_now() BETWEEN insert_ts AND delete_ts\r\n```\r\nThis pattern allows careful control of when events *appear* to occur, by holding them back until `mz_now()` reaches a value, and then retracting them when it reaches a later value. \r\n\r\n#### Making More Realistic Data\r\n\r\nOur random numbers for `item` aren't nearly as nice as what the existing load generator produces.\r\nHowever, we can get the same results by putting those nice values in a view and using our integer `item` to join against the view.\r\n```sql\r\n-- A static view giving names to items.\r\nCREATE VIEW items (id, item) AS VALUES\r\n    (0, 'Signed Memorabilia'),\r\n    (1, 'City Bar Crawl'),\r\n    (2, 'Best Pizza in Town'),\r\n    (3, 'Gift Basket'),\r\n    (4, 'Custom Art');\r\n```\r\n\r\nNow when we want to produce an actual auction record, we can join against items like so\r\n```sql\r\n-- View that mirrors the `auctions` table from our load generator.\r\nCREATE VIEW auctions AS\r\nSELECT id, seller, items.item, end_time\r\nFROM auctions_core, items\r\nWHERE auction.item = items.id;\r\n```\r\n\r\nWe've now got a view `auctions` that mirrors what Materialize's load generator produces, at least superficially.\r\n\r\n#### Introducing Foreign Key Constraints\r\n\r\nEach bid in `bids` references an auction, and we are unlikely to find an extant auction if we just use random numbers for `auction_id`.\r\nWe'd like to base our `bids` on the available auctions, and have them occur at times that make sense for the auction.\r\n\r\nWe can accomplish this by deriving the bids for an auction from `auctions` itself.\r\nWe will use some available pseudorandomness to propose a number of bids, and then create further pseudorandomness to determine the details of each bid.\r\n```sql\r\nCREATE VIEW bids AS\r\n-- Establish per-bid records and pseudorandomness.\r\nWITH prework AS (\r\n    -- Create `get_byte(random, 6)` many bids for each auction, \r\n    -- each with their own freshly generated pseudorandomness.\r\n    SELECT \r\n        id as auction_id,\r\n        moment as auction_start,\r\n        end_time as auction_end,\r\n        digest(random::text || generate_series(1, get_byte(random, 6))::text, 'md5') as random\r\n    FROM auctions_core\r\n)\r\nSELECT\r\n    get_byte(random, 0) +\r\n    get_byte(random, 1) * 256 +\r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) AS buyer,\r\n    auction_id,\r\n    get_byte(random, 4)::numeric AS amount,\r\n    auction_start + (get_byte(random, 5)::text || ' seconds')::interval as bid_time\r\nFROM prework;\r\n```\r\n\r\nWe now have a pile of bids for each auction, with the compelling property that when the auction goes away so too do its bids.\r\nThis gives us 'referential integrity', the property of foreign keys (`bids.auction_id`) that their referent (`auction.id`) is always valid.\r\n\r\nAnd with this, we have generated the `auctions` and `bids` data that continually change, but always make sense.\r\n\r\nThere are several other changes you might want to make!\r\nFor example, random bids means that auctions stop changing as they go on, because new random bids are unlikely to beat all prior bids.\r\nYou could instead have the bids trend up with time, to keep the data interesting.\r\nBut, the changes are pretty easy to roll out, and just amount to editing the SQL that defines them.\r\n\r\nLet's pause for now on noodling on ways we could make the data even more realistic.\r\nUp next we have to unpack how we got that `moments` view in the first place.\r\nOnce we've done that, you are welcome to go back to playing around with load generator novelties and variations!\r\n\r\n### Operational Data from Thin Air\r\n\r\nOur `auctions` and `bids` data was based on a view `moments` that showed us all timestamps within the past three hours.\r\nWe saw how we could go from that to pretty much anything, through extracted pseudorandomness.\r\n\r\nWe used a view that seemed maybe too easy, that looked roughly like so:\r\n```sql\r\n-- Generate a sliding window over timestamp data.\r\n-- Arguments: <volume>, <velocity>\r\nSELECT moment,\r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    <velocity>\r\n) moment\r\nWHERE now() BETWEEN moment AND moment + <volume>;\r\n```\r\n\r\nThis example uses `generate_series` to produce moments at which events will occur.\r\nThe `<velocity>` argument chooses the step size of the `generate_series` call, and locks in the cadence of updates.\r\nThe `<volume>` argument controls for how long each record lingers, and sets the steady state size.\r\nThe result is a sliding window over random data, where you get to control the volume and velocity.\r\n\r\nWe used `'1 second'` for the velocity and `'1 day'` for the volume.\r\n\r\nNow, while you can *type* the above, it won't actually run properly if you press enter.\r\nThe query describes 130 years of data, probably at something like a one second update frequency (because you wanted live data, right?).\r\nI don't even know how to determine how many records this is accurately based on all the leap-action that occurs.\r\nMoreover, you won't be able to materialize this view, because `now()` prevents materializations.\r\n\r\nTo actually get this to work, we'll have to use some clever tricks.\r\nThe coming subsections are a sequence of such tricks, and the punchline will be 'it works!', in case that saves you any time.\r\n\r\n#### Clever trick 1: using `mz_now()`\r\n\r\nOur first clever trick is to move from `now()` to `mz_now()`.\r\nThese are very similar functions, where the `now()` function gets you the contents of the system clock, and `mz_now()` gets you the transaction time of your command.\r\nThe main difference between the two is that we can materialize some queries containing `mz_now()`, unlike any query containing `now()`.\r\n\r\n```sql\r\n-- Generate a sliding window over timestamp data.\r\nSELECT moment,\r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 second'\r\n) moment\r\n--    /------\\---- LOOK HERE!\r\nWHERE mz_now() BETWEEN moment AND moment + '1 day';\r\n```\r\nThis very simple change means that Materialize now has the ability to keep the query up to date.\r\nMaterialize has a feature called ['temporal filters'](https://materialize.com/docs/transform-data/patterns/temporal-filters/) that allows `mz_now()` in `WHERE` clauses, because we are able to invert the clause and see the moment (Materialize time) at which changes will occur.\r\n\r\nUnfortunately, the implementation strategy for keeping this view up to date still involves first producing all the data, and then filtering it (we don't have any magical insight into `generate_series` that allows us to invert its implementation).\r\nBut fortunately, we have other clever tricks available to us.\r\n\r\n#### Clever trick 2: Hierachical Generation\r\n\r\nThe problem above is that we generate all the data at once, and then filter it.\r\nWe could instead generate the years of interest, from them the days of interest, from them the hours of interest, then minutes of interest, then seconds of interest, and finally milliseconds of interest.\r\nIn a sense we are generating *intervals* rather than *moments*, and then producing moments from the intervals.\r\n\r\nLet's start by generating all the years we might be interested in.\r\nWe start with all the years we might reasonably need, and a `WHERE` clause that checks for intersection of the interval (`+ '1 year'`) and the extension by volume (`+ '1 day'`).\r\n```sql\r\n-- Each year-long interval of interest\r\nCREATE VIEW years AS\r\nSELECT * \r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 year') year\r\nWHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';\r\n```\r\nThis view does not have all that many years in it. \r\nRoughly 130 of them.\r\nFew enough that we can filter them down, and get to work on days.\r\n\r\nAt this point, we'll repeatedly refine the intervals by subdividing into the next granularity.\r\nWe'll do this for years into days, but you'll have to use your imagination for the others.\r\nWe have all the SQL at the end, so don't worry that you'll miss out on that.\r\n```sql\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(\r\n        year, \r\n        year + '1 year' - '1 day'::interval, \r\n        '1 day') as day\r\n    FROM years\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day';\r\n```\r\nWe'll repeat this on to a view `seconds`, and stop there.\r\n\r\nAlthough we could continue to milliseconds, experience has been that it's hard to demo things changing that quickly through SQL.\r\nLines of text flow past like the Matrix, and all you can really see is that there is change, not what the change is.\r\n\r\nUnfortunately, there is a final gotcha.\r\nMaterialize is too clever by half, and if you materialize the `seconds` view, it will see that it is able to determine the entire 130 year timeline of the view, history and future, and record it for you.\r\nAt great expense.\r\nThese declarative systems are sometimes just too smart.\r\n\r\n#### Clever trick 3: An empty table\r\n\r\nWe can fix everything by introducing an empty table.\r\n\r\nThe empty table is only present to ruin Materialize's ability to be certain it already knows the right answer about the future.\r\nWe'll introduce it to each of our views in the same place, and its only function is to menace Materialize with the possibility that it *could* contain data.\r\nBut it won't.\r\nBut we wont tell Materialize that.\r\n\r\n```sql\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(\r\n        year, \r\n        year + '1 year' - '1 day'::interval, \r\n        '1 day') as day\r\n    FROM years\r\n    -- THIS NEXT LINE IS NEW!!\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day';\r\n```\r\n\r\nWith these tricks in hand, we now have the ability to spin it up and see what it looks like.\r\n\r\n```sql\r\nCREATE DEFAULT INDEX ON days;\r\n```\r\n\r\nWe'll want to create the same default indexes on our other views: `hours`, `minutes`, and `seconds`.\r\nImportantly, we want to create them in this order, also, to make sure that each relies on the one before it.\r\nIf they did not, we would be back in the world of the previous section, where each would read ahead until the end of time (the year 2099, in this example).\r\n\r\n#### Finishing touches\r\n\r\nAs a final bit of housekeeping, we'll want to go from intervals back to moments, with some additional inequalities.\r\n```sql\r\n-- The final view we'll want to use.\r\nCREATE VIEW moments AS\r\nSELECT second AS moment FROM seconds\r\nWHERE mz_now() >= second\r\n  AND mz_now() < second + '1 day';\r\n```\r\nThe only change here is the `mz_now()` inequality, which now avoids `BETWEEN` because it has inclusive upper bounds.\r\nThe result is now a view that always has exactly 24 * 60 * 60 = 86400 elements in it.\r\nWe can verify this by subscribing to the changelog of the count query:\r\n```sql\r\n-- Determine the count and monitor its changes.\r\nCOPY (\r\n    SUBSCRIBE (SELECT COUNT(*) FROM moments) \r\n    WITH (progress = true)\r\n)\r\nTO stdout;\r\n```\r\nThis reports an initial value of 86400, and then repeatedly reports (second by second) that there are no additional changes.\r\n```\r\nmaterialize=> COPY (\r\n    SUBSCRIBE (SELECT COUNT(*) FROM moments) \r\n    WITH (progress = true)\r\n)\r\nTO stdout;\r\n1716210913609\tt\t\\N\t\\N\r\n1716210913609\tf\t1\t86400\r\n1716210914250\tt\t\\N\t\\N\r\n1716210914264\tt\t\\N\t\\N\r\n1716210914685\tt\t\\N\t\\N\r\n1716210915000\tt\t\\N\t\\N\r\n1716210915684\tt\t\\N\t\\N\r\n1716210916000\tt\t\\N\t\\N\r\n1716210916248\tt\t\\N\t\\N\r\n1716210916288\tt\t\\N\t\\N\r\n1716210916330\tt\t\\N\t\\N\r\n1716210916683\tt\t\\N\t\\N\r\n^CCancel request sent\r\nERROR:  canceling statement due to user request\r\nmaterialize=> \r\n```\r\nAll rows with a second column of `t` are 'progress' statements rather than data updates.\r\nThe second row, the only one with a `f`, confirms a single record (`1`) with a value of `86400`.\r\n\r\nYeah, that's it! The only thing left is to read a wall of text containing all the SQL.\r\nActually, I recommend bouncing up to the start of the post again, and confirming that the pieces fit together for you.\r\nIt's also a fine time to [try out Materialize](https://materialize.com/register/), the only system that can run all of these views. \r\n\r\n### Appendix: All the SQL\r\n\r\n```sql\r\nCREATE TABLE empty (e TIMESTAMP);\r\n\r\n-- Supporting view to translate ids into text.\r\nCREATE VIEW items (id, item) AS VALUES\r\n    (0, 'Signed Memorabilia'),\r\n    (1, 'City Bar Crawl'),\r\n    (2, 'Best Pizza in Town'),\r\n    (3, 'Gift Basket'),\r\n    (4, 'Custom Art');\r\n\r\n-- Each year-long interval of interest\r\nCREATE VIEW years AS\r\nSELECT * \r\nFROM generate_series(\r\n    '1970-01-01 00:00:00+00', \r\n    '2099-01-01 00:00:00+00', \r\n    '1 year') year\r\nWHERE mz_now() BETWEEN year AND year + '1 year' + '1 day';\r\n\r\n-- Each day-long interval of interest\r\nCREATE VIEW days AS\r\nSELECT * FROM (\r\n    SELECT generate_series(year, year + '1 year' - '1 day'::interval, '1 day') as day\r\n    FROM years\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN day AND day + '1 day' + '1 day';\r\n\r\n-- Each hour-long interval of interest\r\nCREATE VIEW hours AS\r\nSELECT * FROM (\r\n    SELECT generate_series(day, day + '1 day' - '1 hour'::interval, '1 hour') as hour\r\n    FROM days\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN hour AND hour + '1 hour' + '1 day';\r\n\r\n-- Each minute-long interval of interest\r\nCREATE VIEW minutes AS\r\nSELECT * FROM (\r\n    SELECT generate_series(hour, hour + '1 hour' - '1 minute'::interval, '1 minute') AS minute\r\n    FROM hours\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN minute AND minute + '1 minute' + '1 day';\r\n\r\n-- Any second-long interval of interest\r\nCREATE VIEW seconds AS\r\nSELECT * FROM (\r\n    SELECT generate_series(minute, minute + '1 minute' - '1 second'::interval, '1 second') as second\r\n    FROM minutes\r\n    UNION ALL SELECT * FROM empty\r\n)\r\nWHERE mz_now() BETWEEN second AND second + '1 second' + '1 day';\r\n\r\n-- Indexes are important to ensure we expand intervals carefully.\r\nCREATE DEFAULT INDEX ON years;\r\nCREATE DEFAULT INDEX ON days;\r\nCREATE DEFAULT INDEX ON hours;\r\nCREATE DEFAULT INDEX ON minutes;\r\nCREATE DEFAULT INDEX ON seconds;\r\n\r\n-- The final view we'll want to use .\r\nCREATE VIEW moments AS\r\nSELECT second AS moment FROM seconds\r\nWHERE mz_now() >= second\r\n  AND mz_now() < second + '1 day';\r\n\r\n-- Extract pseudorandom bytes from each moment.\r\nCREATE VIEW random AS\r\nSELECT moment, digest(moment::text, 'md5') as random\r\nFROM moments;\r\n\r\n-- Present as auction \r\nCREATE VIEW auctions_core AS\r\nSELECT \r\n    moment,\r\n    random,\r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id,\r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 as seller,\r\n    get_byte(random, 5) as item,\r\n    -- Have each auction expire after up to 256 minutes.\r\n    moment + (get_byte(random, 6)::text || ' minutes')::interval as end_time\r\nFROM random;\r\n\r\n-- Refine and materialize auction data.\r\nCREATE MATERIALIZED VIEW auctions AS\r\nSELECT auctions_core.id, seller, items.item, end_time\r\nFROM auctions_core, items\r\nWHERE auctions_core.item % 5 = items.id;\r\n\r\n-- Create and materialize bid data.\r\nCREATE MATERIALIZED VIEW bids AS\r\n-- Establish per-bid records and randomness.\r\nWITH prework AS (\r\n    SELECT \r\n        id AS auction_id,\r\n        moment as auction_start,\r\n        end_time as auction_end,\r\n        digest(random::text || generate_series(1, get_byte(random, 5))::text, 'md5') as random\r\n    FROM auctions_core\r\n)\r\nSELECT \r\n    get_byte(random, 0) + \r\n    get_byte(random, 1) * 256 + \r\n    get_byte(random, 2) * 65536 as id, \r\n    get_byte(random, 3) +\r\n    get_byte(random, 4) * 256 AS buyer,\r\n    auction_id,\r\n    get_byte(random, 5)::numeric AS amount,\r\n    auction_start + (get_byte(random, 6)::text || ' minutes')::interval as bid_time\r\nFROM prework;\r\n```\u3002", "top": 0, "createdAt": 1728537546, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-10-10", "dateLabelColor": "#bc4c00"}, "P3": {"htmlDir": "docs/post/Computing and Maintaining Weird (Outer) Joins.html", "labels": ["documentation"], "postTitle": "Computing and Maintaining Weird (Outer) Joins", "postUrl": "post/Computing%20and%20Maintaining%20Weird%20%28Outer%29%20Joins.html", "postSourceUrl": "https://github.com/nuowoo/blog/issues/3", "commentNum": 0, "wordCount": 24875, "description": "\r\n[Differential dataflow](https://github.com/TimelyDataflow/differential-dataflow) has a single join operator: `join`.\r\nIt takes two input collections, and for each `(key, val1)` and `(key, val2)` in the inputs it produces `(key, (val1, val2))` in the output.\r\nThis makes `join` a 'binary equijoin', where it fishes out exactly the exact matches on `key`.\r\nThis restriction is important, and powerful: when either input experiences a change, the `key` of the change is what directs us to the (other) input records that will help us produce the appropriate output change.\r\nHowever, there are other 'joins' in the larger relational data world, and we need to support them as well.\r\n\r\nIn this post we'll build up an implementation of a **multi-way outer equijoin**.\r\nWe'll start small, but arrive at the best way I know how to build these beasts out of existing parts.\r\nAlong the way, we'll \r\n    get an introduction to how differential dataflow works, \r\n    develop several ways to use it to implement joins of various stripes, and\r\n    deploy these techniques together to take on the outer-est of (equi-)joins.\r\n\r\nAmazingly, to me at least, we end up needing to understand how to efficiently implement multi-way joins of sums of terms.\r\nThat is, how to efficiently implement\r\n```\r\n(A0 + A1 + A2) \u22c8 (B0 + B1 + B2) \u22c8 (C0 + ...) \u22c8 ...\r\n```\r\nTo be honest, I can't recall this pattern from my database education (such as it was), and I'd love any tips or pointers about where else this shows up.\r\nIf you get to the end and it all checks out as old-hat for you, I'd love to know about it!\r\n\r\n### Differential Dataflow and the Binary Equijoin `join`\r\n\r\nDifferential dataflow is a framework for computing and then maintaining functions over continually changing volumes of data.\r\nIt manipulates *updates* to data, written as triples `(data, time, diff)` and indicating that at `time` the number of occurrences of `data` changes by `diff`.\r\nDifferential dataflow provides primitive operators like `map`, `filter`, `join`, `reduce`, and `iterate`, which users compose to build more complex functions.\r\nEach operator translates input updates into the output updates that would result from continually re-evaluating the operator at every time. \r\nSimilarly, the composed dataflow of operators similarly produces output updates that correspond exactly to continual reevaluation on the changing inputs.\r\n\r\nThe `join` operator applies to two input collections, for which their `data` have the shape `(key, _)`: pairs of some common 'key' type and potentially unrelated 'value' types.\r\nThe intended output is a tuple `(key, (val1, val2))` for each pair of inputs that have a matching `key`.\r\nThe output updates can be derived from first principles, but with enough head-scratching one can conclude that each pair of updates with matching key produces one output update:\r\n\r\n```   \r\n    update1: ((key, val1), time1, diff1)     -- First input\r\n    update2: ((key, val2), time2, diff2)     -- Second input\r\n-> \r\n    ((key, (val1, val2)),  max(time1, time2),  diff1 * diff2)\r\n     \\-- output data --/   \\----  time ----/   \\--  diff --/\r\n```\r\n\r\nWe can respond to each input update by iterating over the updates in the *other* input with the same key, and use the rule above.\r\nThere are smarter ways to do this, consider for example the second update introducing and retracting a record before `time1`: we would produce two outputs that exactly cancel.\r\nIn any case, we'll need to retain *some* information about each input, ideally arranged by `key` so that these updates can be efficiently retrieved.\r\n\r\nDifferential dataflow has a primitive called an 'arrangement', which is both a stream of updates and a maintained indexed form of their accumulation.\r\nAn arrangement translates a stream of updates into a sequence of indexed 'batches' of updates, each of which are indexed by `key`.\r\nIt also maintains a collection of these batches that serve as an indexed roll-up of the accumulated updates, using a structure analogous to a [log-structured merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree).\r\nArrangements are the primary mechanism to maintain 'state' as a dataflow runs, and specifically are what `join` uses: each input to `join` must be an arrangement, and if they are not then they will be arranged for you.\r\n\r\n### Technique 1: Shared Arrangements\r\n\r\nA key advantage to using arrangements is that they can be [*shared*](http://www.vldb.org/pvldb/vol13/p1793-mcsherry.pdf).\r\nArranged data can be used by any number of dataflows, avoiding the cost of an additional redundant arrangement.\r\nAs an example, imagine we have a collection of link data `(source, target)`, and we would like to compute and maintain those identifiers within three steps of some query set `query`.\r\nIf the data are arranged, say in an arrangement named `links`, we could write\r\n```rust\r\n// Join `query` against `links` three times, giving\r\n// the identifiers three steps away from each query.\r\nquery.map(|query| (query, query))\r\n     .join(links).map(|(step0, (query, step1))| (step1, query))\r\n     .join(links).map(|(step1, (query, step2))| (step2, query))\r\n     .join(links).map(|(step2, (query, step3))| (step3, query))\r\n```\r\nThis fragment would naively require six arrangements, two for each `join` invocation.\r\nHowever, we are able to re-use the `links` arrangement at no cost, and instead only introduce three arrangements, corresponding to the number of steps (0, 1, and 2) out from `query`.\r\nThese new arrangements can be substantially smaller than `links`, and the amount of work required to compute and maintain the results can be trivial even when `links` is enormous.\r\n\r\n### Technique 2: Functional Joins\r\n\r\nThis one is a bit of cheat, in that by the end of it you may not be sure it is even a join.\r\n\r\nThere are times, and we will see them coming up, where we want to join not against *data* but against a *function*.\r\nFor example, perhaps we have a collection of `(line, text)` of pairs of integers and strings, and we would like to split each `text` into the words it contains.\r\nOne way to do this is with `join`: the first input is our lines of text, and the second input is the quite large collection of pairs `(text, (pos, word))` each indicating a word that can be found in `text`.\r\n\r\nRather than hope to implement this with `join`, because we couldn't hope to maintain the second collection, we could implement this with the `flat_map` operator instead.\r\n```rust\r\n// Convert each `text` into the words it contains.\r\nlines.flat_map(|(line, text)| \r\n    text.split_whitespace()\r\n        .enumerate()\r\n        .map(|(pos, word)| (line, text.clone(), (pos, word.to_owned())))\r\n)\r\n```\r\n\r\nAt this point you may be wondering why we have called this a 'functional join' rather than a 'flat map'.\r\nYou are not wrong that `flat_map` is the best way to implement this.\r\nHowever, we will need to prepare ourselves to see this pattern in joins, and understand that it is one way to implement something that may present as a `join`.\r\nEach input record results in zero or many output records, determined by some key fields in the record.\r\n\r\n### Technique 3: Multi-way Joins\r\n\r\nEven managing a single join can be challenging, but invariably folks actually want to perform multiple joins at once.\r\nRecall our `query` and `links` example, from just up above\r\n```rust\r\n// Join `query` against `links` three times, giving\r\n// the identifiers three steps away from each query.\r\nquery.map(|query| (query, query))\r\n     .join(links).map(|(step0, (query, step1))| (step1, query))\r\n     .join(links).map(|(step1, (query, step2))| (step2, query))\r\n     .join(links).map(|(step2, (query, step3))| (step3, query))\r\n```\r\nThis performs three joins, and introduces new arrangements for the left inputs of each of the three `join` calls.\r\nWe argued that this could be small if `query` is small, and also if each of the intermediate results are small.\r\nBut if this isn't the case, then they might be large, and we might end up maintaining quite a lot of information.\r\n\r\nLet's take a different example that might not be so easy.\r\nImagine you start with a collection `facts` of raw data, and you want to enrich it using dimesion tables that translate foreign keys like 'user id' into further detail.\r\nThe additional detail may result in further keys you want to unpack, like addresses, zipcodes, and the sales agents they map to.\r\n```rust\r\n// Enrich facts with user, address, and sales agent information.\r\nfacts.map(|fact| (fact.user_id, fact)).join(users).map( .. )\r\n     .map(|fact| (fact.addr_id, fact)).join(addrs).map( .. )\r\n     .map(|fact| (fact.zipcode, fact)).join(agent).map( .. )\r\n```\r\nLots and lots of data pipelines have this sort of enrichment in them, in part because 'normalized' database best practices are to factor apart this information.\r\nUnfortunately, stitching it back together efficiently is an important part of these best practices.\r\n\r\nFor this query, we may have arrangements of `users`, `addrs`, `agent`.\r\nHowever, we are unlikely to have arrangements of the left inputs to each of the `join`s.\r\nThe very first left input, `facts` keyed by `user_id`, is plausibly something we might have pre-arranged, but the other two result from the query itself.\r\nNaively implemented, we'll create second and third arrangements of enriched `fact` data, which can be really quite large.\r\n\r\nFortunately, there is a trick for multiway joins that I have no better name for than ['delta joins'](https://github.com/TimelyDataflow/differential-dataflow/tree/master/dogsdogsdogs).\r\nThe gist is that rather than plan a multiway join as a sequence (or tree) of binary joins, as done in System R, you describe how the whole join will vary as a function of each input.\r\nYou can get this derivation by expanding out our derivation for binary joins, in terms of input updates, to multiple inputs.\r\nYou then independently implement each of these response functions for each input as best as you can and then compose their results.\r\n\r\nFor example, our query above joins four relations: `facts`, `users`, `addrs`, and `agent`, subject to some equality constraints.\r\nWhen `facts` changes, we need to look up enrichments in `users`, then `addrs`, then `agent` to find the change to enriched facts.\r\nWhen `agent` changes, we need to find the affected `addrs`, then `users`, then `facts`, in order to update the enrichment of existing facts.\r\n\r\n```\r\n-- rules for how to react to an update to each input.\r\n-- elided: equality constraints for each join (\u22c8).\r\nd_query/d_facts = d_facts \u22c8 users \u22c8 addrs \u22c8 agent\r\nd_query/d_users = d_users \u22c8 addrs \u22c8 agent \u22c8 facts\r\nd_query/d_addrs = d_addrs \u22c8 agent \u22c8 users \u22c8 facts\r\nd_query/d_agent = d_agent \u22c8 addrs \u22c8 users \u22c8 facts\r\n```\r\nThe overall changes to `query` result from adding together these update rules.\r\n\r\nWhat's different above is that each of the `d_term \u22c8` joins are *ephemeral*: no one needs to remember the `d_` part of the input.\r\nEach of these rules are implementable with what differential dataflow calls a `half_join`: an operator that responds to records in one input by look-ups into a second, and which does not respond to changes to the second input.\r\nThe `half_join` operator needs an arrangement of its second input, but not of its first input.\r\n\r\nThis pattern has different arrangement requirements than the sequence of binary `join` operators.\r\nEach collection needs an arrangement by those attributes by which it may be interrogated.\r\nIn the example above, the required arrangements end up being:\r\n\r\n1. input `facts` arranged by `user_id`,\r\n2. input `users` arranged by `user_id` and also by `addr_id`,\r\n3. input `addrs` arranged by `addr_id` and also by `zipcode`,\r\n4. input `agent` arranged by `zipcode`.\r\n\r\nThis ends up being six arrangements, just like before, but they are all arrangements we might reasonably have ahead of time.\r\nThe *incremental* arrangement cost of the query can be zero, if these arrangement are all pre-built.\r\n\r\n### Boss Battle: Left Outer Joins\r\n\r\nAn 'outer' join is a SQL construct that is much like an standard ('inner') join except that any records that 'miss', i.e. do not match any other records, are still produced as output but with `NULL` values in columns we hoped to populate.\r\nOuter joins are helpful in best-effort joins, where you hope to enrich some data, but can't be certain you'll find the enrichment and don't want to lose the input data if you cannot.\r\n\r\nFor example, consider our `facts`, `users`, `addrs`, and `agent` scenario just above.\r\nWhat would happen if there is a `user_id` that does not exist in `users`, or a `addr_id` that does not exist in `addrs`, or a `zipcode` that does not exist in `agent`?\r\nWritten as a conventional join, we would simply drop such records on the floor and never speak of them.\r\nSometimes that is the right thing to do, but often you want to see the data along with any *failures* to find the enrichments.\r\n\r\nIf we take our example from above but use `LEFT JOIN` instead of `join`, we will keep even facts that do not match `users`, `addrs`, or `agent`.\r\n```sql\r\n-- Enrich facts with more data, but don't lose any.\r\nfacts LEFT JOIN users ON (facts.user_id = users.id)\r\n      LEFT JOIN addrs ON (users.addr_id = addrs.id)\r\n      LEFT JOIN agent ON (addrs.zipcode = agent.zc)\r\n```\r\n\r\nThere are also `RIGHT` and `FULL` joins, which respectively go in the other direction (e.g. output users that match no facts, with null fact columns) and in both directions (all bonus records that would be added to a `LEFT` or `RIGHT` join).\r\nWe are only going to noodle on `LEFT` joins, though the noodling should generalize just fine.\r\n\r\nTo implement left joins, we'll need to find a way to express them in terms of the tools we have.\r\nThose tools are .. the operators differential dataflow provides; things like `map`, `filter`, `join`, and `reduce` (no `iterate`. NO!).\r\n\r\n### Step one: turn LEFT JOINs into JOINs\r\n\r\nWhen we left join two collections, some records match perfectly as in an inner join, and some do not.\r\nWhat do we have to add to the results of the inner join to get the correct answer?\r\nSpecifically, any keys that might be present in the first input, but are not present in the second input, could just be added to the second input with `NULL` values.\r\n\r\n```sql\r\n-- Some facts exactly match some entry in users.\r\nSELECT * FROM facts INNER JOIN users ON (facts.user_id = users.id)\r\n-- Some facts totally miss, but need to match something.\r\nUNION ALL\r\nSELECT facts.*, NULL FROM facts\r\nWHERE facts.user_id NOT IN (SELECT id FROM users)\r\n```\r\nThis construction keeps the `INNER JOIN` pristine, but adds in `facts` extended by `NULL`s for any fact whose `user_id` is not found in `users`.\r\nAlthough not totally clear, `NOT IN` results in a join between `facts` and distinct `users.id`.\r\nThis approach feels good, re-uses arrangements on `facts` and `users`, and is pretty close to what Materialize does for you at the moment.\r\n\r\nHowever, this technique is not great for multiway outer joins.\r\nWe need access to the left input (here: `facts`) to complete the outer join, and generally that input is the result of the outer join just before this one.\r\nIf we need to have that answer to form this query fragment, we don't have a story for how they all become one multiway inner join.\r\nLikewise, Materialize currently plans a multiway outer join as a *sequence* of fragments like above that *involve* inner joins, but are not *an* inner join.\r\n\r\n### Step two: Multiway LEFT JOINS into Multiway JOINs\r\n\r\nLet's take the intuition above and see if we can preserve the join structure.\r\nWe want to produce a SQL fragment that is at its root just an inner join.\r\nWe will need to be careful that it should rely on base tables, not its direct inputs (what?).\r\n\r\nLet's start and we'll see where we get.\r\n\r\nFirst, let's rewrite the above fragment in a way that looks more like *one* inner join.\r\nOne one side we have `facts`, and on the other side .. at least `users` but also some other stuff?\r\nFor a first cut, that 'other stuff' is .. the `user_id`s in `facts` but not in `users`?\r\nWe could add those rows to `users`, with `NULL` values in missing columns, and see what we get!\r\n\r\nAs it turns out we get totally the wrong answer. \r\nBest intentions, of course, but the wrong answer.\r\nI believe the right answer is expressed roughly this way, in SQL:\r\n\r\n```sql\r\n-- Some facts exactly match some entry in users.\r\nSELECT facts.*, users.* \r\nFROM facts INNER JOIN users ON (facts.user_id = users.id)\r\n-- Some facts totally miss, but could match something.\r\nUNION ALL\r\nWITH absent(id) AS (\r\n    SELECT user_id FROM facts \r\n    EXCEPT \r\n    SELECT id FROM users\r\n)\r\nSELECT facts.*, NULL \r\nFROM facts INNER JOIN absent ON (facts.user_id = absent.id)\r\n-- Some facts have NULL `user_id` and refuse to be joined.\r\nUNION ALL\r\nSELECT facts.*, NULL\r\nFROM facts WHERE facts.user_id IS NULL\r\n```\r\n\r\nWe do grab the `absent` keys, but importantly we produce `NULL` in their key columns.\r\nWe also need to deal with potentially null `user_id` values, which we do in the third clause, because SQL's `NULL` values do not equal themselves.\r\nAgain, best intentions, I'm sure.\r\n\r\nThe good news is that we have framed the SQL in a way that looks like (taking some notational liberties):\r\n```\r\n  facts \u22c8 users\r\n+ facts \u22c8 absent    -- with null outputs\r\n+ facts \u22c8 NULLs     -- only for null user_id\r\n```\r\nEach of the three joins are slightly different, but they all have the property that `facts` arranged by `user_id` is enough for them.\r\nWe can and will now factor out `facts` from these three terms, which puts us in a position to write our multiway left join as:\r\n\r\n```\r\n-- Left join of facts, users, addrs, and agent.\r\nfacts \u22c8 (users + absent(users) + NULL)\r\n      \u22c8 (addrs + absent(addrs) + NULL)\r\n      \u22c8 (agent + absent(agent) + NULL)\r\n```\r\nThis is starting to look a bit more like the joins over sums of terms advertised in the beginning of the post.\r\nFor the moment, we are just going to add together the terms, though.\r\n\r\nThere is quite a lot unsaid here, and the nature of the \u22c8 varies a bit for each of the terms in parentheses.\r\nYou do have to populate the `absent(foo)` collections with values from base relations, rather than their immediate inputs.\r\nAnd fortunately, SQL notwithstanding, differential dataflow *does* equate NULL with itself, and everything works out just fine.\r\nMaterialize [recently merged](https://github.com/MaterializeInc/materialize/pull/24345) an approach that looks like this for multiway outer joins.\r\nIt's early days, but we'll soon start exploring how this work for folks with stacks of left joins.\r\n\r\nBut the story doesn't end here. \r\nSomewhat stressfully, this approach takes existing inputs `facts`, `users`, `addrs`, and `agent` and .. fails to use any of their pre-existing arrangements.\r\nIt has some other performance issues as well.\r\n\r\n### Step three: Rendering JOINs of UNIONs\r\n\r\nThe last step, or next step at least .. perhaps not the last, is to render these query plans efficiently.\r\nAt the moment we have no better plan than to treat the augmented collections as new collections, arrange them, and join them.\r\nRoughly like so:\r\n```\r\n-- Left join of facts, users, addrs, and agent.\r\nwith users_aug as (users + absent(users) + NULL)\r\nwith addrs_aug as (addrs + absent(users) + NULL)\r\nwith agent_aug as (agent + absent(agent) + NULL)\r\nfacts \u22c8 users_aug \u22c8 addrs_aug \u22c8 agent_aug\r\n```\r\n\r\nThese `_aug` collections are as big (somewhat bigger) than their unaugmented counterparts, and it feels somewhat bad to re-arrange them.\r\nIt feels bad that despite pre-arranging `users`, `addrs`, and `agent` we can re-use none of them.\r\nIt feels bad that all `NULL` values will be routed to a single worker just to find out that they map to `NULL`; lots of work for no surprise.\r\n\r\nHowever, we can get around all of these bad feels with some dataflow shenanigans.\r\nUnfortunately, they are shenanigans that as far as I can tell neither Materialize nor SQL can describe.\r\n\r\nWe have two strategies for evaluating multiway joins: as a sequence of binary joins, and using delta join rules.\r\nThe shenanigans are easier with the sequence of binary joins, so let's start there.\r\nWe are going to do something as simple as re-distributing over the `\u22c8` operator, performing each join the way we want.\r\nWe then add up the results of each step of the join rather than adding up the inputs to each step of the join.\r\n\r\n```\r\n-- Left join of facts, users, addrs, and agent.\r\nstep0 = facts;\r\nstep1 = step0 \u22c8 users + step0 \u22c8 absent(users) + step0 \u22c8 NULL;\r\nstep2 = step1 \u22c8 addrs + step1 \u22c8 absent(addrs) + step1 \u22c8 NULL;\r\nstep3 = step2 \u22c8 agent + step2 \u22c8 absent(agent) + step2 \u22c8 NULL;\r\nstep3\r\n```\r\nEach of these \u22c8 operators are slightly different. \r\nThe first \u22c8 in each row is the traditional equijoin.\r\nThe second \u22c8 in each row is an equijoin that projects away matched keys and puts `NULL` in their place.\r\nThe third \u22c8 in each row only matches nulls.\r\n\r\nHowever, in each line we only have to arrange non-null `stepx` and determine and arrange `absent(foo)`. \r\nWe can re-use existing arrangements of `users`, `addrs`, and `agent`.\r\nThe join with `NULL` can be implemented as a `flat_map` rather than by co-locating all null records for a `join` (omg finally explained).\r\n\r\nIn actual fact, we can implement this in both SQL and Materialize, but in doing so we'll lose the multiway join planning benefit of avoiding intermediate arrangements.\r\nWe will need to arrange `stepx` for each `x`, and the nice folks with stack of left joins 30+ deep (yes, seriously) will be sitting on 30x as much data as they feel they should.\r\n\r\nTo recover the benefits, let's grab the delta join construction from way up above. \r\nI'll use `_aug` suffixes to remind us that it isn't going to be as easy as joining against the pre-arranged collections.\r\n```\r\n-- rules for how to react to an update to each input.\r\n-- elided: equality constraints for each join (\u22c8).\r\nd_query/d_facts     = d_facts     \u22c8 users_aug \u22c8 addrs_aug \u22c8 agent_aug\r\nd_query/d_users_aug = d_users_aug \u22c8 addrs_aug \u22c8 agent_aug \u22c8 facts\r\nd_query/d_addrs_aug = d_addrs_aug \u22c8 agent_aug \u22c8 users_aug \u22c8 facts\r\nd_query/d_agent_aug = d_agent_aug \u22c8 addrs_aug \u22c8 users_aug \u22c8 facts\r\n```\r\nIgnore for the moment the fact that `d_users_aug` is complicated (an update to `users` may induce the opposite update to `absent(users)`).\r\nEach line up above describes a sequence of `half_join` applications, which like `join` also distributes over `+`.\r\n\r\n```\r\n  d_query/d_facts    \r\n= d_facts \u22c8 users_aug \u22c8 addrs_aug \u22c8 agent_aug\r\n= ( \r\n    d_step0 = d_facts;\r\n    d_step1 = d_step0 \u22c8 users + d_step0 \u22c8 absent(users) + d_step0 \u22c8 NULL;\r\n    d_step2 = d_step1 \u22c8 addrs + d_step1 \u22c8 absent(addrs) + d_step1 \u22c8 NULL;\r\n    d_step3 = d_step2 \u22c8 agent + d_step2 \u22c8 absent(agent) + d_step2 \u22c8 NULL;\r\n    d_step3\r\n)\r\n```\r\nEach time we need to do a `half_join`, we can unpack the right argument to it and conduct the half join as we see fit.\r\nWe can either `half_join` with a pre-existing arrangement, `half_join` with a new arrangement of absent values, or `flat_map` some `NULL` values into place.\r\n\r\nWriting the whole thing out is exhausting, especially for 30-deep stacks of left joins.\r\nFortunately this is something computers are good at.\r\nMuch like it now seems that they may be good at computing and maintaining deep stacks of left equijoins.\r\n\r\n### What's next?\r\n\r\nThere is an expressivity gap to close between SQL/Materialize and differential dataflow.\r\nI'm not aware of a SQL-to-SQL rewrite that gets us the desired implementation, because we cannot afford to distribute the joins out across the unions, and SQL does not have a `half_join` operator.\r\nWe're pondering options now, including expanding our lowest level IR to reflect e.g. half joins, and tweaking the renderer to recognize the idiom of joins between sums of terms.\r\nThere will certainly be some amount of measurement as we try and assess the remaining gap, and draw down the amount of time and resources spent on outer joins.\r\n\r\nI have a concurrent effort to spread the gospel of [referential integrity](https://en.wikipedia.org/wiki/Referential_integrity) so that we can turn those outer joins to inner joins.\r\nYou can understand how in weakly consistent systems you'd need the outer joins to cover for inconsistencies, but do you need it in a strongly consistent system like Materialize?\r\n\r\nOf course, if you've read this far you have an obligation to fill me in on what I've missed about all of this.\r\nIs there an easier transform, one that doesn't end up joining terms that are themselves sums of useful constituents?\r\nDo you have an exciting use case for maintaining stacks of outer joins, and you've been burned before?\r\nDo reach out in these cases, and [take Materialize for a spin](https://materialize.com/register/) (though, if you have stacks of 30+ left joins, please reach out for some personal attention).\u3002", "top": 0, "createdAt": 1728537911, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-10-10", "dateLabelColor": "#bc4c00"}}, "singeListJson": {}, "labelColorDict": {"bug": "#d73a4a", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "question": "#d876e3", "wontfix": "#ffffff"}, "displayTitle": "Blog Title", "faviconUrl": "https://github.githubassets.com/favicons/favicon.svg", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://nuowoo.github.io/blog", "prevUrl": "disabled", "nextUrl": "disabled"}